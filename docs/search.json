[
  {
    "objectID": "chap19_Natural_experiments_revisited.html",
    "href": "chap19_Natural_experiments_revisited.html",
    "title": "\n11  自然実験:再訪\n",
    "section": "",
    "text": "11.1 再現危機？\nこの章では、自然実験のトピックを再訪する。  まず、登録報告（Registered Reports）の概念、その目的、およびその限界について論じる。  次に、米国証券取引委員会(SEC)によって実施された実験「Reg SHO」と、それに関する研究を検討する。特に、Fang et al.（2016）の研究に焦点を当て、この規制を利用して利益マネジメント(Earnings Management)への影響を分析した点を詳しく論じる。\nこの章では、以下のような領域でスキルと知識を磨く機会を提供する。  第1に、利益マネジメントのテーマを再訪し、Dechow et al. (1995)以来の測定手法の発展について学ぶ。このテーマは第16章ですでに扱っている。  第2に、Reg SHOおよび頻繁に研究されている証券会社の閉鎖(broker-closure shocks)の事例を用いて、自然実験とされる評価スキルをさらに発展させる。  第3に、差分の差(diffrence-in-differences)手法を探求し、ランダムな割当がある場合と、ランダムな割当がない場合の「平行トレンド仮定」(parallel trends assumption)に基づく場合の両方について考察する。  第4に、第4章および第18章で扱った因果ダイアグラムと因果メカニズムに関連する概念を応用する機会をもつ。  第5に、統計的推論のテーマを再訪し、この章をランダム化推論(randomization inference)を考慮する機会として利用する。  第6に、Frisch-Waugh-Lovellの定理を基に、会計研究の多くの分野で一般的な二段階回帰の使用に関連する問題を考察する。\nロバート・ウィグルズワースによるFinancial Timesの記事は、ファイナンス研究における「再現危機」を取り上げた。  Wigglesworthは、デューク大学のファイナンス教授であるCanmpbell Harveyの発言を引用しており、Harveyは「これまでのトップのファイナンス・ジャーナルで特定された400以上の市場を打ち負かすとされる戦略のうち、少なくとも半分は偽物である」と述べている。\nWigglesworthは、研究者が「有意な」および「ポジティブな」結果を探すという実践であるp-hackingを研究者が行っていると指摘し、「問題の核心」を特定した。  ここで「有意性」とは統計的有意性を指し、「ポジティブ」とはいわゆる「帰無仮説」を棄却し、それによって（推定される）人間の知識を前進させる結果を指す。  Harvey（2017）は、90%の発表された研究がこのような「有意な」および「ポジティブな」結果を報告していると示唆する研究を引用している。  「ポジティブ」な結果を報告することは、出版されるだけでなく、引用を集めるためにも重要であり、これは研究者やジャーナルの行動を促すことができる。\nSimmons et al.（2011, p.1359）は、彼らが研究者の自由度と呼ぶものを説明している。   「データを収集し分析する過程で、研究者は多くの決定を下さなければならない。より多くのデータを収集すべきか？ 一部の観測を除外すべきか？ どの条件を統合し、どの条件を比較すべきか？ どの統制変数を考慮すべきか？ 特定の測定値を統合すべきか、それとも変換すべきか、あるいはその両方を行うべきか？」  Simmons et al. (2011, p.1364) は、もう一つのよく知られた研究者の自由度として、「うまくいった実験のみを報告する」ことを挙げている。これは「ファイルドロワー問題」として知られており、うまくいかなかった実験が引き出しにしまわれてしまうことを指す。\n研究者の自由度の影響力を示すために、Simmons et al. (2011) は、ライブの被験者を用いた実験に基づく2つの仮想的な研究を実施した。  彼らは、これらの研究が「統計的に有意な証拠を、誤った仮説のために蓄積し（そして報告し）てしまうことが、いかに容認できないほど容易であるかを示している」と主張している [p.1359]。  また、Simmons et al. (2011, p.1359) は、「データ収集、分析、および報告における柔軟性が、実際の偽陽性率を劇的に上昇させる」と結論付けている。\nおそらく、Simmons at al. (2011) が提起したものと同様の懸念に対応する形で、Journal of Accounting Research (JAR) は、2017年5月に開催された年次カンファレンスにおいて試験的な取り組みを実施した。  JARのウェブサイトによると、このカンファレンスでは「著者が登録ベースの編集プロセス（Registration-based Editorial Process, REP）を通じて作成した論文を発表した」。  このカンファレンスの目的は、会計研究においてREPを導入できるかを検証し、またその最適な実施方法を探ることであった。  カンファレンスで発表された論文は、2018年5月にその後出版された。\nBloomfield et al. (2018, p.317) によると、「REPでは、著者が自身の予測を検証するためにデータを収集・分析する計画を提案する。  ジャーナルは、有望な提案を一人または複数の査読者に送付し、修正を推奨する。  著者はこれに応じて提案を見直す機会を与えられ、しばしば複数回の修正を経た後、提案はリジェクトされるか、または‘原則的アクセプト’が与えられる。… これは、[その後の] 結果が彼らの予測を支持するかどうかに関わらず行われる。」\nBloomfield et al. (2018, p.317) は、REPと従来の編集プロセス（Traditional Editorial Process, TEP）を対比している。  TEPのもとでは、「著者はデータを収集し、分析し、原稿を執筆・修正したうえで、編集者に送る前に何度も見直しを行う」。  また、Bloomfield et al. (2018, p.317) は、「社会科学の査読付き論文のほぼすべてが … TEPのもとで出版されている」と指摘している。\nREPは、Simmons et al. (2011)によって特定されたものを含む疑問の余地のある研究手法を排除するために設計されている。  たとえば、p-hackingの一形態はHARKing（「結果がわかってから仮説を立てる」）である。  その極端な形態では、HARKingは「有意な」相関を探し、それを「予測」するための仮説を立てることを含む。  たとえば、Tyler Vigenが提供する見せかけの相関のウェブサイトを考えてみよう。  このサイトは、メイン州の離婚率とマーガリンの消費量の間の99.26%の相関や、アメリカの科学、宇宙、技術への支出と絞首、絞殺、窒息による自殺との間の99.79%の相関など、明らかにスパリアスな相関をいくつかリストしている。  これらの相関は、通常の人間は、これらの相関を説明する根底にある因果関係がないという強い事前の信念を持っているため、見せかけのものと見なされる。  これらは単なる偶然と見なされる。\nしかし、創造的な学者はおそらく、どんな相関も「予測」するためのストーリーを作り出すことができる。  たとえば、科学への支出を増やすことで、それが社会にとって重要であるという認識が高まるかもしれない。  しかし、科学に注目することは、アメリカが科学を含む多くの分野で相対的に衰退していることを浮き彫りにするだけである。  この衰退にもかかわらず、多くのアメリカ人はこれを乗り越えることができるが、他の人々はそれについて楽観的ではなく、その結果として極端な手段に出るかもしれない…。\nこれは明らかにばかげた推論であるが、もしいくつかの参考文献や洒落た用語を加えれば、おそらくいくつかの学術論文の仮説開発セクションとよく似たものになるだろう。\nBloomfield et al. (2018, p.326) は、2017年のJARカンファレンスからの「論文の結果の強さ」を彼らのセクション4.2で検討し、結論付けている。「… 7つの提案で行われた30の予測のうち、著者が報告した134の統計的検定のうち、少なくとも1つでp \\leq 0.05で支持されると数えられるものは10つある。  残りの20の予測は、報告された84の検定のいずれにもp \\leq 0.05で支持されていない。  全体的に、我々の分析は、論文がJARおよびその同僚誌に掲載された論文の中で一般的なよりもはるかに弱く、著者の予測を支持している」と述べている。[^2]",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#再現危機",
    "href": "chap19_Natural_experiments_revisited.html#再現危機",
    "title": "\n11  自然実験:再訪\n",
    "section": "",
    "text": "11.1.1 ディスカッション課題\n\n\nSimmons et al. (2011) は、Bloomfield et al. (2018, pp.318–319)で議論されたTEPの問題について、より詳細な検討を行っている。  Simmons et al.（2011）が研究した2つの実験は、実際の会計研究の進め方を代表しているといえるか。  アーカイバル・データを使用した実証会計研究には、どのような違いがあるのか。\n\n\nBloomfield et al.（2018, p.326）は、「Hail et al.（2018）は正式な仮説を提示していないため、[結果の] 集計から除外した」と述べている。  正式な仮説がないにもかかわらず、Hail et al. (2018) の提案を2017年の Journal of Accounting Research（JAR）カンファレンスに含めるのは妥当だったのだろうか？  REPは、正式な仮説を持たない論文にも適用可能なのか？  正式な仮説がないことは、Hail et al. (2018) が仮説を検証していないことを意味するのか？  最後の質問に対するあなたの答えは、Hail et al. (2018, p.650) が論文の Table 5 に関する結果をどのように議論しているかと整合しているだろうか？\n\n\nBloomfield et al. (2018)の分析によると、30の仮説に対して218のテストが行われ、異なる仮説には異なる数のテストが行われた。  以下の分析では、30の仮説があり、それぞれが7つのテストを行うと仮定する（合計210のテスト）。  この分析は、Bloomfield et al. (2018)が提供した「通常よりもはるかに弱い」という結論以外の結果の可能性の代替的な解釈を示唆しているだろうか。  set.seed()の値を変更すると、この分析の結果の傾向が変わるだろうか？  以下の分析をより確定的にするためには、どのようにすればよいだろうか？[^3]\n\n\nset.seed(2021)\nresults &lt;-\n  expand_grid(hypothesis = 1:30, test = 1:7) |&gt;\n  mutate(p = runif(nrow(pick(everything()))),\n         reject = p &lt; 0.05)\n\nresults |&gt; \n  group_by(hypothesis) |&gt;\n  summarize(reject_one = any(reject), .groups = \"drop\") |&gt;\n  count(reject_one)\n\n# A tibble: 2 × 2\n  reject_one     n\n  &lt;lgl&gt;      &lt;int&gt;\n1 FALSE         19\n2 TRUE          11\n\n\n\n\nBloomfield et al. (2018, p. 326) は、「いくつかの会議論文の改訂により、TEP（Traditional Empirical Paradigm）で発表された論文の大半と同程度の強さの結果を報告できるようになることは容易に想像できる」と主張している。  例えば、「Li and Sandino (2018) は、主要な仮説に対する統計的に有意な支持を得られなかった。   しかし、受理された提案書に含まれていた非公式な予測と整合的な計画的追加分析において、有意な結果を見出した。… [この証拠を踏まえると] 本号の研究が、TEP の下で発表された研究よりも予測に対する支持が弱いと結論づけるには至らない」（2018, p. 326）と述べている。  これらの結果は、TEP のもとで発表された研究の結果の強さについて何かを示唆していると解釈することはできるだろうか？\n\n\nREP（Registered Reports and Enhanced Transparency Paradigm）が会計研究の主要な研究パラダイムとなることは現実的だろうか。その発展にはどのような課題が伴うのか。\n\n\nBloomfield et al.（2018, p. 337）が実施した調査の回答者のコメントを以下に引用する。このコメントについて論じよ。回答者が「学習チャネル（learning channel）」についてどのような考えを持っていると考えられるか。また、REPがこのチャネルを閉ざしてしまうという指摘に賛同するか。\n\n\n「『帰無結果（null results）』が多く報告されることに驚きは感じない。それは自らの経験からも容易に予測できることだ。研究は反復的なプロセスであり、学習を伴うものだ。特に、非常に新規性の高い研究課題で、我々がまだほとんど知らない領域において、学習チャネルを閉ざすことで研究プロセスにおいて何か有益な発見が得られるとは思えない。」",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#the-reg-sho-experiment",
    "href": "chap19_Natural_experiments_revisited.html#the-reg-sho-experiment",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.2 The Reg SHO experiment",
    "text": "11.2 The Reg SHO experiment\n\n上記の議論で取り上げられた問題を実際の研究環境でよりよく理解するために、多くの研究の対象となってきたReg SHO実験に焦点を当てる。  2004年7月、SECは、株式市場における空売り活動を統制する規制であるReg SHOを採択した。  Reg SHOには、Russell 3000指数の株式が各取引所内で取引量によってランク付けされ、3つおきに1つがパイロット株式として指定されるパイロットプログラムが含まれていた。  2005年5月2日から2007年8月6日まで、パイロット株式に対する空売りは、取引所上場株式のチックテストやNASDAQナショナルマーケット株式のビッドテストを含む価格テストの対象外とされた。\n\n最初の命令で、SECは「パイロットは、[SEC]が空売り価格テストのない状況での取引行動を研究することを可能にする」と述べている。  SECの計画は、「価格テストが市場の質（ボラティリティや流動性を含む）に与える影響、価格変動が空売りによって引き起こされるかどうか、価格テストによって課されるコスト、および空売りポジションを確立するための代替手段の使用」などを調査することであった。\n\n\n11.2.1 SHOパイロットサンプル\n\nReg SHO実験における割当メカニズムは、自然実験の基準に照らしても、極めて透明性が高い。  しかしながら、処置群および対照群の企業を特定する際には慎重を要する。本節では、その手順を具体的に示すことが有益であると考える。  （コードの詳細を煩雑に感じる読者は、初回の読解時には 19.2.3 節 に進むことも可能である。  ただし、「初回の読解」と述べたのは、自然実験には微妙な論点が含まれており、本節がそれらを明確にする助けとなるためである。後の内容を読んだ後に本節に戻ることも有意義であろう。）\n\nSECのウェブサイトでは、Reg SHOパイロット企業の名称およびティッカーシンボルに関するデータが提供されている。  これらのデータは解析され、farrパッケージのsho_tickersデータセットとして組み込まれている。\n\nsho_tickers\n\n# A tibble: 986 × 2\n   ticker co_name                 \n   &lt;chr&gt;  &lt;chr&gt;                   \n 1 A      AGILENT TECHNOLOGIES INC\n 2 AAI    AIRTRAN HOLDINGS INC    \n 3 AAON   AAON INC                \n 4 ABC    AMERISOURCEBERGEN CORP  \n 5 ABCO   ADVISORY BOARD CO       \n 6 ABCW   ANCHOR BANCORP INC      \n 7 ABGX   ABGENIX INC             \n 8 ABK    AMBAC FINANCIAL GRP INC \n 9 ABMD   ABIOMED INC             \n10 ABR    ARBOR REALTY TRUST INC  \n# ℹ 976 more rows\n\n\n\nしかし、これらはあくまでパイロット企業に関するものであり、対照群の企業を特定するには別の情報源を利用する必要がある。  SECが処置群の銘柄リストを公表していながら、対照群に関する情報を提供していないことは、一見すると不可解に思われるかもしれない。  ひとつの説明として、特別な措置（すなわち、価格テストの撤廃）が処置群の銘柄に対してのみ適用されたため、対照群の銘柄については市場において通常の取引が継続されることになり、パイロット実験の実施にあたって対照群のリストを作成する必要がなかった可能性がある。  さらに、SEC自身が統計分析を行う際に対照群のリストを保有していたため、公表する必要がなかったとも考えられる。  幸いなことに、SECは対照群の銘柄を明示的には特定していないものの、それを特定するために十分な情報を提供しており、本節ではその方法を示す。\n\nまず、パイロット銘柄は Russell 3000 の構成銘柄から選定されたことが分かっている。これらの構成銘柄は、farrパッケージのsho_r3000データセットに含まれている。\n\nsho_r3000\n\n# A tibble: 3,000 × 2\n   russell_ticker russell_name            \n   &lt;chr&gt;          &lt;chr&gt;                   \n 1 A              AGILENT TECHNOLOGIES INC\n 2 AA             ALCOA INC               \n 3 AACC           ASSET ACCEPTANCE CAPITAL\n 4 AACE           ACE CASH EXPRESS INC    \n 5 AAI            AIRTRAN HOLDINGS INC    \n 6 AAON           AAON INC                \n 7 AAP            ADVANCE AUTO PARTS INC  \n 8 AAPL           APPLE COMPUTER INC      \n 9 ABAX           ABAXIS INC              \n10 ABC            AMERISOURCEBERGEN CORP  \n# ℹ 2,990 more rows\n\n\n\nRussell 3000には3,000銘柄が含まれているが、SECおよびBlack et al. (2019)によると、パイロットサンプルの構築にあたり、SECは以下の基準で一部の銘柄を除外している。 まず、2004年6月25日時点でNasdaq National Market, NYSE, AMEX に上場していなかった32銘柄を除外している。 SECによれば、これらの銘柄は「現時点で空売りに対する価格テストの適用対象外である」ため、パイロットサンプルから除外された。  さらに、2004年4月30日以降に IPO または スピンオフ により新規上場した 12銘柄 も除外された。 また、Black et al. (2019) によると、2004年6月25日以降、SECがサンプルを構築する2004年6月28日までの間に取引が停止した2銘柄 も除外されている。  これらの基準に該当する銘柄を特定するためには、CRSP のデータを活用する必要がある。ただし、その前にRussell 3000のデータとCRSPを統合し、各銘柄に対応する適切な PERMNO を特定する必要がある。  そのために、以下のCRSPの5つのデータテーブルを用いる。\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nmse &lt;- tbl(db, Id(schema = \"crsp\", table = \"mse\"))\nmsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"msf\"))\nstocknames &lt;- tbl(db, Id(schema = \"crsp\", table = \"stocknames\"))\ndseexchdates &lt;- tbl(db, Id(schema = \"crsp\", table = \"dseexchdates\"))\nccmxpf_lnkhist &lt;- tbl(db, Id(schema = \"crsp\", table = \"ccmxpf_lnkhist\"))\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nmse &lt;- load_parquet(db, schema = \"crsp\", table = \"mse\")\nmsf &lt;- load_parquet(db, schema = \"crsp\", table = \"msf\")\nstocknames &lt;- load_parquet(db, schema = \"crsp\", table = \"stocknames\")\ndseexchdates &lt;- load_parquet(db, schema = \"crsp\", table = \"dseexchdates\")\nccmxpf_lnkhist &lt;- load_parquet(db, schema = \"crsp\", table = \"ccmxpf_lnkhist\")\n\n\n\n\n\nラッセル3000のサンプルに含まれる一部のティッカーは、ティッカーに株式のクラスを付加している。このようなケースを検出するには、正規表現（regex）を使用してドット（.）を探す。ただし、ドットは正規表現において特別な意味を持つため、バックスラッシュ（\\）を使ってエスケープする必要がある。Rの文字列内では、バックスラッシュ自体にも特別な意味があるため、入力通りにバックスラッシュを表現するには、さらにバックスラッシュをエスケープしなければならない。したがって、ドットを検出するには、正規表現として “\\\\.” を使用する。\n\nsho_r3000 |&gt; \n  filter(str_detect(russell_ticker, \"\\\\.\"))\n\n# A tibble: 12 × 2\n   russell_ticker russell_name           \n   &lt;chr&gt;          &lt;chr&gt;                  \n 1 AGR.B          AGERE SYSTEMS INC      \n 2 BF.B           BROWN FORMAN CORP      \n 3 CRD.B          CRAWFORD & CO          \n 4 FCE.A          FOREST CITY ENTRPRS    \n 5 HUB.B          HUBBELL INC            \n 6 JW.A           WILEY JOHN & SONS INC  \n 7 KV.A           K V PHARMACEUTICAL CO  \n 8 MOG.A          MOOG INC               \n 9 NMG.A          NEIMAN MARCUS GROUP INC\n10 SQA.A          SEQUA CORPORATION      \n11 TRY.B          TRIARC COS INC         \n12 VIA.B          VIACOM INC             \n\n\n\nこのような場合、CRSPは異なるアプローチを取る。  例えば、ラッセル3000のサンプルでは AGR.B となっているが、CRSPではティッカーが AGR、株式クラス（shrcls）が B となっている。\n\nもう一つの問題は、ラッセル3000のデータに含まれる一部のティッカーが、CRSPでは4文字のティッカーとして表示されるのに対し、末尾にEが付加されているケースがあることである。\n\nsho_r3000 |&gt; \n  filter(str_length(russell_ticker) == 5,\n         str_sub(russell_ticker, 5, 5) == \"E\")\n\n# A tibble: 4 × 2\n  russell_ticker russell_name      \n  &lt;chr&gt;          &lt;chr&gt;             \n1 CVNSE          COVANSYS CORP     \n2 SONSE          SONUS NETWORKS INC\n3 SPSSE          SPSS INC          \n4 VXGNE          VAXGEN INC        \n\n\n\n好奇心のある読者は、これら2つのティッカーの問題をどのように特定したのか、また、それらがデータ内の問題を網羅しているとどのように判断したのか疑問に思うかもしれない。これらの問いについては、本節の演習で詳しく探求する。\n\nこれらの問題に対応するために、2つの関数を作成する。1つは、ティッカーをCRSPと照合できるように“クリーン”にするclean_ticker()であり、もう1つは、ラッセル3000データ内で指定されている株式クラス（もしあれば）を抽出するget_shrcls()である。\n\nどちらの関数も、正規表現を用いて、テキストがドット（\\\\.）の後にAまたはBで終わるケースを検出する（正規表現では[AB]$）。 この表現では、キャプチャ用の括弧( ( と ) )を使用し、文字列の先頭からドットまでの部分を (^(.*)) で、末尾のAまたはBを([AB])$でキャプチャする。\n\nregex &lt;- \"^(.*)\\\\.([AB])$\"\n\n\nclean_ticker()関数は、case_when()を使用して、5文字のティッカーの末尾にEが付加されているケースを処理し、次にregexに一致するケースから“クリーン”なティッカー（最初にキャプチャされたテキスト）を抽出する。最後に、それ以外のケースでは元のティッカーを返す。\n\nclean_ticker &lt;- function(x) {\n  case_when(str_length(x) == 5 & str_sub(x, 5, 5) == \"E\" ~ str_sub(x, 1, 4),\n            str_detect(x, regex) ~ str_replace(x, regex, \"\\\\1\"),\n            .default = x)\n}\n\n\nget_shrcls()関数は、正規表現から2番目のキャプチャグループを抽出する（str_match()が返す最初の値は完全な一致、2番目の値は最初のキャプチャグループであるため、2番目のキャプチャグループを取得するために[, 3]を使用する）。\n\nget_shrcls &lt;- function(x) {\n  str_match(x, regex)[, 3]\n}\n\n\nclean_ticker()とget_shrcls()を使用して、sho_r3000_tickersを構築することができる。\n\nsho_r3000_tickers &lt;-\n  sho_r3000 |&gt;\n  select(russell_ticker, russell_name) |&gt;\n  mutate(ticker = clean_ticker(russell_ticker),\n         shrcls = get_shrcls(russell_ticker))\n\nsho_r3000_tickers |&gt;\n  filter(russell_ticker != ticker)\n\n# A tibble: 16 × 4\n   russell_ticker russell_name            ticker shrcls\n   &lt;chr&gt;          &lt;chr&gt;                   &lt;chr&gt;  &lt;chr&gt; \n 1 AGR.B          AGERE SYSTEMS INC       AGR    B     \n 2 BF.B           BROWN FORMAN CORP       BF     B     \n 3 CRD.B          CRAWFORD & CO           CRD    B     \n 4 CVNSE          COVANSYS CORP           CVNS   &lt;NA&gt;  \n 5 FCE.A          FOREST CITY ENTRPRS     FCE    A     \n 6 HUB.B          HUBBELL INC             HUB    B     \n 7 JW.A           WILEY JOHN & SONS INC   JW     A     \n 8 KV.A           K V PHARMACEUTICAL CO   KV     A     \n 9 MOG.A          MOOG INC                MOG    A     \n10 NMG.A          NEIMAN MARCUS GROUP INC NMG    A     \n11 SONSE          SONUS NETWORKS INC      SONS   &lt;NA&gt;  \n12 SPSSE          SPSS INC                SPSS   &lt;NA&gt;  \n13 SQA.A          SEQUA CORPORATION       SQA    A     \n14 TRY.B          TRIARC COS INC          TRY    B     \n15 VXGNE          VAXGEN INC              VXGN   &lt;NA&gt;  \n16 VIA.B          VIACOM INC              VIA    B     \n\n\n\nこれで“クリーン”なティッカーを取得したので、CRSPと結合することができる。以下のコードは2つのステップで進行する。まず、SECが使用したRussell 3000が作成された日である2004-06-25に適用されるpermno、ticker、shrclsの値を含むcrsp_sampleを作成する。\n\ncrsp_sample &lt;-\n  stocknames |&gt;\n  mutate(test_date = as.Date(\"2004-06-25\")) |&gt;\n  filter(test_date &gt;= namedt, test_date &lt;= nameenddt) |&gt;\n  select(permno, permco, ticker, shrcls) |&gt;\n  distinct() |&gt;\n  collect()\n\n\n次に、tickerを使用してsho_r3000_tickersとcrsp_sampleを結合し、filter()を使用して、SECが提供したティッカーに株式クラスが指定されている場合、その株式クラスに一致するCRSPの1行を保持し、SECが提供したティッカーに株式クラスが指定されていない場合はすべての行を保持する。\n\nsho_r3000_merged &lt;-\n  sho_r3000_tickers |&gt;\n  inner_join(crsp_sample, by = \"ticker\", suffix = c(\"\", \"_crsp\")) |&gt;\n  filter(shrcls == shrcls_crsp | is.na(shrcls)) |&gt;\n  select(russell_ticker, permco, permno)\n\n\n残念ながら、このアプローチでは、いくつかのティッカーが複数のPERMNO値にマッチする結果となる。\n\nsho_r3000_merged |&gt;\n  group_by(russell_ticker) |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\n\n各ケースでは、同じ企業（permco値）に対して複数の証券（permno値）が存在するようである。SECが使用したRussell 3000指数に最も含まれていると考えられる証券を選択するために、2004年6月の最も大きな取引量を持つ証券を保持することにする。取引量に関するデータをtrading_volデータフレームに収集する。\n\ntrading_vol &lt;-\n  msf |&gt;\n  filter(date == \"2004-06-30\") |&gt; \n  mutate(dollar_vol = coalesce(abs(prc) * vol, 0)) |&gt; \n  select(permno, dollar_vol) |&gt;\n  collect()\n\n\nこれで、各ティッカーに対して最も取引量の多いpermno値を含む新しいバージョンのsho_r3000_mergedテーブルを作成することができる。\n\nsho_r3000_merged &lt;-\n  sho_r3000_tickers |&gt;\n  inner_join(crsp_sample, by = \"ticker\", suffix = c(\"\", \"_crsp\")) |&gt;\n  filter(is.na(shrcls) | shrcls == shrcls_crsp) |&gt;\n  inner_join(trading_vol, by = \"permno\") |&gt;\n  group_by(russell_ticker) |&gt;\n  filter(dollar_vol == max(dollar_vol, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(russell_ticker, permno)\n\n\nBlack et al. (2019) は、「CRSPの月次株式ファイルからの歴史的な取引所コード（exchcd）とNASDAQナショナルマーケットインジケータ（nmsind）を使用して、Nasdaq National Market、NYSE、AMEXに上場していない32銘柄を特定した」と述べている（実際には、これらの32銘柄は小規模なNasdaq上場銘柄である）。  しかし、exchcdとnmsindは、私たちが使用しているcrsp.msfファイルには含まれていない。Black et al. (2019) は、おそらくWRDSが提供するWebインターフェースから取得したCRSPの月次株式ファイルを使用しており、このファイルは他のテーブルからのデータを結合することが多い。\n\n幸いなことに、CRSPの月次イベントファイル（crsp.mse）からnmsindを取得することができる。  このファイルには、上場廃止イベント、配当などの配布、NASDAQ情報の変更（nmsindなど）、名前の変更に関する情報が含まれている。nmsindに関するデータは、2004-06-28以前のcrsp.mseの最新の観測値を取得し、NASDAQステータスに関連するイベント（event == \"NASDIN\"）を取得することで取得できる。\n\nnmsind_data &lt;-\n  mse |&gt; \n  filter(date &lt;= \"2004-06-28\", event == \"NASDIN\") |&gt;\n  group_by(permno) |&gt;\n  filter(date == max(date, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(permno, date, nmsind) |&gt;\n  collect()\n\n\nexchcdは、CRSPの株式名ファイル（crsp.stocknames）から取得することができる。再度、2004-06-28に適用される値を取得する。1\n\nexchcd_data &lt;-\n  stocknames |&gt;\n  filter(exchcd &gt; 0) |&gt;\n  mutate(test_date = as.Date(\"2004-06-28\")) |&gt;\n  filter(between(test_date, namedt, nameenddt)) |&gt;\n  select(permno, exchcd) |&gt;\n  distinct() |&gt;\n  collect()\n\n\nSECのウェブサイトによると、「2004年4月30日以降に開始された新規株式公開（IPO）を行った発行体も除外した」という。  Black et al. (2019) に従い、これらの企業を特定するためにCRSPデータを使用する。  具体的には、テーブルcrsp.dseexchdatesには変数begexchdateが含まれている。\n\nipo_dates &lt;-\n  dseexchdates |&gt; \n  distinct(permno, begexchdate) |&gt; \n  collect()\n\n\n最後に、SECがパイロットプログラムのサンプルを最終的に確定したと思われる2004-06-28より前に上場廃止された銘柄が、SECがおそらく使用したRussell 3000ファイルに含まれていたようである。これらの企業を特定するために、再度crsp.mseを使用する。\n\nrecent_delistings &lt;-\n  mse |&gt;\n  filter(event == \"DELIST\", \n         between(date, \"2004-06-25\", \"2004-06-28\")) |&gt;\n  rename(delist_date = date) |&gt;\n  select(permno, delist_date) |&gt;\n  collect()\n\n\nこれで、これらの要素を組み合わせて、上記で説明した3つの除外基準に対応する変数nasdaq_small、recent_listing、delistedを作成する。\n\nsho_r3000_permno &lt;-\n  sho_r3000_merged |&gt;\n  left_join(nmsind_data, by = \"permno\") |&gt;\n  left_join(exchcd_data, by = \"permno\") |&gt;\n  left_join(ipo_dates,   by = \"permno\") |&gt;\n  left_join(recent_delistings, by = \"permno\") |&gt;\n  mutate(nasdaq_small = coalesce(nmsind == 3 & exchcd == 3, FALSE), \n         recent_listing = begexchdate &gt; \"2004-04-30\",\n         delisted = !is.na(delist_date),\n         keep = !nasdaq_small & !recent_listing & !delisted)\n\n\n表19.1に示すように、パイロット指標を作成するために結合できる2954銘柄の最終サンプルが得られた。\n\nsho_r3000_permno |&gt; \n  count(nasdaq_small, recent_listing, delisted, keep)\n\n\nsho_r3000_sample &lt;-\n  sho_r3000_permno |&gt;\n  filter(keep) |&gt;\n  rename(ticker = russell_ticker) |&gt;\n  left_join(sho_tickers, by = \"ticker\") |&gt;\n  mutate(pilot = !is.na(co_name)) |&gt;\n  select(ticker, permno, pilot)\n\n\n表19.1に示すように、sho_r3000_sampleの処置群と対照群の企業数は、Black et al. (2019, p. 42)で提供された数値と完全に一致している。\n\nsho_r3000_sample |&gt;\n  count(pilot)\n\n\n最後に、これらのデータをCompustatのデータとリンクしたいと考えている。つまり、これらの観測値をGVKEYとリンクする必要がある。  これには、後の分析で使用できるサンプルであるsho_r3000_gvkeysを生成するために、ccm_link（第7章で使用および議論された）を使用する。\n\n最後に、これらのデータをCompustatのデータとリンクしたいと考えている。つまり、これらの観測値をGVKEYとリンクする必要がある。  これには、後の分析で使用できるサンプルであるsho_r3000_gvkeysを生成するために、ccm_link（第7章で使用および議論された）を使用する。\n\nccm_link &lt;-\n  ccmxpf_lnkhist |&gt;\n  filter(linktype %in% c(\"LC\", \"LU\", \"LS\"),\n         linkprim %in% c(\"C\", \"P\")) |&gt;\n  rename(permno = lpermno) |&gt;\n  mutate(linkenddt = coalesce(linkenddt, max(linkenddt, na.rm = TRUE))) |&gt;\n  select(gvkey, permno, linkdt, linkenddt)\n\n\n単一の「テスト日」に焦点を当てるため、最終的なリンクテーブルにはgvkeyとpermnoの2つの変数のみが含まれる。2\n\ngvkeys &lt;-\n  ccm_link |&gt;\n  mutate(test_date = as.Date(\"2004-06-28\")) |&gt;\n  filter(between(test_date, linkdt, linkenddt)) |&gt;\n  select(gvkey, permno) |&gt;\n  collect()\n\n\n最後に、sho_r3000_sampleにgvkeyを追加してsho_r3000_gvkeysを作成することができる。\n\nsho_r3000_gvkeys &lt;-\n  sho_r3000_sample |&gt;\n  inner_join(gvkeys, by = \"permno\")\n\nsho_r3000_gvkeys\n\n\npilot指標変数を構築する際の潜在的な問題をよりよく理解するために、上記のアプローチとFang et al. (2016)のアプローチを比較することが有益である。  Fang et al. (2016)のようにsho_dataを構築するために、farrパッケージからfhk_pilotを使用する。以下の演習で、sho_r3000_sampleとsho_r3000_gvkeysをsho_dataと比較する。\n\nsho_data &lt;- \n  fhk_pilot |&gt;\n  select(gvkey, pilot) |&gt;\n  distinct() |&gt;\n  group_by(gvkey) |&gt;\n  filter(n() == 1) |&gt;\n  ungroup() |&gt;\n  inner_join(fhk_pilot, by = c(\"gvkey\", \"pilot\")) \n\n\n11.2.2 Exercises\n\n\n以下のコードを実行する前に、上記の出力からこのクエリが何行を返すかを判断できますか？  このコードは何をしているのか？  このようなコードは、上記のサンプルを作成する過程でどの段階で使用されたか？  なぜこのようなコードは上記に含まれていないのか？\n\n\nsho_r3000 |&gt;\n  anti_join(crsp_sample, join_by(russell_ticker == ticker)) |&gt;\n  collect()\n\n\n\n\nfhk_pilotのtickerとpilotの値に焦点を当てると、fhk_pilotとsho_r3000_sampleの間にどのような違いがあるかを観察しなさい。  これらの相違の根本的な原因は何だと思うか？\n\n\n以下の観察は何を表しているか？  この出力からいくつかの観察を選択し、これらがsho_r3000_sampleまたはfhk_pilotに問題を示しているかどうかを調べなさい。\n\n\nsho_r3000_sample |&gt;\n  inner_join(fhk_pilot, by = \"ticker\", suffix = c(\"_ours\", \"_fhk\")) |&gt;\n  filter(permno_ours != permno_fhk)\n\n\n\n\npilot指標を構築する際、FHKは、指標の値が複数ある場合（gvkey値）にはケースを省略している。  質問は：これらの企業は誰か？  これらの企業に対してpilotに複数の値があるのはなぜか？  そして、これらを省略することは理にかなっているか？  （ヒント：fhk_pilotの重複を特定し、これらの企業についてsho_r3000_gvkeysを比較する。）\n\n\n以下のコードの出力には何の問題が含まれているか？  この問題をどのように修正できるか？  この問題の修正が回帰結果に大きな影響を与えると予想されるか？その理由は何か？\n\n\nsho_data |&gt; \n  count(gvkey, ticker) |&gt; \n  arrange(desc(n))\n\n\n\n11.2.3 Reg SHOの初期の研究\n\nReg SHOの影響に関する最初の研究は、SECの経済分析局によって行われた。  SECの研究は、「パイロットがショートセール、流動性、ボラティリティ、市場効率、および極端な価格変動に与える影響」を調査している[p. 86]。\n\n2007年のSECの調査の著者らは、「価格規制は、総取引量に対する空売りの実行量を減少させることを確認しており、これは価格規制が実際に空売りの制約として機能していることを示している」と述べている。  しかし、どの市場においても、パイロット銘柄とコントロール銘柄の間で空売り残高に有意な差は見られなかった。  また、株式の空売りに対する価格規制がオプション取引や建玉に影響を与える証拠も見つからなかった。  さらに、気配値の厚み（quoted depth）は価格規制によって増加するものの、実現流動性には影響を与えないことが分かった。  加えて、価格規制が日中の短期的なリターンの変動を抑える証拠はあるものの、平均的に測定すると、日次リターンの変動性には影響を与えないようであることが示された。\n\n\nSECの研究者は次のように結論づけている。「パイロット・プログラムの開始に対する価格の反応に基づくと、ティック・テストが株価を歪めるという証拠は限定的である。パイロット・プログラムが発効した当日、パイロット・サンプルに含まれる上場株は、対照サンプルに含まれる上場株と比べて約24ベーシスポイント低いパフォーマンスを示した。しかし、パイロット株と対照株のリターンは、プログラム開始から最初の6か月間で類似していた。」\n\n要約すると、SECは価格テストの免除が市場の関心のある結果に対して比較的限定的な影響しか与えず、いくつかの結果には明らかな影響がなかったと判断したと言える。\n\n\nAlexander and Peterson (2008, 84) は、「価格テストがトレーダーの行動や市場の質にどのような影響を与えるかを調査しており、これはSECがこれらのテストを評価する際に関心を持っている領域である」と述べている。Alexander and Peterson (2008, 86) によれば、NYSEのパイロット株はスプレッドが類似しているものの、取引サイズが小さく、ショート取引が増え、ショート取引量が多く、アスクの深さが小さくなっているとしている。 Nasdaqに関して、Alexander and Peterson (2008, 86) は「撤廃されたビッド・テストの影響は比較的軽微である」と指摘している。\n\nDiether, Lee, and Werner (2009, 37) は、「NYSEおよびNasdaqに上場するパイロット株では空売り活動が増加するものの、日次レベルでのリターンやボラティリティには影響がない」と述べている。\n\n\n11.2.4 ディスカッション課題と練習問題\n\n\n\n\n先ほど、ランダム化比較試験（RCT）の特徴の一つとして、「提案された分析が事前に指定される」ことを挙げた。これは登録報告（registered reports）のプロセスと同様である。 SECが2007年の論文で登録報告を使用しなかったのはなぜだと思うか。 また、SECの分析が登録報告プロセスの一環として実施されていれば、より信頼性が高まったと思うか。その理由も含めて説明せよ。\n\n\n\nAlexander and Peterson (2008) の結果がp-hackedされた可能性について懸念があるか。この点に関して懸念を増大させる要因と減少させる要因は何か。\n\n\n\nDiether, Lee, and Werner (2009) の検証可能な仮説というセクションに見られる仮説をHARKingに関する懸念に特に敏感に評価せよ。このような仮説を評価する際にはどのような専門知識が必要か。\n\n\nSECは、Alexander and Peterson (2008) や@Diether:2009vu のような外部研究チームに公開された登録報告プロセスの一環としてReg SHOを実施することができたか。そのようなプロセスはどのように実施されたか。そのようなプロセスはどのような課題に直面するか。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#自然実験の分析",
    "href": "chap19_Natural_experiments_revisited.html#自然実験の分析",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.3 自然実験の分析",
    "text": "11.3 自然実験の分析\n\nAlexander and Peterson (2008) と Diether, Lee, and Werner (2009) は、第3章で見た因果効果の差分の差推定量（“DiD”）を使用している。DiDの典型的なアプローチは、以下の形式の回帰を推定することを含む。\n\n\\begin{aligned}\nY_{it} = &\\beta_0 + \\beta_1 \\times POST_t + \\beta_2 \\times TREAT _i + \\\\\n&\\beta _3 \\times POST_t \\times TREAT_i + \\varepsilon_{it}\n\\end{aligned}\n\n\nこのモデルで、推定された処置効果は、推定された係数 \\hat{\\beta} _3 によって与えられる。\n\nDiDは、経済学や関連分野の研究者の間で明らかに人気があるが、すべての実験設定で最良の選択肢であるとは明らかではなく、信頼性のある代替手法が存在することに注意することが重要である。\n\n別のアプローチは、サンプルを処置後の期間に限定し、以下の回帰を推定することである。\n\nY_{it} = \\beta_0 + \\beta_1 \\times TREAT_i + \\varepsilon_{it}\n\n\nこのモデルで、推定された処置効果は、推定された係数 \\hat{\\beta}_1 によって与えられる。  このアプローチは、通常RCTとして実施される薬物試験で一般的である。  たとえば、paxlovid試験では、「参加者は1:1で無作為に割り付けられ、半分はpaxlovidを、もう半分は5日間12時間ごとに経口でプラセボを受け取った。」  「症状発現後3日以内に治療を受けた患者のうち、paxlovidを受け取った患者の0.8%（3/389）が入院し、死亡者はいなかった。」  「対照群の患者のうち、7%（27/385）が入院し、7人が死亡した。」(Mahase, 2021, p. 1)。  病院入院の結果については、差分の差分（difference-in-differences）分析に過去の入院率を組み込むことも可能であった。しかし、これはある期間の入院率がその後の入院率を高い精度で予測できる場合にのみ意味をなす。[^8]\n\nもう1つのアプローチは、処置前の結果変数の値を制御変数として含めることである。\n\nY_{it} = \\beta_0 + \\beta_1 \\times TREAT _i + \\beta_2 \\times Y_{i,t-1} + \\varepsilon_{it}\n\n\nこれらのアプローチを評価するために、シミュレーション分析を使用することができる。以下の分析は、Frison and Pocock (1992)に触発されたもので、彼らは（医学的な）設定に適した異なるデータの仮定を使用し、数学的な分析に焦点を当てている。\n\nFrison and Pocock (1992) は、観測間の時間に依存しない、ある単位（たとえば、患者）の結果変数の測定値における相関の程度を仮定している。  多くのビジネス設定では、ある単位（たとえば、企業）の結果変数の測定値における相関が、観測間の時間が経つにつれて薄れるというモデルがより妥当である。  このような考え方に沿って、以下のget_outcomes()を作成して、処置がない場合の結果のデータを生成する。  具体的には、処置効果や期間効果がない場合、対象の結果は、get_outcomes()に埋め込まれた自己回帰過程に従うと仮定する。この過程には、主要なパラメータ \\rho (rho) が含まれている。3\n\nget_outcomes &lt;- function(\n  rho = 0, \n  periods = 7\n  ) {\n  e &lt;- rnorm(periods)\n  y &lt;- rep(NA, periods)\n  y[1] &lt;- e[1]\n  for (i in 2:periods) {\n    y[i] &lt;- rho * y[i - 1] + e[i]\n  }\n  tibble(t = 1:periods, y = y)\n}\n\n\n次のget_sample()関数は、rho、periods（各企業について観測される期間の数）、およびeffect（yに対する処置の効果の基礎となるサイズ）の与えられた値に対して、n企業のためにget_outcomes()を使用する。  ここでは、処置はサンプルの半分の企業にランダムに割り当てられ、treatとpostの両方が真の場合にyに効果が追加される。  また、各期間に対して時間特有の効果（t_effect）を追加し、すべての観測に共通である（DiDの使用の一般的な正当化の理由の1つは、このような期間効果が存在することである）。\n\nget_sample &lt;- function(n = 100, rho = 0, periods = 7, effect = 0) {\n  treat &lt;- sample(1:n, size = floor(n / 2), replace = FALSE)\n  \n  t_effects &lt;- tibble(t = 1:periods, t_effect = rnorm(periods))\n  \n  f &lt;- function(x) tibble(id = x, get_outcomes(rho = rho, \n                                               periods = periods))\n  df &lt;- \n    map(1:n, f) |&gt;\n    list_rbind() |&gt; \n    inner_join(t_effects, by = \"t\") |&gt;\n    mutate(treat = id %in% treat,\n           post = t &gt; periods / 2,\n           y = y + if_else(treat & post, effect, 0) + t_effect) |&gt;\n    select(-t_effect)\n}\n\n\n以下のest_effect()関数は、与えられたデータセットにいくつかの推定量を適用し、各推定量の推定処置効果を返す。  考慮する推定量は以下の通りである（ラベルPOST、CHANGE、ANCOVAはFrison and Pocock, 1992から来ている）。\n\n\n\nDiDは、処置指標treatと処置後指標postを交互作用させたものを回帰することによって推定される差分の差推定量であり、treatとpostの主効果を自動的に含む[lm()](https://rdrr.io/r/stats/lm.html)関数によって推定される。Equation 19.1のようなものである。 \n\n\nPOSTは、yをtreatに対してOLS回帰するが、サンプルを処置後の観測に制限したものであり、Equation 19.2のようなものである。 \n\n\nCHANGEは、アウトカムの変化をtreatに対してOLS回帰するものである。アウトカムの変化（y_change）は、各単位について処置後のアウトカム値（y_post）の平均から処置前のアウトカム値（y_pre）の平均を引いたものである。 \n\n\nANCOVAは、y_postをtreatとy_preに対して回帰するものであり、Equation 19.3のようなものである。\n\n\nest_effect &lt;- function(df) {\n  \n  fm_DiD &lt;- lm(y ~ treat * post, data = df)\n  \n  df_POST &lt;- \n    df |&gt; \n    filter(post) |&gt;\n    group_by(id, treat) |&gt;\n    summarize(y = mean(y), .groups = \"drop\")\n    \n  fm_POST &lt;- lm(y ~ treat, data = df_POST)\n  \n  df_CHANGE &lt;- \n    df |&gt; \n    group_by(id, treat, post) |&gt;\n    summarize(y = mean(y), .groups = \"drop\") |&gt;\n    pivot_wider(names_from = \"post\", values_from = \"y\") |&gt;\n    rename(y_pre = `FALSE`,\n           y_post = `TRUE`) |&gt;\n    mutate(y_change = y_post - y_pre) \n  \n  fm_CHANGE &lt;- lm(I(y_post - y_pre) ~ treat, data = df_CHANGE)\n  fm_ANCOVA &lt;- lm(y_post ~ y_pre + treat, data = df_CHANGE)\n  \n  tibble(est_DiD = fm_DiD$coefficients[[\"treatTRUE:postTRUE\"]],\n         est_POST = fm_POST$coefficients[[\"treatTRUE\"]], \n         est_CHANGE = fm_CHANGE$coefficients[[\"treatTRUE\"]], \n         est_ANCOVA = fm_ANCOVA$coefficients[[\"treatTRUE\"]])\n}\n\n\nこのrun_sim()関数は、与えられたパラメータ値に対してget_sample()を呼び出してデータセットを作成し、そのデータセットにest_effect()を適用した結果を含むデータフレームを返す。\n\nrun_sim &lt;- function(i, n = 100, rho = 0, periods = 7, effect = 0) {\n  df &lt;- get_sample(n = n, rho = rho, periods = periods, effect = effect)\n  tibble(i = i, est_effect(df))\n}\n\n\neffectとrhoのさまざまな値に対してシミュレーションを実行するために、effectが0から1まで、\\rho \\in \\{ 0, 0.18, 0.36, 0.54, 0.72, 0.9 \\} であるデータフレーム（params）を作成する。\n\nrhos &lt;- seq(from = 0, to = 0.9, length.out = 6) \neffects &lt;- seq(from = 0, to = 1, length.out = 5)\nparams &lt;- expand_grid(effect = effects, rho = rhos)\n\n\n以下のrun_sim_n()関数は、与えられたeffectとrhoの値に対して1000回のシミュレーションを実行し、結果を含むデータフレームを返す。\n\nrun_sim_n &lt;- function(effect, rho, ...) {\n  n_sims &lt;- 1000\n  set.seed(2021)\n  \n  res &lt;- \n    1:n_sims |&gt;\n    map(\\(x) run_sim(x, rho = rho, effect = effect)) |&gt;\n    list_rbind()\n  \n  tibble(effect, rho, res)\n                                    \n}\n\n\n\n\n\n\n\n\n次のコードは実行に数分かかる。pmap()の代わりにfurrrパッケージのfuture_pmapを使用すると、シミュレーションを実行するために必要な時間が大幅に短縮される。  幸いなことに、後続の演習では、このコードのいずれかを実行する必要はない。したがって、時間がある場合やresultsを直接調べたい場合にのみ実行すること。\n\n\n\n\nplan(multisession)\n\nresults &lt;-\n  params |&gt; \n  future_pmap(run_sim_n, \n              .options = furrr_options(seed = 2021)) |&gt; \n  list_rbind() |&gt; \n  system_time()\n\n\nresultsが手に入ったので、いくつかの分析を行うことができる。  最初に注意すべきことは、est_CHANGEがest_DiDと同等であることである。これら2つの方法について、すべての推定値がお互いに丸め誤差の範囲内にあるためである。\n\nresults |&gt; \n  filter(abs(est_DiD - est_CHANGE) &gt; 0.00001) |&gt; \n  nrow()\n\n\nしたがって、以降の分析では、DiDというラベルを使用する。\n\n2つ目に確認することは、各方法が因果効果のバイアスのない推定値を提供するかどうかである。  Figure 19.1は、すべての3つの方法について、推定値が因果効果の真の値に非常に近いことを示している。\n\nresults |&gt;\n  pivot_longer(starts_with(\"est\"), \n               names_to = \"method\", values_to = \"est\") |&gt;\n  mutate(method = str_replace(method, \"^est.(.*)$\", \"\\\\1\")) |&gt;\n  group_by(rho, method) |&gt;\n  summarize(bias = mean(est - effect), .groups = \"drop\") |&gt;\n  filter(method != \"CHANGE\") |&gt;\n  ggplot(aes(x = rho, y = bias, \n             colour = method, linetype = method)) +\n  geom_line() +\n  ylim(-0.1, 0.1)\n\n\nこの設定において、どの推定量にも明らかなバイアスがないことを確認した後、次に各方法の実証的標準誤差を考慮する。  真の効果の各値で本質的に同一のプロットを得るため、以下の分析ではeffect == 0.5に焦点を当てる。  ここでは、推定された因果効果のmethod列とest列を持つデータを再配置する。  次に、各methodとrhoの値について、estの標準偏差を計算し、求めている実証的標準誤差を得る。  最後に、Figure 19.2に各rhoの値をプロットする。\n\nresults |&gt;\n  filter(effect == 0.5) |&gt;\n  pivot_longer(\n    starts_with(\"est\"), # estで始まる列を対象\n    names_to = \"method\",\n    values_to = \"est\"\n    ) |&gt;\n  mutate(\n    method = str_replace(method, \"^est.(.*)$\", \"\\\\1\")\n    ) |&gt;\n  filter(method != \"CHANGE\") |&gt; # CHANGEは除外\n  group_by(method, rho) |&gt; # methodとrhoでグループ化\n  summarize(\n    se = sd(est), .groups = \"drop\" # estの標準偏差を計算\n    ) |&gt;\n  ggplot() + \n    aes(x = rho, y = se, colour = method, linetype = method) +\n    geom_line()\n\nFrom the above, we can see that for low values of \\rho , subtracting pre-treatment outcome values adds noise to our estimation of treatment effects. We actually have lower standard errors when we throw away the pre-treatment data and just compare post-treatment outcomes. But for higher levels of \\rho , we see that DiD outperforms POST; by subtracting pre-treatment outcome values, we get a more precise estimate of the treatment effect. However, we see that both DiD and POST are generally outperformed by ANCOVA, which in effect allows for a flexible, data-driven relation between pre- and post-treatment outcome values.\nIn short, notwithstanding its popularity, it is far from clear that DiD is the best approach to use for all analyses of causal effects based on experimental data. Even in the context of the Reg SHO experiment, the appropriate method may depend on the outcome of interest. For a persistent outcome, DiD may be better than POST, but for a less persistent outcome, POST may be better than DiD. And ANCOVA may be a better choice than either POST or DiD unless there are strong a priori reasons to believe that DiD or POST is more appropriate (and such reasons seem more likely to hold for POST than for DiD).",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#valuating-natural-experiments",
    "href": "chap19_Natural_experiments_revisited.html#valuating-natural-experiments",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.4 valuating natural experiments",
    "text": "11.4 valuating natural experiments\nBecause of the plausibly random assignment mechanism used by the SEC, Reg SHO provides a very credible natural experiment. However, in many claimed natural experiments, it will be “nature” who is assigning treatment. In Michels (2017), it was literally nature doing the assignment through the timing of natural disasters. While natural disasters are clearly not completely random, as hurricanes are more likely to strike certain locations at certain times of year, this is not essential for the natural experiment to provide a setting from which causal inferences can be credibly drawn. What is necessary in Michels (2017) is that treatment assignment is as-if random with regard to the timing of the natural disaster before or after the end of the fiscal period and discussion questions in Chapter 17 explored these issues.\nMore often in claimed natural experiments, it will be economic actors or forces, rather than nature, assigning treatment. While such economic actors and forces are unlikely to act at random, again the critical question is whether treatment assignment is as-if random. To better understand the issues, we consider a well-studied setting, that of brokerage closures as studied in Kelly and Ljungqvist (2012)\nKelly and Ljungqvist (2012, 1368) argue that “brokerage closures are a plausibly exogenous source of variation in the extent of analyst coverage, as long as two conditions are satisfied. First, the resulting coverage terminations must correlate with an increase in information asymmetry. … Second, the terminations must only affect price and demand through their effect on information asymmetry.” Interestingly, these are essentially questions 2 and 3 that we ask in evaluating instrumental variables in Section 20.3. The analogue with instrumental variables applies because Kelly and Ljungqvist (2012) are primarily interested in the effects of changes in information asymmetry, not the effects of brokerage closures per se. In principle, brokerage closures could function much like an instrumental variable, except that Kelly and Ljungqvist (2012) estimate reduced-form regressions whereby outcomes are related directly to brokerage closures, such as in Table 3 Kelly and Ljungqvist (2012, 1391). But the first of the three questions from Section 20.3 remains relevant, and this is the critical question for evaluating any natural experiment: Is treatment assignment (as-if) random?\nLike many researchers, Kelly and Ljungqvist (2012) do not address the (as-if) randomness of treatment assignment directly. Instead, Kelly and Ljungqvist (2012) focus on whether brokerage closure-related terminations of analyst coverage “constitute a suitably exogenous shock to the investment environment”. Kelly and Ljungqvist (2012) argue that they do “unless brokerage firms quit research because their analysts possessed negative private information about the stocks they covered.” But this reasoning is incomplete. For sure, brokerage firms not quitting research for the reason suggested is a necessary condition for a natural experiment (otherwise the issues with using brokerage closures as a natural experiment are quite apparent). But it is not a sufficient condition. If the firms encountering brokerage closure-related terminations of analyst coverage had different trends in information asymmetry for other reasons, the lack of private information is inadequate to give us a natural experiment.\nIn general, we should be able to evaluate the randomness of treatment assignment much as we would do so with an explicitly randomized experiment. Burt (2000) suggest that “statisticians will compare the homogeneity of the treatment group populations to assess the distribution of the pretreatment demographic characteristics and confounding factors.” With explicit randomization, statistically significant differences in pretreatment variables might prompt checks to ensure that, say, there was not “deliberate alteration of or noncompliance with the random assignment code” or any other anomalies. Otherwise, we might have greater confidence that randomization was implemented effectively, and hence that causal inferences might reliably be drawn from the study.\nSo, a sensible check with a natural experiment would seem to be to compare various pre-treatment variables across treatment groups to gain confidence that “nature” has indeed randomized treatment assignment. In this regard, Table 1 of Kelly and Ljungqvist (2012) is less than assuring. Arguably, one can only encounter brokerage closure-related terminations of analyst coverage if one has analyst coverage in the first place; so the relevant comparison is arguably between the first and third columns of data. There we see that the typical firm in the terminations sample (column 1) is larger, has higher monthly stock turnover, higher daily return volatility, and more brokers covering the stock than does the typical firm in the universe of covered stocks in 2004 (column 3). So clearly “nature” has not randomly selected firms from the universe of covered stocks in 2004.\nHowever, we might come to a similar conclusion if we compared the Reg SHO pilot stocks with the universe of traded stocks or some other broad group. Just as it was essential to correctly identify the population that the SEC considered in randomizing treatment assignment, it is important to identify the population that “nature” considered in assigning treatment in evaluating any natural experiment. While the SEC provided a statement detailing how it constructed the sample, “nature” is not going to do the same and researchers need to consider carefully which units were considered for (as if) random assignment to treatment.\nIn this regard, even assuming that the controls used in Table 2 of@Kelly:2012ul [p.1388] were the ones “nature” herself considered, it seems that the natural experiment did not assign treatment in a sufficiently random way. Table 2 studies four outcomes: bid-ask spreads, the Amihud illiquidity measure, missing- and zero-return days, and measures related to earnings announcements. In each case, there are pre-treatment differences that sometimes exceed the DiD estimates. For example, pre-treatment bid-ask spreads for treatment and control firms are 1.126 and 1.089, a 0.037 difference that is presumably statistically significant given that the smaller DiD estimate of 0.020 has a p-value of 0.011.10 In light of this evidence, it seems that Kelly and Ljungqvist (2012) need to rely on the parallel trends assumption to draw causal inferences and we evaluate the plausibility of this assumption in the next section.\nIt is important to recognize that the shortcomings of broker closures as a natural experiment do not completely undermine the ability of Kelly and Ljungqvist (2012) to draw causal inferences. There appears to be an unfortunate tendency to believe, on the one hand, that without some kind of natural experiment, one cannot draw causal inferences. On the other hand, there is an apparent tendency to view natural experiments as giving carte blanche to researchers to draw all kinds of causal inferences, even when the alleged identification strategies do not, properly understood, support such inferences.\nIn the case of Kelly and Ljungqvist (2012), it seems the authors would like to believe that they have a natural experiment that allows them to draw inferences about the effects of broker closures on information asymmetry (Table 2) and, because broker closures only affect stock prices through their effects on information asymmetry, to conclude from the evidence in Table 3 that increases in information asymmetry reduce stock prices. But Table 2 could have been based on a bullet-proof identification strategy without implying that broker closures only affect stock prices through their effects on information asymmetry. There is really no evidence offered for this claim, one that is arguably very difficult to support.\nAt the same time, it is conceptually possible that Kelly and Ljungqvist (2012) could provide compelling evidence that the only plausible explanation for the abnormal returns in Table 3 is reductions in information asymmetry, even if the results in Table 2 were irredeemably corrupted (e.g., because of failure of parallel trends). Evidence that firms did not “quit research because their analysts possessed negative private information about the stocks they covered” might support drawing certain inferences from Table 3 even without a credible equivalent of Table 2.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#the-parallel-trends-assumption",
    "href": "chap19_Natural_experiments_revisited.html#the-parallel-trends-assumption",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.5 The parallel trends assumption",
    "text": "11.5 The parallel trends assumption\nExamining the studies of the direct effects of Reg SHO discussed in Section 19.2.3, we see that randomization generally provided balance in pre-treatment outcome values. In such settings, we see that DiD can provide unbiased estimates of causal effects, but that it has little appeal when treatment assignment is random. Indeed, if there is little to distinguish treatment and control observations in terms of pre-treatment outcome values, DiD will differ little from simply comparing differences in post-treatment means (the POST estimator discussed above).\nBut in many settings, such as that in Kelly and Ljungqvist (2012), differences in pre-treatment outcome values exist, suggesting that random assignment is not an appropriate assumption. In such settings, it will therefore be necessary to rely on a different assumption to justify the use of DiD for causal inferences. This parallel trends assumption posits that, but for treatment, the expected change in the outcome variable for the treated observations would equal that for control observations. Using this assumption, we can attribute any difference in the change in the outcome variable between the treated and control observations to a treatment effect and random variation and use standard tools of statistical inference to evaluate the null hypothesis that any difference is due to random variation.\nThat DiD can be predicated on an assumption other than (as-if) random assignment may explain its popularity. Cunningham (2021, p. 406) suggests that DiD “has become the single most popular research design in the quantitative social sciences” and Armstrong et al. (2022, 4) find rapid “increase in [the number of] papers using quasi-experimental methods to draw causal inferences, that more than 75% of such papers use variations of the classic difference-in-differences design.”\nArguably DiD is more often used in settings that without (as-if) random assignment of treatment. For example, one of the most highly cited papers using DiD is Card and Krueger (2000), which compares the change in employment in the fast-food industry in New Jersey and Philadelphia before and after an increase in the minimum wage in New Jersey. In this case, treatment assignment was very clearly non-random—it was a function of being located in New Jersey.\nUnlike the assumption of random assignment, the parallel trends assumption is not implied by a reasonable description of a physical or economic process and thus is of a fundamentally different nature. Random assignment is widely regarded as a reasonable description of, say, a coin toss or the generation of pseudo-random numbers using a computer. In contrast, it is difficult to think of a mechanism for imposing parallel trends on the data. Because DiD is—unlike instrumental variables or regression discontinuity designs—generally not predicated on “as-if random variation in the explanatory variable of interest”, it is not correct to consider DiD as a quasi-experimental method (Armstrong et al., 2022, p. 3).11\nInstead the basis for the parallel trends assumption appears to be that it is the assumption that is necessary (and sufficient) for the DiD estimator to provide an unbiased estimator of the causal effect of treatment. But “assuming a can-opener” seems to be a weak foundation for an approach as widespread as DiD.\nOn the one hand, as discussed above, there is no obvious economic basis for the parallel trends assumption with general applicability. On the other hand, there are often reasons to believe that the parallel trends assumption will not hold for various outcomes. The parallel trends assumption will be dubious when the outcome variable tends to be mean-reverting. For example, it is well known that accounting-based measures of operating performance tend to revert towards the mean. So if treatment and control observations have different levels of pre-treatment operating performance, the parallel trends assumption will be a highly dubious basis for causal inference.\nAnother reason to doubt the parallel trends assumption is the fact that the measurement of outcomes is often arbitrary. For example, Li et al. (2018) examine the effect of legal changes on disclosure of customer identities using a variant of DiD.12 One primary outcome measure considered by Li et al. (2018) is ratio , the proportion of significant customers whose identities are not disclosed. But if the parallel trends assumption holds in ratio then, so long as there are pre-treatment differences between treatment and control observations, it is not mathematically possible for parallel trends to hold in \\log(1 + ratio) , which is the measure used in the regression analysis in Li et al. (2018).\nThe apparent flimsiness of the parallel trends assumption underlying DiD analysis in non-randomized settings is perhaps reinforced by the treatment of DiD in textbooks. Imbens and Rubin (2015), a significant recent tome on causal inference, buries DiD in endnotes, merely noting that DiD is “widely used” (2015, p. 44) before directing the reader to Angrist and Pischke (2008). While Angrist and Pischke (2008) discuss DiD and its assumptions, and relate it to fixed-effects regressions and panel data methods, they do little to justify the parallel trends assumption. Cunningham (2021) is much more cautious in discussing the parallel trends assumption, which he notes “is by definition untestable since we cannot observe this counterfactual conditional expectation [of post-treatment outcomes absent treatment]”.\nTwo popular approaches to address the parallel trends assumption are discussed by Cunningham (2021) and Huntington-Klein (2021). The first approach compares the trends in pre-treatment outcome values for treatment and control observations. If these trends are similar before treatment, it is perhaps reasonable to assume they are similar after treatment. But Cunningham (2021, p. 426) notes that “pre-treatment similarities are neither necessary nor sufficient to guarantee parallel counterfactual trends” and this seems an especially dubious assumption if treatment is endogenously selected.\nThe second approach is the placebo test, variants of which are discussed by Cunningham (2021) and Huntington-Klein (2021). One placebo test involves evaluating the treatment effect in a setting where prior beliefs hold that there should be no effect. Another approach involves a kind of random assignment of a pseudo-treatment. In either case, not finding an effect is considered as providing support for the parallel trends assumption in the analysis of greater interest to the researcher. Of course, one might be sceptical of such placebo tests in light of the concerns raised at the start of this chapter. If applying DiD to state-level data on spending on science, space, and technology provides evidence of an effect on suicides by hanging, strangulation, and suffocation, not finding an effect on deaths by drowning after falling out of a canoe or kayak may provide limited assurance.\nTo illustrate, we now apply a kind of placebo test to evaluate the parallel trends assumption for bid-ask spreads, one of the variables considered in the DiD analysis of Table 2 of Kelly and Ljungqvist (2012) (we choose spreads in part because it is easy to calculate).\nWe first create the data set spreads, which contains data on the average spread for stocks over three-month periods—aligning with one measure used Table 2 of Kelly and Ljungqvist (2012)—for a sample period running from Q1, 2001 (the first quarter of 2001) to Q1, 2008, which is the sample period in Kelly and Ljungqvist (2012). We will conduct a study of a pseudo-treatment that we will assume applies for periods beginning in Q1, 2004, which is roughly halfway through the sample period and we code post accordingly.\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\ndsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"dsf\"))\n\nspreads &lt;-\n  dsf |&gt;\n  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),\n         quarter = as.Date(floor_date(date, 'quarter'))) |&gt;\n  group_by(permno, quarter) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(post = quarter &gt;= \"2004-01-01\") |&gt;\n  filter(!is.na(spread), \n         between(quarter, \"2000-01-01\", \"2008-01-01\")) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\ndsf &lt;- load_parquet(db, schema = \"crsp\", table = \"dsf\")\n\nspreads &lt;-\n  dsf |&gt;\n  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),\n         quarter = as.Date(floor_date(date, 'quarter'))) |&gt;\n  group_by(permno, quarter) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(post = quarter &gt;= \"2004-01-01\") |&gt;\n  filter(!is.na(spread), \n         between(quarter, \"2000-01-01\", \"2008-01-01\")) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\nWe now randomize treatment assignment. Because we want to evaluate the parallel trends assumption and completely randomized treatment assignment implies a trivial version of the parallel trends assumption, we specify a small difference in the probability of receiving treatment for observations whose pre-treatment spread exceeds the median ((p = 0.55)) from those whose pre-treatment spread is below the median ((p = 0.45)). This ensures that we have pre-treatment differences and thus need to rely on the parallel trends assumption in a meaningful way.13\n\nset.seed(2021)\n\ntreatment &lt;-\n  spreads |&gt;\n  filter(!post) |&gt;\n  group_by(permno) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(treat_prob = if_else(spread &gt; median(spread), 0.55, 0.45),\n         rand = runif(n = nrow(pick(everything()))),\n         treat = rand &lt; treat_prob) |&gt;\n  select(permno, treat)\n\nObviously the null hypothesis of zero treatment effect holds with this “treatment”, but the question is whether the parallel trends assumption holds for spread. If we find evidence of a “treatment effect”, the only sensible interpretation is a failure of the parallel trends assumption for spread.\nMerging in the treatment data set, we estimate a DiD regression (and cluster standard errors by permno for reasons that will be apparent after reading the discussion below). Results are reported in Table 19.2.\n\nreg_data &lt;-\n  spreads |&gt;\n  inner_join(treatment, by = \"permno\") \n\nfm &lt;- feols(spread ~ post * treat, vcov = ~ permno, data = reg_data)\n\n\nmodelsummary(fm,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\nBecause we find a statistically significant effect of -0.343 with a t-statistic of -4.96 with this meaningless “treatment”, we can conclude with some confidence that the parallel trends assumption simply does not hold for spread in this sample. Given that we might have passed this placebo test even if the parallel trends assumption did not hold for a particular treatment, say due to endogenous selection, it seems reasonable to view this test as being better suited to detecting a failure of parallel trends (as it does here) than it is to validation of that assumption.\nCunningham (2021) and Huntington-Klein (2021) provide excellent pathways to a recent literature examining DiD. However, it is important to recognize that some variant of the scientifically flimsy parallel trends assumption imbues all of these treatments. It would seem to be productive for researchers to discard the “quasi-experimental” pretence attached to DiD and to apply techniques appropriate to what some call interrupted time-series designs (e.g., Shadish et al., 2002).14\nWhile a randomized experiment provides a sound basis for attributing observed differences in outcomes to either treatment effects or sampling variation, without such randomization, it is perhaps more appropriate to take a more abductive approach of identifying causal mechanisms, deeper predictions about the timing and nature of causal effects, explicit consideration of alternative explanations, and the like (Armstrong et al., 2022; Heckman and Singer, 2017). Some evidence of this is seen in the discussion of specific papers in Cunningham (2021), perhaps reflecting reluctance to lean too heavily on the parallel trends assumption.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#indirect-effects-of-reg-sho",
    "href": "chap19_Natural_experiments_revisited.html#indirect-effects-of-reg-sho",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.6 Indirect effects of Reg SHO",
    "text": "11.6 Indirect effects of Reg SHO\nThe early studies of Reg SHO discussed above can be viewed as studying the more direct effects of Reg SHO. As a policy change directly affecting the ability of short-sellers to trade in securities, the outcomes studied by these earlier studies are more closely linked to the Reg SHO pilot than are the outcomes considered in later studies. Black et al. (2019, pp. 2–3) point out that “despite little evidence of direct impact of the Reg SHO experiment on pilot firms, over 60 papers in accounting, finance, and economics report that suspension of the price tests had wide-ranging indirect effects on pilot firms, including on earnings management, investments, leverage, acquisitions, management compensation, workplace safety, and more.”\nOne indirect effect of short-selling that has been studied in subsequent research is that on earnings management. To explore this topic, we focus on Fang et al. (2016, p. 1251), who find “that short-selling, or its prospect, curbs earnings management.”\n\n11.6.1 Earnings management after Dechow et al. (1995)\nIn Chapter 16, we saw that early earnings management research used firm-specific regressions in estimating standard models such as the Jones (1991) model. Fang et al. (2016) apply subsequent innovations in measurement of earnings management, such the performance-matched discretionary accruals measure developed in Kothari, Leone, and Wasley (2005) .\nKothari, Leone, and Wasley (2005) replace the firm-specific regressions seen in Dechow et al. (1995) with regressions by industry-year, where industries are defined as firms grouped by two-digit SIC codes. Kothari, Leone, and Wasley (2005) also add an intercept term to the Jones Model and estimate discretionary accruals under the Modified Jones Model by applying the coefficients from the Jones Model to the analogous terms of the Modified Jones Model.15\nTo calculate performance-matched discretionary accruals, Kothari et al. (2005, p. 1263) “match each sample firm with the firm from the same fiscal year-industry that has the closest return on assets as the given firm. The performance-matched discretionary accruals … are then calculated as the firm-specific discretionary accruals minus the discretionary accruals of the matched firm.” This is the primary measure of earnings management used by Fang et al. (2016), but note that Fang et al. (2016) use the 48-industry Fama-French groupings, rather than the two-digit SIC codes used in Kothari, Leone, and Wasley (2005) .\n\n11.6.2 FHK: Data steps\nTo construct measures of discretionary accruals, Fang et al. (2016) obtain data primarily from Compustat, along with data on Fama-French industries from Ken French’s website and data on the SHO pilot indicator from the SEC’s website. The following code is adapted from code posted by the authors of Fang et al. (2016), which was used to produce the results found in Tables 15 and 16 of Fang et al. (2019). The bulk of the Compustat data used in Fang et al. (2016) come from comp.funda. Following Fang et al. (2019), the code below collects data from that table for fiscal years between 1999 and 2012, excluding firms with SIC codes between 6000 and 6999 or between 4900 and 4949.\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nfunda &lt;- tbl(db, Id(schema = \"comp\", table = \"funda\"))\n\ncompustat_annual &lt;-\n  funda |&gt;\n  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', \n         between(fyear, 1999, 2012),\n         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |&gt;\n  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,\n         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, \n         rect, ceq, csho, prcc_f) |&gt;\n  mutate(fyear = as.integer(fyear)) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nfunda &lt;- load_parquet(db, schema = \"comp\", table = \"funda\")\n\ncompustat_annual &lt;-\n  funda |&gt;\n  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', \n         between(fyear, 1999, 2012),\n         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |&gt;\n  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,\n         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, \n         rect, ceq, csho, prcc_f) |&gt;\n  mutate(fyear = as.integer(fyear)) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\nSome regressions in Fang et al. (2019) consider controls for market-to-book, leverage, and return on assets, which are calculated as mtob, leverage, and roa, respectively, in the following code:\n\ncontrols_raw &lt;-\n  compustat_annual |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  mutate(lag_fyear = lag(fyear),\n         mtob = if_else(lag(ceq) != 0, \n                        lag(csho) * lag(prcc_f) / lag(ceq), NA),\n         leverage = if_else(dltt + dlc + seq != 0, \n                            (dltt + dlc) / (dltt + dlc + seq), NA),\n         roa = if_else(lag(at) &gt; 0, oibdp / lag(at), NA)) |&gt;\n  filter(fyear == lag(fyear) + 1) |&gt;\n  ungroup() |&gt;\n  select(gvkey, datadate, fyear, at, mtob, leverage, roa)\n\nFollowing Fang et al. (2019), we create controls_filled, which uses [fill()](https://tidyr.tidyverse.org/reference/fill.html) to remove many missing values for the controls.\n\ncontrols_filled &lt;-\n  controls_raw |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  fill(at, mtob, leverage, roa) |&gt;\n  ungroup()\n\nFollowing Fang et al. (2019), we create controls_fyear_avg, which calculates averages of controls by fiscal year.\n\ncontrols_fyear_avg &lt;-\n  controls_filled |&gt;\n  group_by(fyear) |&gt;\n  summarize(across(c(at, mtob, leverage, roa), \n                   \\(x) mean(x, na.rm = TRUE)))\n\nLike Fang et al. (2019), we use values from controls_fyear_avg to replace missing values for controls.\n\ndf_controls &lt;-\n  controls_filled |&gt;\n  inner_join(controls_fyear_avg, by = \"fyear\", suffix = c(\"\", \"_avg\")) |&gt;\n  mutate(at = coalesce(at, at_avg),\n         mtob = coalesce(mtob, mtob_avg),\n         leverage = coalesce(leverage, leverage_avg),\n         roa = coalesce(roa, roa_avg)) |&gt;\n  select(gvkey, fyear, at, mtob, leverage, roa)\n\nThere are multiple steps in the code above and the reasons for the steps involved in calculating controls_filled from controls_raw and df_controls from controls_filled are explored in the exercises below.\nAs discussed above, FHK estimate discretionary-accrual models by industry and year, where industries are based on the Fama-French 48-industry grouping. To create these industries here, we use [get\\_ff\\_ind()](https://rdrr.io/pkg/farr/man/get_ff_ind.html), introduced in Chapter 9 and provided by the farr package.\n\nff_data &lt;- get_ff_ind(48)\n\nWe now create functions to compile the data we need to estimate performance-matched discretionary accruals. We use a function to compile the data because (i) doing so is easy in R and (ii) it allows us to re-use code much easily, which will be important for completing the exercises in this chapter.\nThe first function we create is get_das(), which takes as its first argument (compustat) a data set derived from Compustat with the requisite data. The second argument (drop_extreme) allows to easily tweak the handling of outliers in a way examined in the exercises.\nWithin get_das(), the first data set we compile is for_disc_accruals, which contains the raw data for estimating discretionary-accruals models. Following FHK, we require each industry-year to have at least 10 observations for inclusion in our analysis and impose additional data requirements, some of which we explore in the exercises below.\nFollowing FHK, we estimate discretionary-accrual models by industry and year and store the results in the data frame fm_da. We then merge the underlying data (for_disc_accruals) with fm_da to use the estimated models to calculate non-discretionary accruals (nda). Because the coefficient on sale_c_at is applied to salerect_c_at, we cannot use [predict()](https://rdrr.io/r/stats/predict.html) or [residuals()](https://rdrr.io/r/stats/residuals.html) in a straightforward fashion and need calculate nda “by hand”. We then calculate discretionary accruals (da = acc_at - nda) and return the data.\n\nget_das &lt;- function(compustat, drop_extreme = TRUE) {\n  \n  for_disc_accruals &lt;-\n    compustat |&gt;\n    inner_join(ff_data, \n               join_by(between(sich, sic_min, sic_max))) |&gt;\n    group_by(gvkey, fyr) |&gt;\n    arrange(fyear) |&gt;\n    filter(lag(at) &gt; 0) |&gt;\n    mutate(lag_fyear = lag(fyear),\n           acc_at = (ibc - (oancf - xidoc)) / lag(at),\n           one_at = 1 / lag(at),\n           ppe_at = ppegt / lag(at),\n           sale_c_at = (sale - lag(sale)) / lag(at),\n           salerect_c_at = ((sale - lag(sale)) - \n                              (rect - lag(rect))) / lag(at)) |&gt;\n    ungroup() |&gt;\n    mutate(keep = case_when(drop_extreme ~ abs(acc_at) &lt;= 1,\n                            .default = TRUE)) |&gt;\n    filter(lag_fyear == fyear - 1,\n           keep, \n           !is.na(salerect_c_at), !is.na(acc_at), !is.na(ppe_at)) |&gt;\n    group_by(ff_ind, fyear) |&gt;\n    mutate(num_obs = n(), .groups = \"drop\") |&gt;\n    filter(num_obs &gt;= 10) |&gt;\n    ungroup()\n  \n  fm_da &lt;-\n    for_disc_accruals |&gt;\n    group_by(ff_ind, fyear) |&gt;\n    do(model = tidy(lm(acc_at ~ one_at + sale_c_at + ppe_at, data = .))) |&gt;\n    unnest(model) |&gt;\n    select(ff_ind, fyear, term, estimate) |&gt;\n    pivot_wider(names_from = \"term\", values_from = \"estimate\", \n                names_prefix = \"b_\")\n  \n  for_disc_accruals |&gt;\n    left_join(fm_da, by = c(\"ff_ind\", \"fyear\")) |&gt;\n    mutate(nda = `b_(Intercept)` + one_at * b_one_at + ppe_at * b_ppe_at + \n                   salerect_c_at * b_sale_c_at,\n           da = acc_at - nda) |&gt;\n    select(gvkey, fyear, ff_ind, acc_at, da) \n}\n\nThe next step in the data preparation process is to match each firm with another based on performance. Following FHK, we calculate performance as lagged “Income Before Extraordinary Items” (ib) divided by lagged “Total Assets” (at) and perf_diff, the absolute difference between performance for each firm-year and each other firm-year in the same industry. We then select the firm (gvkey_other) with the smallest value of perf_diff. We rename the variable containing the discretionary accruals of the matching firm as da_other and calculate performance-matched discretionary accruals (da_adj) as the difference between discretionary accruals for the target firm (da) and discretionary accruals for the matched firm (da_other), and return the results. Note that get_pm() includes the argument pm_lag with default value TRUE. If pm_lag is set to FALSE, then performance for matching is calculated using contemporary values of ib and at (this option is examined in the exercises below).\n\nget_pm &lt;- function(compustat, das, pm_lag = TRUE, drop_extreme = TRUE) {\n  \n  das &lt;- get_das(compustat, drop_extreme = drop_extreme)\n  \n  perf &lt;-\n    compustat |&gt;\n    group_by(gvkey) |&gt;\n    arrange(fyear) |&gt;\n    mutate(ib_at = \n      case_when(pm_lag ~ if_else(lag(at) &gt; 0, lag(ib) / lag(at), NA),\n                .default = if_else(at &gt; 0, ib / at, NA))) |&gt;\n    ungroup() |&gt;\n    inner_join(das, by = c(\"gvkey\", \"fyear\")) |&gt;\n    select(gvkey, fyear, ff_ind, ib_at)\n  \n  perf_match &lt;-\n    perf |&gt;\n    inner_join(perf, by = c(\"fyear\", \"ff_ind\"),\n               suffix = c(\"\", \"_other\")) |&gt;\n    filter(gvkey != gvkey_other) |&gt;\n    mutate(perf_diff = abs(ib_at - ib_at_other)) |&gt;\n    group_by(gvkey, fyear) |&gt;\n    filter(perf_diff == min(perf_diff)) |&gt;\n    select(gvkey, fyear, gvkey_other)\n  \n  perf_matched_accruals &lt;- \n    das |&gt;\n    rename(gvkey_other = gvkey,\n           da_other = da) |&gt;\n    select(fyear, gvkey_other, da_other) |&gt;\n    inner_join(perf_match, by = c(\"fyear\", \"gvkey_other\")) |&gt;\n    select(gvkey, fyear, gvkey_other, da_other)\n  \n  das |&gt;\n    inner_join(perf_matched_accruals, by = c(\"gvkey\", \"fyear\")) |&gt;\n    mutate(da_adj = da - da_other) |&gt;\n    select(gvkey, fyear, acc_at, da, da_adj, da_other, gvkey_other)\n}\n\nThe final step is performed in get_pmdas(). This function gets the needed data using get_pm(), then filters duplicate observations based on (gvkey, fyear) (the rationale for this step is explored in the discussion questions).\n\nget_pmdas &lt;- function(compustat, pm_lag = TRUE, drop_extreme = TRUE) {\n  \n  get_pm(compustat, \n         pm_lag = pm_lag,\n         drop_extreme = drop_extreme) |&gt;\n    group_by(gvkey, fyear) |&gt;\n    filter(row_number() == 1) |&gt;\n    ungroup() \n}\n\nFinally, we simply pass the data set compustat_annual to get_pmdas() and store the result in pmdas.\n\npmdas &lt;- get_pmdas(compustat_annual)\n\nThe remaining data set used by FHK is sho_data, which we discussed in Section 19.2.1.\n\nsho_data &lt;- \n  fhk_pilot |&gt;\n  select(gvkey, pilot) |&gt;\n  distinct() |&gt;\n  group_by(gvkey) |&gt;\n  filter(n() == 1) |&gt;\n  ungroup() |&gt;\n  inner_join(fhk_pilot, by = c(\"gvkey\", \"pilot\")) \n\nThe final sample sho_accruals is created in the following code and involves a number of steps. We first merge data from FHK’s sho_data with fhk_firm_years to produce the sample firm-years and treatment indicator for FHK. As fhk_firm_years can contain multiple years for each firm, so we expect each row in sho_data to match multiple rows in fhk_firm_years. At the same time, some gvkey values link with multiple PERMNOs, so some rows in fhk_firm_years will match multiple rows in sho_data. As such, we set relationship = \"many-to-many\" in this join below. We then merge the resulting data set with df_controls, which contains data on controls. The final data merge brings in data on performance-matched discretionary accruals from pm_disc_accruals_sorted. Finally, following FHK, we winsorize certain variables using the [winsorize()](https://rdrr.io/pkg/farr/man/winsorize.html) function from the farr package to do this here.16\n\nwin_vars &lt;- c(\"at\", \"mtob\", \"leverage\", \"roa\", \"da_adj\", \"acc_at\")\n\nsho_accruals &lt;-\n  sho_data |&gt;\n  inner_join(fhk_firm_years, \n             by = \"gvkey\",\n             relationship = \"many-to-many\") |&gt;\n  select(gvkey, datadate, pilot) |&gt;\n  mutate(fyear = year(datadate) - (month(datadate) &lt;= 5)) |&gt;\n  left_join(df_controls, by = c(\"gvkey\", \"fyear\")) |&gt;\n  left_join(pmdas, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(fyear) |&gt;\n  mutate(across(all_of(win_vars),\n                \\(x) winsorize(x, prob = 0.01))) |&gt;\n  ungroup()\n\n\n11.6.3 Discussion questions and exercises\n\nWhat would be the effect of replacing the code that creates ff_data above with the following code? What changes would we need to make to the code creating for_disc_accruals in get_das() to use this modified version of ff_data?\n\n\nff_data &lt;- \n  get_ff_ind(48) |&gt;\n  rowwise() |&gt;\n  mutate(sich = list(seq(from = sic_min, to = sic_max))) |&gt; \n  unnest(sich)\n\n\nWhat issue is filter(row_number() == 1) addressing in the code above? What assumptions are implicit in this approach? Do these assumptions hold in this case? What would be an alternative approach to address the issue?\nWhy is filter(fyear == lag(fyear) + 1) required in the creation of controls_raw?\nDoes the argument for using salerect_c_at * b_sale_c_at in creating non-discretionary accruals make sense to you? How do Kothari, Leone, and Wasley (2005) explain this?\nDoes the code above ensure that a performance-matched control firm is used as a control just once? If so, which aspect of the code ensures this is true? If not, how might you ensure this and does this cause problems? (Just describe the approach in general; no need to do this.)\nWhat are FHK doing in the creation of controls_filled? (Hint: The key “verb” is [fill()](https://tidyr.tidyverse.org/reference/fill.html).) Does this seem appropriate? Does doing this make a difference?\nWhat are FHK doing in the creation of df_controls from controls_fyear? Does this seem appropriate? Does doing this make a difference?\n\n11.6.4 FHK: Regression analysis\nFHK consider a number of regression specifications including: with and without controls, with and without firm fixed effects, and with standard errors clustered by firm alone and clustered by firm and year. We make a small function (reg_year_fe()) that calculates variables used in the regression (like during and post) and allows us to specify each of these different options, to change the dependent variable from the default (dv = \"da_adj\") and to supply a different data set. This function returns a fitted model that is estimated using [feols()](https://lrberge.github.io/fixest/reference/feols.html) from the fixest package.\n\nctrls_list &lt;- c(\"log(at)\", \"mtob\", \"roa\", \"leverage\")\n\nreg_year_fe &lt;- function(df, dv = \"da_adj\",\n                        controls = TRUE, firm_fe = FALSE, cl_2 = TRUE,\n                        vcov = NULL) {\n  df &lt;- \n    df |&gt;\n    mutate(year = year(datadate),\n           during = year %in% c(2005, 2006, 2007),\n           post = year %in% c(2008, 2009, 2010))\n  \n  model &lt;- str_c(dv, \" ~ pilot * (during + post) \",\n                 if_else(controls, \n                         str_c(\" + \", str_c(ctrls_list, \n                                            collapse = \" + \")), \"\"),\n                    if_else(firm_fe, \"| gvkey + year \", \"| year \"))\n  if (is.null(vcov)) {\n    vcov = as.formula(if_else(!cl_2, \"~ gvkey \", \"~ year + gvkey\"))\n  }\n  \n  feols(as.formula(model), \n        vcov = vcov,\n        notes = FALSE,\n        data = df)\n}\n\nTo facilitate the output of variations, we next make a function that runs regressions with and without controls and with and without firm fixed effects and returns a nicely formatted regression table.\n\nmake_reg_table &lt;- function(df, dv = \"da_adj\", cl_2 = TRUE) {\n  omit &lt;- str_c(\"^(\", str_c(str_replace_all(c(\"during\", \"post\", ctrls_list),\n                                            \"[()]\", \".\"), \n                            collapse=\"|\"), \")\")\n  \n  run_reg &lt;- function(controls, firm_fe) {\n    reg_year_fe(df, dv = dv, controls = controls, firm_fe = firm_fe,\n                cl_2 = cl_2)\n  }\n  \n  params &lt;- tibble(controls = c(FALSE, TRUE, FALSE, TRUE),\n                   firm_fe = c(FALSE, FALSE, TRUE, TRUE))\n  \n  fms &lt;- pmap(params, run_reg)\n  \n  notes &lt;- tribble(~term,  ~`1`,  ~`2`, ~`3`, ~`4`,\n                   \"Firm FEs\", \"No\", \"No\", \"Yes\", \"Yes\",\n                   \"Controls\", \"No\", \"Yes\", \"No\", \"Yes\")\n  \n  modelsummary(fms,\n               estimate = \"{estimate}{stars}\",\n               gof_map = \"nobs\",\n               stars = c('*' = .1, '**' = 0.05, '***' = .01),\n               coef_omit = str_c(str_replace_all(ctrls_list, \"[()]\", \".\"),\n                                 collapse = \"|\"),\n               add_rows = notes)\n}\n\nWe now use this function with our version of FHK’s data set (sho_accruals) to create the regression results reported in Table 19.3.\n\nmake_reg_table(sho_accruals)\n\nWe next create a function that allows us to plot by-year coefficients for the treatment and control firms. (We leave the details of what this function is doing as an exercise for the reader below.)\n\nplot_coefficients &lt;- function(model) {\n  tibble(name = names(model$coefficients),\n         value = as.vector(model$coefficients)) |&gt;\n  filter(str_detect(name, \"^year.\")) |&gt;\n  separate(name, into = c(\"year\", \"pilot\"), sep = \":\", fill = \"right\") |&gt;\n  mutate(year = as.integer(str_replace(year, \"^year\", \"\")),\n         pilot = coalesce(pilot == \"pilotTRUE\", FALSE)) |&gt;\n  ggplot(aes(x = year, y = value, \n             linetype = pilot, color = pilot)) +\n  geom_line() +\n  scale_x_continuous(breaks = 2000:2012L) +\n  geom_rect(xmin = 2005, xmax = 2007, ymin = -Inf, ymax = Inf,\n              color = NA, alpha = 0.01) +\n  theme_bw()\n}\n\nTo produce Figure 19.3, we estimate one of the models above by year and feed the fitted model to plot_coefficients().\n\nsho_accruals |&gt;\n  mutate(year = as.factor(year(datadate))) |&gt;\n  feols(da_adj ~ year * pilot - pilot - 1 + log(at) + mtob + roa + leverage,\n        vcov = ~ year + gvkey, data = _) |&gt;\n  plot_coefficients()\n\n\n11.6.5 Exercises\n\nIn words, how does sho_accruals_alt (defined below) differ from sho_accruals? Does using sho_accruals_alt in place of sho_accruals affect the regression results?\n\n\nfirm_years &lt;-\n  controls_raw |&gt;\n  select(gvkey, datadate, fyear)\n\nsho_accruals_alt &lt;-\n  sho_r3000_gvkeys |&gt;\n  inner_join(firm_years, by = \"gvkey\") |&gt;\n  left_join(df_controls, by = c(\"gvkey\", \"fyear\")) |&gt;\n  left_join(pmdas, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(fyear) |&gt;\n  mutate(across(all_of(win_vars), winsorize, prob = 0.01)) |&gt;\n  ungroup()\n\n\nIn an online appendix, BDLYY say “FHK winsorize covariates for their covariate balance table at 1/99%. We inferred that they also winsorized accruals at this level. Whether they winsorize across sample years or within each year, they do not specify.” The code above winsorized within each year. How would you modify the code to winsorize “across sample years”? Does doing so make a difference?\nHow would you modify the code to winsorize at the 2%/98% level? Does this make a difference to the results? (Hint: With the farr package loaded, type [? winsorize](https://rdrr.io/pkg/farr/man/winsorize.html) in the R console to get help on this function.)\nHow would you modify the code to not winsorize at all? Does this make a difference to the results?\nSome of the studies discussed by BDLYY exclude 2004 data from the sample. How would you modify the code above to do this here? Does excluding 2004 here make a significant difference?\nWhat is the range of values for year in sho_accruals? Does this suggest any issues with the code post = year %in% c(2008, 2009, 2010) above? If so, does fixing any issue have an impact on the results reported above?\nWould it make sense, in creating perf above, if we instead calculated ib_at as if_else(at &gt; 0, ib / at, NA))? What is the effect on the regression results if we use this modified calculation of ib_at? What do Kothari, Leone, and Wasley (2005) recommend on this point? (Hint: Use pm_lag = FALSE where applicable.)\nFang et al. (2019, p. 10) follow Fang et al. (2016), who “exclude observations for which the absolute value of total accruals-to-total assets exceeds one. This is a standard practice in the accounting literature because firms with such high total accruals-to-total assets are often viewed as extreme outliers. Nonetheless, the FHK results are robust to winsorizing the accrual measures at the 1% and 99% levels instead of excluding extreme outliers.” Does this claim hold up in the reproduction above? What happens if the [filter()](https://dplyr.tidyverse.org/reference/filter.html) on abs(acc_at) &lt;= 1 is removed from the code above? (Hint: Use drop_extreme = FALSE where applicable.)\nExplain what each line of the function plot_coefficients() before the line starting with [ggplot()](https://ggplot2.tidyverse.org/reference/ggplot.html) is doing. (Hint: It may be helpful to store the model that is fed to the function above in the variable model and then run the function line by line.)",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#統計的推論",
    "href": "chap19_Natural_experiments_revisited.html#統計的推論",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.7 統計的推論",
    "text": "11.7 統計的推論\n\nFHKとBDLYYの間の違いの一つは、クラスター標準誤差に関することである。  Fang et al. (2016)は一般的に「年と企業でクラスタリングされた標準誤差」を使用しているが、Black et al. (2019)は企業でクラスタリングされた標準誤差の使用を提唱している。  Cameron et al. (2008)を引用して、Black et al. (2019, p.30)は「クラスタリングされた標準誤差は、クラスター数が少ない場合には下方バイアスになる可能性がある」と述べている。  FHKの文脈では数千の企業が存在しているが、比較的少数の年数であり、年でクラスタリング（または企業および年度でクラスタリング）を行うと、問題のある標準誤差の推定値が得られる可能性があります（Section 5.6.6を参照）。\n\n適切なクラスタリングを決定するためのアプローチの一つは、より経験的なものである。  この点で、クラスター誤差の標準誤差は、White (1980)のアイデアの一般化であることを指摘しておくと、有用である。 White (1980) provides not only an estimator of standard errors that is robust to heteroskedasticity, but also a test of a null hypothesis of homoskedasticity. Intuitively, if the covariance matrix assuming heteroskedasticity is sufficiently different from that assuming homoskedasticity, then we may reject the null hypothesis of homoskedasticity. With a little algebra, it would be possible to develop a test analogous to that of White (1980) of the null hypothesis of no clustering on variable (g). In practice, many researchers will, lacking a formally derived test, compare standard errors with and without clustering on variable (g) and elect to cluster on variable (g) when the standard errors when doing so seem significantly higher than when not doing so. This heuristic breaks down in the case of Fang et al. (2016) because standard errors are generally lower when clustering on firm and year than when clustering firm alone. However, if clustering on firm alone is appropriate, standard errors clustering on firm and year will provide noisier estimates than clustering on firm alone, and thus could be lower or higher in any given data set.\nA more theoretical approach can be used in the setting of FHK because of our deeper understanding of the assignment mechanism. In this regard, it is important to note that cluster-robust standard errors address correlation in both (X) and () across units within clusters. To explore this (slightly) more formally, recall from Chapter 5 that the cluster-robust covariance matrix is estimated using the following expression:\n\n\\begin{aligned}\n\\hat{V}(\\hat{\\beta}) =\n(\\boldsymbol{X}'\\boldsymbol{X})^{-1} \\hat{\\boldsymbol{B}} (\\boldsymbol{X}'\\boldsymbol{X})^{-1}, \\quad  \\text{where} \\quad  \\hat{B} = \\sum_{g = 1}^G \\boldsymbol{X}'_g u_g u'_g \\boldsymbol{X}_g\n\\end{aligned}\n\nwhere the observations grouped into G clusters of N_g observations for g in {1, \\dots, G} , X_g is the N_g \\times K matrix of regressors, and u_g is the N_g -vector of residuals for cluster g .\nIf we have a single regressor, demeaned x with no constant term and two firms ( i and j ) in a cluster, then the contribution of that cluster to \\hat{B} will be\n\n\\begin{aligned}\n\\begin{bmatrix}\nx_i & x_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu_i \\\\\nu_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu_i & u_j\n\\end{bmatrix}\n\\begin{bmatrix}\nx_i \\\\\nx_j\n\\end{bmatrix} &=\n\\begin{bmatrix}\nx_i & x_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu_i^2 & u_i u_j \\\\\nu_i u_j & u_j^2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_i \\\\\nx_j\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nx_i & x_j\n\\end{bmatrix}\n\\begin{bmatrix}\nx_i u_i^2 + x_j u_i u_j \\\\\nx_i u_i u_j + x_j u_j^2\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nx_i^2 u_i^2 + x_i x_j u_i u_j \\\\\nx_i x_j u_i u_j + x_j^2 u_j^2\n\\end{bmatrix}\n\\end{aligned}\n\nNow, if x_i and x_j are uncorrelated then, even if \\epsilon_i and \\epsilon_j are correlated, this resolves in expectation to\n\n\\begin{bmatrix}\nx_i^2 \\sigma _i^2 \\\\\nx_j^2 \\sigma _j^2\n\\end{bmatrix}\n\n\nこれは、White (1980)の不均一分散頑健推定量の類似の成分の期待値である。  Fang et al. (2016)の設定では、主要な関心事である「(x)」は、ランダムに割り当てられると仮定されているReg SHOパイロット指標であり、したがって（期待値では）企業間で相関がない。  このため、クロスセクション依存性が平均的に標準誤差推定値に影響を与えることはないと予想される。  一方、Reg SHOパイロット指標は、企業内で時間経過とともに完全に相関しているため、時間経過にわたる企業内の誤差の直列依存性が標準誤差推定値に時間系列依存性の影響を与える。  この（やや緩い）理論的分析は、Black et al. (2019, p. 12)が示唆するように、企業（時間系列依存性）でクラスタリングするべきであるが、年でクラスタリングするべきではないことを示唆している。\n\nしかし、仮定されたランダムな処置の割り当てにより、データのクラスタリングの形式に無関心な統計推論の代替アプローチを採用することができる。  このアプローチはランダム化推論(randomization inference)と呼ばれ、いかなる影響も存在しないというフィッシャーの強い帰無仮説(Fisher sharp null hypothesis)に基づく。  これは「シャープの強い帰無仮説」と呼ばれるのは、ゼロ平均効果の帰無仮説よりも制約が厳しいためである。たとえば、観測の半数が +1 の処置効果を持ち、残りの半数が -1 の処置効果を持つ場合、ゼロ平均効果の帰無仮説は成立するが、フィッシャーの強い帰無仮説は成立しない。\n\nフィッシャーの強い帰無仮説の下で、処置へのランダムな割り当てが行われた場合、原則として、すべての可能な割り当てを考慮することで、任意の検定統計量の分布を評価することができる。  初期サンプルとしてSECが焦点を当てた2954社に焦点を当てると、処置の割り当てが純粋にランダムであれば、985社への処置の他の割り当ては、選択された割り当てと同じくらい可能性があった。  フィッシャーの強い帰無仮説が処置の割り当てが結果に影響を与えなかったことを意味するので、SECが他の割り当てのいずれかを選択した場合、テスト統計量の分布がどのようになるかを知っている。なぜなら、結果はまったく同じだったからである。  サンプルが小さい場合、すべての可能な割り当てに対して検定統計量を計算し、フィッシャーの強い帰無仮説の下での検定統計量の正確な分布を構築することができる。[^17]  しかし、我々の場合、2954の可能性から985の処置企業を選ぶ方法は膨大であるため、可能な割り当てのランダムサンプルを抽出し、そのランダムサンプルの検定統計量の経験分布を正確な分布の近似として使用することがより現実的である。\n\nget_coef_rand &lt;- function(i) {\n  treatment &lt;-\n    sho_accruals |&gt;\n    select(gvkey, pilot) |&gt;\n    distinct() |&gt;\n    mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE))\n  \n  reg_data_alt &lt;-\n    sho_accruals |&gt;\n    select(-pilot) |&gt;\n    inner_join(treatment, by = \"gvkey\")\n  \n  reg_data_alt |&gt; \n    reg_year_fe(controls = TRUE, firm_fe = TRUE) |&gt; \n    broom::tidy() |&gt; \n    select(term, estimate) |&gt;\n    pivot_wider(names_from = \"term\", values_from = \"estimate\") |&gt;\n    mutate(iteration = i) |&gt;\n    suppressWarnings()\n}\n\n\nここで興味がある検定統計量は、PILOT \\times DURING の係数である。  以下では、PILOT に関する変数の係数について、係数の経験的分布を用いてp値を計算し、係数に対応する標準誤差をそれらの係数の標準偏差として求める。\n\nset.seed(2021)\nrand_results &lt;-\n  1:1000 |&gt;\n  map(get_coef_rand) |&gt; \n  list_rbind() |&gt;\n  system_time()\n\n\nplan(multisession)\n\nrand_results &lt;- \n  1:1000 |&gt; \n  future_map(get_coef_rand, \n             .options = furrr_options(seed = 2021)) |&gt; \n  list_rbind() |&gt;\n  system_time()\n\n\n以下では、企業と年によるクラスタリング、企業のみによるクラスタリング、およびランダム化推論を用いた標準誤差を基に回帰を実行する。  最初に、企業によるクラスタリング（\"CL-i\"）と企業と年によるクラスタリング（\"CL-2\"）に基づいた標準誤差を用いて、コントロールと企業固定効果を持つ回帰を実行する。\n\nfms &lt;- list(reg_year_fe(sho_accruals, cl_2 = FALSE),\n            reg_year_fe(sho_accruals, cl_2 = TRUE))\n\n\nこれら2つのモデルの分散共分散行列を抽出し、それらをリストvcovsに配置する。\n\nvcovs &lt;- list(vcov(fms[[1]]), vcov(fms[[2]]))\n\n\n次に、ランダム化推論（\"RI\"）を用いて標準誤差を計算するための第3のモデルを追加する。  この第3のモデルのfmsに格納された係数は、すでに格納されている2つのモデルのどちらかから取得できる。\n\nfms[[3]] &lt;- fms[[2]]\n\n\n分散共分散行列については、CL-i標準誤差を出発点として使用する。  rand_resultsに格納された経験的分布を用いて、PILOT に関する変数の係数の要素を置き換える。\n\nvcov &lt;- vcovs[[1]]\nvcov[\"pilotTRUE:duringTRUE\", \"pilotTRUE:duringTRUE\"] &lt;-\n  var(rand_results[[\"pilotTRUE:duringTRUE\"]])\nvcov[\"pilotTRUE:postTRUE\", \"pilotTRUE:postTRUE\"] &lt;- \n  var(rand_results[[\"pilotTRUE:postTRUE\"]])\nvcovs[[3]] &lt;- vcov\n\n\nこの分析の結果は、Table 19.4に示されている。\n\nse_notes &lt;- tribble(~term,  ~`1`,  ~`2`, ~`3`,\n                    \"SEs\", \"CL-i\", \"CL-2\", \"RI\")\n\nmodelsummary(fms, vcov = vcovs, \n             estimate = \"{estimate}{stars}\",\n             gof_map = \"nobs\",\n             stars = c('*' = .1, '**' = 0.05, '***' = .01),\n             coef_omit = \"^(during|post|pilot)TRUE$\",\n             add_rows = se_notes)\n\n\n\n11.7.1 練習問題\n\n\n\nget_coef_rand()関数では、まずデータセットtreatmentを作成し、次にこれをreg_data_altとマージする。  なぜ、reg_data_altに直接mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE))を適用するのではなく、このように行ったのか。\n\n\nランダム化推論を使用して、H_1: \\beta &lt; 0（ここで、 \\beta は PILOT \\times DURING の係数である）の片側の代替仮説の p 値を計算する。  （ヒント：ランダム化を再実行する必要はありません。p_valueの計算を変更するだけで十分である。）\n\n\n\nrand_resultsの係数の分布によって示される経験的標準誤差は何か？  cl_2 = TRUE または cl_2 = FALSE で推定した2方向クラスター頑健標準誤差により近いか？  推定された係数と経験的標準誤差に基づいたt統計量からp値を計算する代わりに、ランダム化推論を用いて検定統計量の経験的分布を用いてp値を計算することが望ましい理由は何か？  前者のアプローチを使用して異なるp値を得ることができるか？\n\n\n\nrand_resultsの係数の分布によって示される経験的標準誤差を使用して、制御変数（例：log(at)）の標準誤差を計算しなかった理由は何か？",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#因果ダイアグラム",
    "href": "chap19_Natural_experiments_revisited.html#因果ダイアグラム",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.8 因果ダイアグラム",
    "text": "11.8 因果ダイアグラム\n\n我々は、裁量的アクルーアルではなく、総アクルーアルを観測していることに注意することが重要である。  代わりに、裁量的アクルーアルの尺度を構築する必要がある。  裁量的アクルーアルのJones (1991)モデルは、売上高成長とPP&Eを「制御」し、Kothari, Leone, and Wasley (2005) モデルはさらにパフォーマンスを「コントロール」している。\n\n以下の因果図が正しいと仮定すると、前処置の結果値（例：DiDを使用）を「制御」するかどうか（例：POSTを使用）に関係なく、因果効果のバイアスのない推定値を得ることができ、総アクルーアルを駆動する他の要因を制御する必要があるかどうかは明確ではない。  Reg SHOのパイロット企業であることが収益管理の低下につながる場合、我々は、我々が直接観測していない裁量的アクルーアルを通じて効果があると仮定していても、より低い総アクルーアルを観測するはずである。  この因果図を受け入れると、制御する要因の選択は、DiD、POST、ANCOVAの選択と同様に、バイアスではなく統計的効率性の問題である。\n\nこの文脈では、Figure 19.4の因果図が不完全である場合、問題が複雑になる可能性があるため、問題をより鮮明に理解するために因果図を考慮することが役立つかもしれない。\n\n\n11.8.1 ディスカッション課題\n\n\n\nFigure 19.4のどの特徴が、Reg SHOが総勘定科目に対する因果効果を推定する際に、パフォーマンス、売上高、PP&Eを制御する必要がないことを示しているのか。  これらの特徴を因果図で仮定する根拠は何か。\n\n\nBlack et al. (2024)は、「会計、ファイナンス、経済学の60以上の論文が、価格テストの停止がパイロット企業に間接的な影響を及ぼし、収益管理、投資、レバレッジ、買収、経営者の報酬、職場安全性などに広範囲の影響を及ぼしたことを報告している（Internet Appendix, Table IA-1を参照）」と報告している。  Black et al. (2024)のInternet Appendixを踏まえると、Reg SHOがパフォーマンス、売上成長、またはPP&Eに影響を与える可能性があるかどうかについての証拠はあるか。  もしそうであれば、これらの結果を考慮するためにFigure 19.4をどのように修正する必要があるか。  これらの変更が、Reg SHOがアクルーアルに対する因果効果を推定するための適切な検定に与える影響は何か。\n\n\n\nTable 19.3のような回帰結果の表とFigure 19.3のようなプロットを作成しなさい。ただしパフォーマンスマッチング裁量的アクルーアルではなく、パフォーマンスマッチングを行わない裁量的アクルーアルを使用なさい。  これらの結果をどのように解釈しますか。\n\n\nFHKのレプリケーションのような回帰表とプロットを作成しなさい。ただし、裁量的アクルーアルの代わりに総アクルーアルを使用し、コントロールを除外しなさい（したがって、係数は単純な条件付きサンプル平均になります）。  これらの結果をどのように解釈しますか。\n\n\nSECによって、FHKが検討した研究問題を登録された報告書の形で調査する研究を設計するように依頼されたとする。  最適な研究設計を理解するためにどのような分析を行いますか。  たとえば、DiD、POST、ANCOVA、およびその他の経験的アプローチの間でどのように選択するのか？  どのようなコントロールを含めるのか？  どのようにしてコントロールを含めるかを決定するのか？  （たとえば、パフォーマンスを収益管理のモデルの回帰変数として含める、パフォーマンスにマッチングする、またはパフォーマンスを主要な回帰仕様に含めることで、パフォーマンスを制御することができる。）  標準誤差はどのように計算するのか？  提案された経験的テストがFHKのものとどのように異なるかについて議論しなさい。  FHKが報告した結果と同様の結果を報告するか？\n\n\nFHKの実証分析が利益マネジメントに対するReg SHOの正の効果を示したと仮定する。  これは彼らの仮説を支持しないことを意味するか？  Journal of Financeでの発表が負の効果を見つけることに依存していたと思うか？\n\n\nReg SHOが収益管理に与える影響を見つけることができなかった場合、FHKのJournal of Financeでの発表にはどのような影響があったか。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#因果メカニズム",
    "href": "chap19_Natural_experiments_revisited.html#因果メカニズム",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.9 因果メカニズム",
    "text": "11.9 因果メカニズム\n\nBlack et al. (2024)は、Reg SHO実験が企業や第三者の行動に影響を与える可能性のあるいくつかの因果関係を示唆している。これには、ショートインタレスト、リターン、価格効率、および「manager fear」が含まれる。  最後のものに関して、Black et al. (2024, p.4)は、「Reg SHO実験がショートインタレストやリターンに実際に影響を与えなかったとしても、パイロット企業の経営者はショートセラーに狙われることを恐れ、予防的な行動を取る可能性がある」と述べている。\n\nBlack et al. (2024, p.5133) は「もし企業の経営者が、価格テストの緩和が自社に影響を及ぼすことを恐れていたのであれば、さまざまな方法で懸念を表明すると考えられる。例えば、ビジネスニュースの記者に話す、SEC（証券取引委員会）が意見を募集した際に書面で意見を提出する、あるいはSEC当局者との面会を求めて反対意見を伝えるなどである。… 私たちは、規則が提案された2003年、実験が発表された2004年、SECが撤廃を提案した2006年の間に、ビジネス報道を調査した。しかし、経営者による反対の証拠は見つからなかった。」 と主張している。\n\nBlack et al. (2024, p.5134) は次のように示唆している。「FHK は、経営者の恐怖チャネルに依拠している。彼らは、空売りの脅威が高まることに対応して、パイロット企業の経営者が空売り投資家を事前に抑止するために、利益操作を抑制したと推測している。」\n\n\n11.9.1 ディスカッション課題\n\n\nBlack et al. (2024)の主張「FHK は、経営者の恐怖チャネルに依存している」と同意するか？  Fang et al. (2016)では、どのような因果メカニズムが示唆されているか？  Fang et al. (2016)は、これらのメカニズムを支持する証拠をどのように提供しているか？\n\n\n因果メカニズムに関連するFang et al. (2019)のBlack et al. (2024)への反応を評価しなさい。\n\n\n研究者のコントロール外で実施される自然実験（つまり、通常は実施後に分析される実験）を行う場合、因果メカニズムの証拠は、無作為化実験を行う場合よりも重要だと思うか？  この章で取り上げられたさまざまな問題を考慮して、その理由を説明しなさい。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#二段階回帰",
    "href": "chap19_Natural_experiments_revisited.html#二段階回帰",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.10 二段階回帰",
    "text": "11.10 二段階回帰\n\nChen, Hribar, and Melessa (2018) は、1つの回帰の残差が後続の回帰の従属変数として使用される場合の統計推論について検討しており、これを「2段階手続き」と呼んでいる。  たとえば、Jones (1991)モデルを用いた裁量的アクルーアルは、売上高の変化額とPP&Eの変化額に関する回帰の残差である。  第16章でカバーしたDechow et al. (1995)で見たように、Jones (1991)モデルの裁量的アクルーアルが、利益マネジメントの様々なインセンティブとどのように関連しているかを多くの論文が検討している。\n\nChen, Hribar, and Melessa (2018, 755)は、「2段階手続きは、多くの研究でバイアスのある係数とt統計量を生成する可能性が高い」と述べており、Frisch-Waugh-Lovellの定理（Section 3.3を参照）に基づいて、2段階手続きの代わりに単一の回帰を使用することを提案している。  Jones (1991)モデルの場合、これは、第1ステップの回帰変数を第2ステップと同じ回帰式に含め、従属変数として裁量的アクルーアルの代わりに総アクルーアルを使用することを意味する。\n\n\n11.10.1 ディスカッション課題\n\n\n\nChen, Hribar, and Melessa (2018) の単一回帰の推奨事項を使用する研究者が、Kothari, Leone, and Wasley (2005) のパフォーマンスマッチングされた裁量的アクルーアルを使用する際には、どのような課題があるか。\n\n\n\nChen, Hribar, and Melessa (2018) が2段階手続きに関して提起した問題が、ランダム化推論を使用する場合にも当てはまると考えるのか？その理由は何か？",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#references",
    "href": "chap19_Natural_experiments_revisited.html#references",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.11 References",
    "text": "11.11 References\n\n\n\n\nAlexander, Gordon J, and Mark A Peterson. 2008. “The Effect of Price Tests on Trader Behavior and Market Quality: An Analysis of Reg SHO.” Journal of Financial Markets 11 (1): 84–111.\n\n\nArmstrong, Christopher, John D Kepler, Delphine Samuels, and Daniel Taylor. 2022. “Causality Redux: The Evolution of Empirical Methods in Accounting Research and the Growth of Quasi-Experiments.” Journal of Accounting and Economics 74 (2-3): 101521.\n\n\nCard, David, and Alan B Krueger. 2000. “Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania: Reply.” American Economic Review 90 (5): 1397–1420.\n\n\nChen, Wei, Paul Hribar, and Samuel Melessa. 2018. “Incorrect Inferences When Using Residuals as Dependent Variables.” Journal of Accounting Research 56 (3): 751–96.\n\n\nDiether, Karl B, Kuan-Hui Lee, and Ingrid M Werner. 2009. “Short-Sale Strategies and Return Predictability.” The Review of Financial Studies 22 (2): 575–607.\n\n\nFrison, Lars, and Stuart J Pocock. 1992. “Repeated Measures in Clinical Trials: Analysis Using Mean Summary Statistics and Its Implications for Design.” Statistics in Medicine 11 (13): 1685–1704.\n\n\nKelly, Bryan, and Alexander Ljungqvist. 2012. “Testing Asymmetric-Information Asset Pricing Models.” The Review of Financial Studies 25 (5): 1366–1413.\n\n\nKothari, Sagar P, Andrew J Leone, and Charles E Wasley. 2005. “Performance Matched Discretionary Accrual Measures.” Journal of Accounting and Economics 39 (1): 163–97.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#footnotes",
    "href": "chap19_Natural_experiments_revisited.html#footnotes",
    "title": "\n11  自然実験:再訪\n",
    "section": "",
    "text": "We exclude non-positive values of exchcd as these appear to be rows flagging special temporary trading statuses that create duplicate rows.↩︎\nIn other words, we don’t need date ranges in the gvkeys table.↩︎\n999↩︎",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  }
]