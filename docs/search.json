[
  {
    "objectID": "chap16_em.html",
    "href": "chap16_em.html",
    "title": "\n8  利益マネジメント\n",
    "section": "",
    "text": "8.1 利益マネジメントを測定する\n利益マネジメントに焦点を当てた会計研究が数多く存在する。  利益マネジメントの定義の1つは、「経営者にベネフィットをもたらす財務報告結果を達成することを目的とした，経営者による会計プロセスへの介入」というものである。\n古典的な利益マネジメントの形態の1つにチャネル・スタッフィング(channel stuffing)がある。 これは、実際の需要を超える製品を流通チャネルを押し込むことで、企業がある期間の売上高（場合によっては利益）をより高く見せる手法である。  チャネル・スタッフィングの古典的な事例として、ボシュロム（B&L）のコンタクトレンズ部門（CLD）が関与したものがある。  米国証券取引委員会(SEC)によると、「ボシュロムはコンタクトレンズの売上高を不適切に認識することにより、1993年の純利益を実質的に過大に報告した。  これらの売上高の過大報告は、ボシュロムの1993年度末までの2週間以内にCLDの販売代理店に大量のコンタクトレンズを販売し、事実上委託販売となったマーケティングプログラムに関連して発生した。  この事例では，「コンタクトレンズ部門の従業員が特定の代理店に対して無許可の返品権を与え、期末後にコンタクトレンズを出荷した」ため、1993年の売上高が適切に認識されなかった。\nボシュロムのチャネル・スタッフィングは明らかに利益マネジメントであり（一般に公正妥当と認められる会計原則, GAAPに違反している）、企業は当期により高い売上高を達成したいという動機から、より極端でない会計慣行に従事する場合がある。これらはGAAPや会計処理の直接的な操作には違反しないものの，一般的には利益マネジメントと見なされるだろう。  このような場合、利益マネジメントは、いわゆる実際の活動（つまり、製品が配送されるときなどのビジネスの現実に影響を与えるもの）によって達成され、この形態の利益マネジメントは実体的利益マネジメント(real earnings management)と呼ばれる。  したがって、利益マネジメントのすべての形態が会計プロセスの直接的な操作を含むわけではない。\nしかし、実体的利益マネジメントを許容すると、原則として、会社価値を高めるために行われた行動と、財務報告の影響によって経営者に利益をもたらす行動を区別することは困難になるかもしれない。\nBeaver (1998)が議論したもう1つの困難は、利益マネジメントの代替的な見方が存在することである。  「経営者が資本提供者やその他の人々にとって不利益となる方法で財務報告システムを操作する」(Beaver、1998年, p.84)という定義を満たす行動は明らかであるが、利益マネジメントによって経営者が「投資家に向けて…私的情報(private information)を明らかにする」ことが可能である。\nまた本書では、Beaver (1998, p.83)が利益マネジメントを利益予測などの自発的開示を含む広範な「裁量的行動」の1つの形態と見なしていることにも注意する。\n定義上の問題を置いておいても、利益マネジメントを理解しようとする研究者にとっての課題は、それを検出し、測定することである。  この節では、2つの初期の論文（Jones, 1991; McNichols and Wilson, 1988）を用いて、研究者がこれらの問題に取り組むために使用してきたいくつかの主要な問題やアプローチを探るために、ディスカッション問題を用いる。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#利益マネジメントを測定する",
    "href": "chap16_em.html#利益マネジメントを測定する",
    "title": "\n8  利益マネジメント\n",
    "section": "",
    "text": "8.1.1 Discussion questions\n\n\nJones (1991)は少数の企業に焦点を当てている。なぜJones (1991)はそのように小さなサンプルを持っているのか？小さなサンプルの欠点は何か？小さなサンプルや狭い焦点の利点はあるか？\n\n\nJones (1991)の主な結論は何か？  Jones (1991)の主な結果を示しているのはどの表か？  その表で使用されている実証テストを説明してください。代替手法を提案できますか？  Jones (1991)の結論に対する主な課題は何だと思いますか？\n\n\n広範な研究問題についてどのような改良案が思い浮かびますか？これらを検討するためにどのようなテストを使用するか？\n\n\nMcNichols and Wilson (1988)は、論文の冒頭で「経営者が利益を操作しているかどうかを検証する」と述べています。  これは、McNichols and Wilson (1988)の主要な研究課題をよく表しているか？  そうでない場合、McNichols and Wilson (1988)の研究問題の代替の要約を提案しなさい。\n\n\nMcNichols and Wilson (1988)のいう「非裁量的アクルーアル」とは何を意味しているのか？  この概念はどれだけ「操作可能」か？1\n\n\n\nMcNichols and Wilson (1988)は、「観測可能であれば、利益マネジメントのアクルーアルに基づくテストは、次の回帰式で表される」と述べている。\n\n\nDA = \\alpha + \\beta PART + \\varepsilon\n\n\nここで、　PART　はデータを2つのグループに分割し、利益マネジメントの予測が指定されたダミー変数である」と述べている。  Healy (1985)は、ボーナス制度が、状況に応じて経営者に利益を増やすか減らすかのインセンティブを与えることを指摘している。  これは、上記のMcNichols and Wilson (1988)の定式化にとって問題となるのはなぜか？  研究者はどのように対処すべきだろうか？\n\n\n利益マネジメントの研究で1つの項目（貸倒引当金）に焦点を当てる利点と欠点は何か？\n\n\nMcNichols and Wilson (1988)の主な結果は、表6と表7にある。  これらの表の「残存規定」列に見られる利益マネジメントの証拠はどれほど説得力があるのか？\n\n\nこのフレームワークはJones (1991)に適用されていますか？この論文にはフレームワークの修正が必要か？ Jones (1991)ではどの期間が1に設定されるのか？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#evaluating-measures-of-earnings-management",
    "href": "chap16_em.html#evaluating-measures-of-earnings-management",
    "title": "\n3  利益マネジメント\n",
    "section": "\n3.2 Evaluating measures of earnings management",
    "text": "3.2 Evaluating measures of earnings management\nA natural question that arises is how well measures of earnings management such as that used in Jones (1991) perform. An ideal measure would detect earnings management when it is present, but not detect earnings management when it is absent. This leads to two questions. First, how well does a given measure detect earnings management when it is present? Second, how does a given measure behave when earnings management is not present?\nDechow et al. (1995) evaluate five earnings management measures from prior research on these terms. Each of these measures uses an estimation period to create a model of non-discretionary accruals which is then applied to measure discretionary accruals for a test period as the difference between total accruals and estimated non-discretionary accruals. Assuming that the estimation period runs from t=1 to t = T, these measures are defined, for firm i in year \\tau, as follows:\n\nThe Healy Model (Healy, 1985) measures non-discretionary accruals as mean total accruals during the estimation period\n\n\nNDA_{i, tau} = \\frac{\\sum _{t=1}^{T} TA_{i,t}}{T}\n\n\nここで、TA_{i,t}は（以下も同様）遅延総資産でスケーリングされた総アクルーアルである。\n\nDeAngeloモデル（DeAngelo、1986）は、前期の総アクルーアルを非裁量的アクルーアルの尺度として使用する。\n\nNDA_{i, tau} = TA_{i,t}\n\n\nJonesモデル（Jones、1991）は、「企業の経済状況の変化が非裁量的アクルーアルに与える影響を制御しようとする」次のモデルを使用する。\n\nNDA_{i, tau} = \\alpha_1 (1/AT_{i, \\tau -1} ) + \\alpha_2 \\Delta REV_{i,\\tau} + \\alpha_3 PPE_{i, \\tau}\n\nここで AT_{i, \\tau -1} は企業 i の \\tau -1 時点の総資産、\\Delta REV _{i, \\tau} は \\tau 年度の売上高から \\tau - 1 年度の売上を引いたものを AT_{i, \\tau - 1} で基準化したもの、 PPE _{i, \\tau} は i 企業の \\tau 年度における有形固定資産を AT_{i, \\tau-1} で基準化したものである。\n\n\n\n修正Jonesモデル : Dechow et al. (1995) は、Jonesモデルの修正バージョンを考慮しており、「Jonesモデルが収益に対して裁量を行うときに裁量的アクルーアルを誤って測定するという推測される傾向を排除するために設計された」 (1995)、p.199）。  このモデルでは、イベント期間中の非裁量的アクルーアルは次のように推定される。\n\n\nNDA_{i, \\tau} = \\alpha_1 (1/AT_{i, \\tau -1} ) +\n    \\alpha_2 ( \\Delta REV_{i,\\tau} - \\Delta REC_{i,\\tau} ) +\n    \\alpha_3 PPE_{i, \\tau}\n\nここで \\Delta REC_{i,\\tau} は、企業 i の \\tau 年度の売掛金から \\tau - 1 年度の売掛金を引いたものを AT_{i, \\tau - 1} で基準化したものである。\n\n\n\n産業モデル : 「非裁量的アクルーアルが時間とともに一定であるという仮定を緩和する。産業モデルは、非裁量的アクルーアルの決定要因の変動が、同じ業界の企業間で共通であると仮定している」 (1995)、p.199）。  このモデルでは、非裁量的アクルーアルは次のように計算される。\n\n\nNDA_{i, \\tau} = \\gamma _1 + \\gamma _2 \\text{median} (TA_{I, \\tau)})\n\nここで TA_{I,\\tau} は産業 I \\ (\\forall j \\in I) における全企業の TA_{j, \\tau } の値である。\n\n上記の各モデルでは、パラメータ（すなわち、(\\alpha _1, \\alpha _2, \\alpha _3)または(\\gamma _1, \\gamma _2)）は、推定期間中に企業ごとに推定される。\n\nDechow et al. (1995) は、異なる質問をテストするために設計された4つの異なるサンプルで分析を行う。  McNichols and Wilson (1988)のフレームワークを活用し、各サンプルの一部の企業年度に対して指標変数PARTを1に設定する。\n\nRandomly selected samples of 1000 firm-years.\nSamples of 1000 firm-years randomly selected from firm-years experiencing extreme financial performance.\nSamples of 1000 firm-years randomly selected to which a fixed and known amount of accrual manipulation is introduced.\nSamples based on SEC enforcement actions.\n\nここでは、Dechow et al. (1995) のある種の複製を行う。  最初の3つのサンプルを考慮するが、4番目のサンプルは省略する。 Data for our analysis come from two tables on Compustat: comp.funda and comp.company.2 本稿の分析のためのデータは、Compustatの2つのテーブル、comp.fundaとcomp.companyから取得される。2\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nfunda &lt;- tbl(db, Id(schema = \"comp\", table = \"funda\"))\ncompany &lt;- tbl(db, Id(schema = \"comp\", table = \"company\"))\n\n\n財務諸表データについては、第15章と同様にfunda_modを構築する。\n\nsics &lt;-\n  company |&gt;\n  select(gvkey, sic) |&gt;\n  mutate(sic = as.integer(sic))\n\nfunda_mod &lt;-\n  funda |&gt;\n  filter(indfmt == \"INDL\", datafmt == \"STD\",\n         consol == \"C\", popsrc == \"D\") |&gt;\n  left_join(sics, by = \"gvkey\") |&gt;\n  mutate(sic = coalesce(sich, sic))\n\n\nSloan (1996, p.293) は、「銀行、生命保険、損害保険会社」のデータを計算するためのデータが利用できないと述べているため、これらの企業（SICコードが6で始まる企業）を除外する。  Dechow et al. (1995) に従い、サンプルを1950年から1991年までの年に制限する。3  また、資産が欠損していない企業年度と、会計期間が12か月の企業年度に制限する(pddur == 12)。\n\nacc_data_raw &lt;-\n  funda_mod |&gt;\n  filter(!is.na(at), # ATの欠損値を除外\n         pddur == 12, # 会計期間が12か月の企業年度\n         !between(sic, 6000, 6999)) |&gt; # SICコードが6で始まる企業を除外\n  mutate( # che, dlc, sale, rectの欠損値を0で埋める\n    across(c(che, dlc, sale, rect), \\(x) coalesce(x, 0))\n  ) |&gt;\n  select(gvkey, datadate, fyear, at, ib, dp, rect,\n    ppegt, ni, sale, act, che, lct, dlc, sic) |&gt; # 変数を選択\n  filter(between(fyear, 1950, 1991)) |&gt; # 1950年から1991年までの年に制限\n  arrange(gvkey, fyear) |&gt; # gvkeyとfyearで並び替え\n  collect() # データを収集\n\n\nSloan (1996)やJones (1991)と同様に、Dechow et al. (1995) は、貸借対照表アプローチを使用してアクルーアルを測定する。  次の関数は、必要なCompustat変数を持つデータフレームを取り、各企業年度の総アクルーアルを計算し、結果のデータセットを返す。\n\ncalc_accruals &lt;- function(df) {\n  df |&gt;\n    group_by(gvkey) |&gt; # gvkeyでグループ化\n    arrange(datadate) |&gt; # datadateで並び替え\n    mutate(lag_at = lag(at),\n           d_ca = act - lag(act), # 流動資産変化\n           d_cash = che - lag(che), # 現金変化額\n           d_cl = lct - lag(lct), # 流動負債変化\n           d_std = dlc - lag(dlc), # 短期借入金変化\n           d_rev = sale - lag(sale), # 売上高変化\n           d_rec = rect - lag(rect)) |&gt; # る売掛金変化\n    ungroup() |&gt; # グループ化を解除\n    mutate(　# 総アクルーアルを計算\n      acc_raw =  (d_ca - d_cash - d_cl + d_std) - dp\n      )\n}\n\n\nJones (1991)と同様に、Dechow et al. (1995) は、企業レベルのデータを推定期間とテスト企業年度に分割し、企業ごとに利益マネジメントモデルを推定する。  Dechow et al. (1995) は、推定期間に少なくとも10年が必要であり、各サンプル企業は構築により1つのテスト企業年度を持つ。  これを実現するために、少なくとも11年間のデータを持つ企業からなる候補企業年度のサンプルを構築する。\n\ntest_sample &lt;-\n  acc_data_raw |&gt;\n  calc_accruals() |&gt; # 関数を適用\n  filter(# 条件を満たすデータを抽出\n    lag_at &gt; 0, sale &gt; 0, ppegt &gt; 0, !is.na(acc_raw),\n    !is.na(d_rev), !is.na(d_rec), !is.na(ppegt)) |&gt;\n  group_by(gvkey) |&gt; # gvkeyでグループ化\n  filter(n() &gt;= 11) |&gt; # 11観測値以上のデータを持つ企業年を抽出\n  ungroup() |&gt; # グループ化を解除\n  arrange(gvkey, fyear) |&gt; # gvkeyとfyearで並び替え\n  select(gvkey, fyear) # 変数を選択\n\n\nほとんどの分析は、1000社のランダムサンプルに焦点を当てる。  これらの1000社の各企業について、partをTRUEに設定する1つの会計年度を選択する。  DeAngeloモデルでは総アクルーアルの前期の値を使用するため、ランダム選択を最初の年以外の任意の年に制限する。\n\nset.seed(2022) # 乱数のシードを設定\n\nsample_1_firm_years &lt;-\n  test_sample |&gt;\n  mutate( # ランダムに1つの会計年度を選択\n    rand = rnorm( # 正規分布から乱数を生成\n      n = nrow(pick(everything())) # 行数分の乱数を生成\n      )) |&gt;\n  group_by(gvkey) |&gt;\n  filter(rand == min(rand),  # 最小の乱数を持つ行を抽出\n         fyear &gt; min(fyear) # 最初の年以外の行を抽出\n         ) |&gt;\n  ungroup() |&gt; # グループ化を解除\n  top_n(1000, wt = rand) |&gt; # 乱数の値が最小の1000行を抽出\n  select(gvkey, fyear) |&gt; # 変数を選択\n  mutate(part = TRUE) # partをTRUEに設定\n\n\nsample_1を作成するために、2つの結合を使用する。  最初の結合は、gvkeyによるsemi_join()であり、sample_1_firm_yearsで選択された企業年度を持つtest_sampleの企業年度を抽出する。  2番目の結合は、gvkeyとfyearによるleft_join()であり、sample_1_firm_yearsからのpart指標を該当する企業年度のデータに追加する。  ほとんどの企業年度はsample_1_firm_yearsには見つからないため、最終ステップでは、sample_1_firm_yearsに欠落している企業年度に対してpartをFALSEに設定するためにcoalesce()を使用する。\n\nsample_1 &lt;-\n  test_sample |&gt;\n  semi_join(sample_1_firm_years, by = \"gvkey\") |&gt; # セミ結合\n  left_join(sample_1_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt; # 左結合\n  mutate(part = coalesce(part, FALSE))\n\n\nサンプル企業年度のpartデータをCompustatデータのacc_data_rawと組み合わせてmerged_sample_1を作成する。4\n\nmerged_sample_1 &lt;-\n  sample_1 |&gt;\n  inner_join(acc_data_raw, by = c(\"gvkey\", \"fyear\")) # 内部結合\n\n\n観察された利益マネジメントの単純な研究を行う場合、利益マネジメントの尺度を計算してから分析に進むのが自然である。  しかし、ここでの分析では、Dechow et al. (1995) のように、自分で会計尺度を操作することになるため、利益マネジメントの尺度や総アクルーアルの尺度などの入力を再計算する必要がある。  このプロセスを容易にするために、以下の関数get_nda()に5つの利益マネジメント尺度の計算を埋め込む。5  summarize()の代わりにreframe()を使用していることに注意する。reframe()は、結果が各グループごとに1行であるとは限らないためである。\n\n# ジョーンズモデル\nfit_jones &lt;- function(df) {\n  fm &lt;- lm(acc_at ~ one_at + d_rev_at + ppe_at - 1,\n           data = df, model = FALSE, subset = !part)\n\n  df |&gt;\n    mutate(nda_jones = predict(fm, newdata = df),\n           da_jones = acc_at - nda_jones) |&gt;\n    select(fyear, nda_jones, da_jones)\n}\n\n# 修正ジョーンズモデル\nfit_mod_jones &lt;- function(df) {\n  fm &lt;- lm(acc_at ~ one_at + d_rev_alt_at + ppe_at - 1,\n           data = df, model = FALSE, subset = !part)\n  df |&gt;\n    mutate(nda_mod_jones = predict(fm, newdata = df),\n           da_mod_jones = acc_at - nda_mod_jones) |&gt;\n    select(fyear, nda_mod_jones, da_mod_jones)\n}\n\n# 非裁量的アクルーアルを計算\nget_nda &lt;- function(df) {\n\n  df_mod &lt;-\n    df |&gt;\n    calc_accruals() |&gt; # 総アクルーアルを計算\n    mutate(sic2 = str_sub(as.character(sic), 1, 2),\n           acc_at = acc_raw / lag_at,\n           one_at = 1 / lag_at,\n           d_rev_at = d_rev / lag_at,\n           d_rev_alt_at = (d_rev - d_rec) / lag_at,\n           ppe_at = ppegt / lag_at) |&gt;\n    group_by(sic2) |&gt; # sic2でグループ化\n    mutate(\n      acc_ind = median(if_else(part, NA, acc_at), na.rm = TRUE)) |&gt;\n    ungroup()\n  # ヒーリーのモデル\n  da_healy &lt;-\n    df_mod |&gt;\n    group_by(gvkey) |&gt;\n    arrange(fyear) |&gt;\n    mutate(nda_healy = mean(if_else(part, NA, acc_at), na.rm = TRUE),\n           da_healy = acc_at - nda_healy,\n           nda_deangelo = lag(acc_at),\n           da_deangelo = acc_at - nda_deangelo) |&gt;\n    ungroup() |&gt;\n    select(gvkey, fyear, part, nda_healy, da_healy, nda_deangelo,\n           da_deangelo)\n  # ジョーンズモデル\n  df_jones &lt;-\n    df_mod |&gt;\n    nest_by(gvkey) |&gt;\n    reframe(fit_jones(data))\n  # 修正ジョーンズモデル\n  df_mod_jones &lt;-\n    df_mod |&gt;\n    nest_by(gvkey) |&gt;\n    reframe(fit_mod_jones(data))\n\n  # 産業モデル\n  fit_industry &lt;- function(df) {\n    fm &lt;- lm(acc_at ~ acc_ind, data = df, model = FALSE, subset = !part)\n\n    df |&gt;\n      mutate(nda_industry = suppressWarnings(predict(fm, newdata = df)),\n             da_industry = acc_at - nda_industry) |&gt;\n      select(fyear, nda_industry, da_industry)\n  }\n  # 産業モデルのnda\n  df_industry &lt;-\n    df_mod |&gt;\n    nest_by(gvkey) |&gt;\n    reframe(fit_industry(data))\n\n  da_healy |&gt;\n    left_join(df_jones, by = c(\"gvkey\", \"fyear\")) |&gt;\n    left_join(df_mod_jones, by = c(\"gvkey\", \"fyear\")) |&gt;\n    left_join(df_industry, by = c(\"gvkey\", \"fyear\"))\n}\n\n\nget_nda()をメインサンプル（merged_sample_1）に適用してさらなる分析のためのreg_dataを作成するには、1行だけで済む。\n\nreg_data &lt;- get_nda(merged_sample_1)",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#results-under-the-null-hypothesis-random-firms",
    "href": "chap16_em.html#results-under-the-null-hypothesis-random-firms",
    "title": "\n8  利益マネジメント\n",
    "section": "\n8.3 Results under the null hypothesis: Random firms",
    "text": "8.3 Results under the null hypothesis: Random firms\nTable 1 of Dechow et al. (1995) presents results from regressions of discretionary accruals on PART for each of the five models. For each model, three rows are provided. The first row provides summary statistics for the estimated coefficients on PART from firm-specific regressions for the 1000 firms in the sample. The second and third rows provide summary statistics on the estimated standard errors of the coefficients on PART and t-statistic testing the null hypothesis that the coefficients on PART are equal to zero.\nTo facilitate creating a similar table, we make two functions. The first function - fit_model() - takes a data frame and, for each firm, regresses the measure of discretionary accruals corresponding to measure on the part variable, returning the fitted models. As we did in Chapter 14, we use !! to distinguish the measure supplied to the function from measure found in df.6\n\nfit_model &lt;- function(df, measure = \"healy\") {\n  df |&gt;\n    nest_by(gvkey) |&gt;\n    summarize(model = list(lm(as.formula(str_c(\"da_\", measure, \" ~ part\")),\n                              model = FALSE, data = data)),\n              .groups = \"drop\") |&gt;\n    mutate(measure = !!measure)\n}\n\nThe second function - multi_fit() - runs fit_model() for all five models, returning the results as a data frame.\n\nmulti_fit &lt;- function(df) {\n  models &lt;- c(\"healy\", \"deangelo\", \"jones\", \"mod_jones\", \"industry\")\n  models |&gt;\n    map(\\(x) fit_model(df, x)) |&gt;\n    list_rbind()\n}\n\nWith these functions in hand, estimating firm-specific regressions for the five models requires a single line of code.\n\nresults &lt;- multi_fit(reg_data)\n\nThe returned results comprise three columns: gvkey, model, and type, with model being the fitted model for the firm and model indicated by gvkey and type. Note that model is a list column and contains the values returned by lm(). We can interrogate the values stored in model to extract whatever details about the regression we need.\n\nhead(results)\n\nWe will use tidy() to extract the coefficients, standard error, t-statistics, and p-values in each fitted model as a data frame. For Table 16.1 (our version of Table 1 of Dechow et al. (1995)), we are only interested in the coefficient on part (i.e., the one labelled partTRUE) and thus can discard the other row (this will be the constant of each regression) and the column term in the function get_stats() that will be applied to each model.\n\nget_stats &lt;- function(fm) {\n  fm |&gt;\n    tidy() |&gt;\n    filter(term == \"partTRUE\") |&gt;\n    select(-term)\n}\n\nThe function table_1_stats() calculates the statistics presented in the columns of Table 1 of Dechow et al. (1995).\n\ntable_1_stats &lt;- function(x) {\n  tibble(mean = mean(x, na.rm = TRUE),\n         sd = sd(x, na.rm = TRUE),\n         q1 = quantile(x, p = 0.25, na.rm = TRUE),\n         median = median(x, na.rm = TRUE),\n         q3 = quantile(x, p = 0.75, na.rm = TRUE))\n}\n\nTo produce Table 16.1, our version of Table 1 of Dechow et al. (1995), we use map() from the purrr library to apply get_stats() to each model, then unnest_wider() and pivot_longer() (both from the tidyr package) to arrange the statistics in a way that can be summarized to create a table.\n\nresults |&gt;\n  mutate(stats = map(model, get_stats)) |&gt;\n  unnest_wider(stats) |&gt;\n  pivot_longer(estimate:statistic, names_to = \"stat\") |&gt;\n  group_by(measure, stat) |&gt;\n  summarize(table_1_stats(value), .groups = \"drop\")\n\nTable 2 of Dechow et al. (1995) presents rejection rates for the null hypothesis of no earnings management in the PART year for the five measures. Given that Sample 1 comprises 1000 firms selected at random with the year in each case also being selected at random, we expect the rejection rates to equal the size of the test being used (i.e., either 5% or 1%). To help produce a version of Table 2 of Dechow et al. (1995), we create h_test(), which extracts statistics from fitted models and returns data on rejection rates for different hypotheses and different size tests.\n\n h_test &lt;- function(fm) {\n  coefs &lt;- coef(summary(fm))\n\n  if (dim(coefs)[1]==2) {\n    t_stat &lt;- coefs[2 ,3]\n    df &lt;- fm$df.residual\n\n    tibble(neg_p01 = pt(t_stat, df, lower = TRUE) &lt; 0.01,\n           neg_p05 = pt(t_stat, df, lower = TRUE) &lt; 0.05,\n           pos_p01 = pt(t_stat, df, lower = FALSE) &lt; 0.01,\n           pos_p05 = pt(t_stat, df, lower = FALSE) &lt; 0.05)\n  } else {\n    tibble(neg_p01 = NA, neg_p05 = NA, pos_p01 = NA, pos_p05 = NA)\n  }\n}\n\nWe then map this function to the models in results and store the results in test_results.\n\ntest_results &lt;-\n  results |&gt;\n  mutate(map_dfr(model, h_test))\n\nUsing, test_results, Table 16.2 provides our analogue of Table 2 of Dechow et al. (1995).\n\ntest_results |&gt;\n  group_by(measure) |&gt;\n  summarize(across(matches(\"p0\"),\n                   \\(x) mean(x, na.rm = TRUE)))\n\nDechow et al. (1995) indicate cases where the Type I error rate is statistically significantly different from the size of the test using a “two-tailed binomial test”. This may be confusing at an initial reading, as the statistics presented in Table 2 of Dechow et al. (1995) are—like those in Table 16.2—based on one-sided tests. But note that whether we are conducting one-sided tests or two-sided tests of the null hypothesis, we should expect rejection rates to equal the size of the test (e.g., 5% or 1%) if we have constructed the tests correctly. For example, if we run 1000 tests with a true null and set the size of the test at 5%, then rejecting the null hypothesis 10 times (1%) or 90 times (9%) will lead to rejection of the null (meta-)hypothesis that our test of our null hypothesis is properly sized, as the following p-values confirm. The function binom.test() provides the p-value that we need here.\n\nbinom.test(x = 10, n = 1000, p = 0.05)$p.value\n\nbinom.test(x = 90, n = 1000, p = 0.05)$p.value\n\nWe embed binom.test() in a small function (binom_test()) that will be convenient in our analysis. Without an a priori reason to expect over-rejection or under-rejection, it makes sense to consider two-sided test statistics against a null hypothesis that the rejection rate equals the size of the test. Such statistics are returned by binom.test() by default.\n\nbinom_test &lt;- #| function(x, p) {\n  x &lt;- x[!is.na(x)]\n  binom.test(sum(x), length(x), p = p)$p.value\n}\n\nWe apply binom_test() to the test results above, adjusting the p argument based on the size of the test used in each case and report the results in Table 16.3.\n\ntest_results |&gt;\n  group_by(measure) |&gt;\n  summarize(neg_p01 = binom_test(neg_p01, p = 0.01),\n            neg_p05 = binom_test(neg_p05, p = 0.05),\n            pos_p01 = binom_test(pos_p01, p = 0.01),\n            pos_p05 = binom_test(pos_p05, p = 0.05))\n\nTurning to the Jones Model and the Modified Jones Model, it is quite clear that we are over-rejecting the (true) null hypothesis. One possible explanation for this over-rejection is provided by footnote 11 of Dechow et al. (1995, p. 204):\n\nThe computation of the standard error of \\hat{b}_j requires special attention because the measures of discretionary accruals in the event period (estimation period) are prediction errors (fitted residuals) from a first-pass estimation process. An adjustment must therefore be made to reflect the fact that the standard errors of the prediction errors are greater than the standard errors of the fitted residuals. Likewise, the degrees of freedom in the t-test must reflect the degrees of freedom used up in the first-pass estimation. This can be accomplished by … estimating a single-stage regression that includes both PART and the determinants of nondiscretionary accruals.\n\nThe invocation of a single-stage regression might remind some readers of the Frisch-Waugh-Lovell theorem, which we discussed in Section 3.3. But an important element of the single-stage regression approach suggested by the Frisch-Waugh-Lovell theorem is that the first- and second-stage regressions that are shown by the theorem to be equivalent have the same observations in both stages. In contrast, the first stages of the Jones Model and Modified Jones Model approaches used by Dechow et al. (1995) use only the estimation sample (i.e., they exclude the test firm-year of primary interest). But this is not an issue here because the single-stage regression invoked by Dechow et al. (1995) is actually that attributed to Salkever (1976).\nSalkever (1976) demonstrates that the estimated value of discretionary accruals in the test year can be obtained by running a single regression including both the estimation and test periods and a dummy variable for the test year. The prediction error for the test observation (i.e., the estimated discretionary accruals for the test firm-year) will be equal to the coefficient on the PART variable and the correct standard error for this prediction will be the standard error of that coefficient.\nBecause the Salkever (1976) approach is infrequently used in accounting research, but seems quite relevant in a number of settings, we spend some time exploring it in the discussion questions below. (Note that in the following, to keep things manageable, we pull a single GVKEY value at random from our sample. You may need to modify this code to ensure that you are drawing the GVKEY of a firm in your sample, which may differ from ours.)\nTo keep things simple, we pull one firm from our sample.\n\ndf_test &lt;-\n  merged_sample_1 |&gt;\n  filter(gvkey == \"001304\")\n\n\n次に、Jonesモデルを実行するために必要な変数を作成する。\n\ndf_mod &lt;-#|\n  df_test |&gt;\n  calc_accruals() |&gt;\n  mutate(acc_at = acc_raw / lag_at,\n         one_at = 1 / lag_at,\n         d_rev_at = d_rev / lag_at,\n         d_rev_alt_at = (d_rev - d_rec) / lag_at,\n         ppe_at = ppegt / lag_at) |&gt;\n  ungroup()\n\n\n次に、推定サンプルに（異なる）修正Jonesモデルを適合し、fm1aに格納する。7\n\nfm1a &lt;- lm(acc_at ~ one_at + d_rev_at + ppe_at,\n          data = df_mod, subset = !part)\n\n\n次に、predict()を使用して全サンプルの非裁量的アクルーアルを計算することができる。  13章で議論した理由から、再びpick(everything())を使用する。\n\nres1 &lt;-\n  df_mod |&gt;\n  mutate(nda_jones = predict(fm1a, newdata = pick(everything()))) |&gt;\n  select(fyear, part, acc_at, nda_jones) |&gt;\n  mutate(da_jones = acc_at - nda_jones)\n\n\n最後に、総アクルーアルを PART 変数に対して回帰し、その結果をfm2aに格納する。\n\nfm2a &lt;- lm(da_jones ~ part, data = res1)\n\n\nSalkerver (1976) が提案したアプローチを実装するために、Dechow et al. (1995) が使用したように、サンプル全体に PART 指標を追加して単一の回帰を実行し、その結果をfm2に格納する。\n\nfm2 &lt;- lm(acc_#| at ~ one_at + d_rev_at + ppe_at + part,\n          data = df_mod)",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#results-under-the-null-hypothesis-extreme-performance",
    "href": "chap16_em.html#results-under-the-null-hypothesis-extreme-performance",
    "title": "\n2  利益マネジメント\n",
    "section": "\n2.4 Results under the null hypothesis: Extreme performance",
    "text": "2.4 Results under the null hypothesis: Extreme performance\nTable 3 of Dechow et al. (1995) presents results from regressions using the second set of samples (“samples of 1000 firm-years randomly selected from firm-years experiencing extreme financial performance”). Table 3 is analogous to Table 2, for which we provided parallel results in Table 16.2.\nWe leave reproduction of a parallel analysis to that reported in Table 3 of Dechow et al. (1995) as an exercise for the reader and merely provide code producing a sample that can be used for that purpose.\nThe following code proceeds in four steps. First, we create earn_deciles, which contains a variable earn_dec that sorts firm-years into earnings deciles for all firms meeting the sample criteria (i.e., those in test_sample).\n\nearn_deciles &lt;-\n  acc_data_raw |&gt;\n  semi_join(test_sample, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  mutate(earn = ib / lag(at)) |&gt;\n  ungroup() |&gt;\n  mutate(earn_dec = ntile(earn, 10)) |&gt;\n  select(gvkey, fyear, earn_dec)\n\nSecond, we create sample_2_firm_years, which selects firm-years from the top earnings decile (subject to the constraint that the year is not the first year for the firm, as a prior year is required for the DeAngelo Model). When a firm has more than one firm-year in the top earnings decile, one of those firm-years is selected at random.\n\nsample_2_firm_years &lt;-\n  earn_deciles |&gt;\n  filter(earn_dec == 10) |&gt;\n  select(gvkey, fyear) |&gt;\n  mutate(rand = rnorm(n = nrow(pick(everything())))) |&gt;\n  group_by(gvkey) |&gt;\n  filter(rand == min(rand), fyear &gt; min(fyear)) |&gt;\n  ungroup() |&gt;\n  top_n(1000, wt = rand) |&gt;\n  select(gvkey, fyear) |&gt;\n  mutate(part = TRUE)\n\nThird, we create sample_2 by pulling firm-years from test_sample for firms found in sample_2_firm_years and then pulling in the firm-years where part is TRUE based on the value of part from sample_2_firm_years and then setting the value of part to FALSE when it is missing (i.e., not found on sample_2_firm_years).\n\nsample_2 &lt;-\n  test_sample |&gt;\n  semi_join(sample_2_firm_years, by = \"gvkey\") |&gt;\n  left_join(sample_2_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt;\n  mutate(part = coalesce(part, FALSE))\n\nFinally, we create merged_sample_2—the analogue of merged_sample_1—by merging sample_2 with the underlying accounting data in acc_data_raw.\n\nmerged_sample_2 &lt;-\n  sample_2 |&gt;\n  inner_join(acc_data_raw, by = c(\"gvkey\", \"fyear\"))\n\n\nDechow et al. (1995)のTable 3は、実際には2つのサンプルを含んでいる。  1つのサンプルは上記と似ており、2つ目のサンプルは上記をベースにして、sample_2_firm_yearsの作成にfilter(earn_dec == 1)を使用している。  Dechow et al. (1995)の表4と似ているが、これは営業活動によるキャッシュフローの十分位に基づいている。 Dechow et al. (1995)のほとんどのサンプル期間ではキャッシュフロー計算書が要求されておらず、営業活動によるキャッシュフローは利益とアクルーアルを使用して計算された。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#discussion-questions-and-exercises",
    "href": "chap16_em.html#discussion-questions-and-exercises",
    "title": "\n2  利益マネジメント\n",
    "section": "\n2.5 Discussion questions and exercises",
    "text": "2.5 Discussion questions and exercises\n\n\nDechow et al. (1995)は、Table 1の結果に対してどのような解釈を提供しているか。\n\n\nTable 16.1の結果とDechow et al. (1995)のTable 1の結果を比較して、どのような違いが重要であると思われるか。\n\n\nDechow et al. (1995)のTable 1の標準偏差列の値を他の統計量と比較しよう。 これらの違いは理にかなっていますか？それとも、基礎データに異常があることを示唆しているか？\n\n\nDechow et al. (1995)の表1の「earnings management」行の標準偏差列の値を、標準誤差行の平均列の値と比較しなさい。  これらの値の関係は何か？これらの値の関係はどのようになると予想されるのか？Table 16.1でも同様の関係を観察できるか？\n\n\nHealyモデル、DeAngeloモデル、産業モデルに焦点を当て、Table 16.2の棄却率をDechow et al. (1995)のTable 2で提示されたものと比較してください。  どのような違いが説明されるか？これらはTable 16.1の結果とDechow et al. (1995)のTable 1で報告された結果との違いに起因すると考えられるか？それとも、これらの違いには別の原因があると予想されるか？\n\n\nTable 16.3に報告されたbinom_test()の結果をどのように解釈できるか？ 各列を他の列とは独立して解釈するのは意味があるのか？\n\n\n\nfm2aの回帰の PART の係数がfm2の回帰から回復できることを確認しなさい。標準誤差は同じか？\n\n\n上記のコードを修正して、修正ジョーンズモデルについても同様のことが成立するか確認しなさい。\n\n\n上で説明したジョーンズモデルを「(異なる)修正ジョーンズモデル」と呼ぶ。  このモデルは、上記のfit_jones()で推定されたジョーンズモデルとどのように異なるか？fit_jones()からのジョーンズモデルを使用した場合、Salkever (1976)の同等性は成立するか？  成立する場合、なぜか？成立しない場合、ジョーンズモデルとSalkever (1976)アプローチの使用方法にどのように影響するか？  （たとえば、「(異なる)修正ジョーンズモデル」は、ジョーンズモデルと実質的に異なる結果を生じると予想されるか？）\n\n\n一段目と二段目に関連する問題は、HealyモデルまたはDeAngeloモデルのどちらか、または両方に適用されるか？  もしそうなら、Salkever (1976)アプローチを適用してこれらの問題に対処できるか？  そうでない場合、上記で実装されたHealyモデルとDeAngeloモデルのアプローチに対する「一段階」の同等物はあるか？\n\n\n上記で使用したコードを適応して、merged_sample_2とTable 16.2を作成することで、Dechow et al. (1995)のTable 3の同等物を作成しなさい。（チャレンジバージョン：Salkever (1976)のアプローチを実装しなさい。）\n\n\n上記で使用したコードを適応して、merged_sample_2とTable 16.2を作成することで、Dechow et al. (1995)のTable 4の同等物を作成しなさい。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#power-of-tests-of-earnings-management",
    "href": "chap16_em.html#power-of-tests-of-earnings-management",
    "title": "\n8  利益マネジメント\n",
    "section": "\n8.6 Power of tests of earnings management",
    "text": "8.6 Power of tests of earnings management\n\nここで考慮するDechow et al. (1995)の最終分析は、Dechow et al. (1995, p.200)が考慮したサンプルの3番目のセットに関連しており、「固定された既知の額のアクルーアル操作が人為的に導入された1000のランダムに選択された企業年度のサンプル」である。  Dechow et al. (1995)の図4は、3つの異なる形式の利益管理、5つの利益管理尺度、および総資産の0から100%までのレベルの誘発された利益マネジメントのパワー関数を示している。  「固定された既知の額のアクルーアル操作」を実装するために、Compustatから必要な変数を持つデータセット（例：gvkey、fyear、sale、at）を取る関数manipulate()を使用する。この関数は、前年度の総資産の割合としての利益管理のレベルの引数と、Dechow et al. (1995)で説明されているように、利益管理のタイプとして”expense”、“revenue”、または”margin”のいずれかを取る引数を取る。\n\nmanipulate &lt;- function(df, level = 0, type) {\n  df &lt;-\n    df |&gt;\n    group_by(gvkey) |&gt;\n    arrange(datadate) |&gt;\n    mutate(ni_ratio = median(if_else(part, NA, ni / sale), na.rm = TRUE),\n           lag_at = lag(at),\n           manip_amt = lag_at * level,\n           manip_amt_gross = manip_amt / ni_ratio)\n\n  if (type == \"expense\") {\n    df |&gt;\n      mutate(lct = if_else(part, lct - manip_amt, lct)) |&gt;\n      ungroup()\n  } else if (type == \"revenue\") {\n    df |&gt;\n      mutate(sale = case_when(part ~ sale + manip_amt,\n                              lag(part) ~ sale - manip_amt,\n                              .default = sale),\n             rect = if_else(part, rect + manip_amt, rect),\n             act = if_else(part, act + manip_amt, act)) |&gt;\n      ungroup()\n  } else if (type == \"margin\") {\n    df |&gt;\n      mutate(sale = case_when(part & ni_ratio &gt; 0 ~\n                                sale + manip_amt_gross,\n                              lag(part) & ni_ratio &gt; 0 ~\n                                sale - manip_amt_gross,\n                              .default = sale),\n             rect = if_else(part & ni_ratio &gt; 0,\n                            rect + manip_amt_gross, rect),\n             act = if_else(part & ni_ratio &gt; 0,\n                           act + manip_amt_gross, act),\n             lct = if_else(part & ni_ratio &gt; 0,\n                           lct + manip_amt_gross - manip_amt, lct)) |&gt;\n      ungroup()\n  } else {\n    df |&gt;\n      ungroup()\n  }\n}\n\n\n上記のmanipulate()関数を使用し、各タイプについて、総資産の前年度比率の0から100%までのレベルの利益管理を適用する。  上記のステップの結果は、上記のget_nda()に供給され、その結果はmulti_fit()に供給され、総アクルーアルをPART変数に回帰した結果を計算する。  これらのステップの結果は、manip_dfというデータフレームに格納される。\n\n\n\n\n\n\n\nmanip_dfの作成には時間がかかるため、  以下の演習では、このコードを実行する必要はない。  処理時間に加えて、このコードはかなりのメモリを消費する。メモリは10GB以上必要となる。  したがって、RAMが約16GB未満の場合、このコードをスムーズに実行するには修正が必要かもしれない。\n\n\n\n\nplan(multisession)\n\nmanip_df &lt;-\n  expand_grid(level = seq(from = 0, to = 1, by = 0.1),\n              manip_type = c(\"expense\", \"revenue\", \"margin\")) |&gt;\n  mutate(data = future_map2(level, manip_type,\n                            \\(x, y) manipulate(merged_sample_1, x, y))) |&gt;\n  mutate(accruals = future_map(data, get_nda)) |&gt;\n  mutate(results = future_map(accruals, multi_fit)) |&gt;\n  select(-data, -accruals) |&gt;\n  system_time()\n\n\n様々なレベル値での回帰の結果、3つのmanip_typeの値、および5つのモデル（manip_type）の結果がmanip_dfに格納されているため、以下のコードを使用して、Dechow et al. (1995)の図4に示されているようなプロットを作成できる。  まず、適合モデルを受け取り、5%の水準で帰無仮説が棄却されたかどうかを示す論理値を返す関数（h_test_5()）を作成する。\n\nh_test_5 &lt;- function(fm) {\n\n  coefs &lt;- coef(summary(fm))\n\n  if (dim(coefs)[1]==2) {\n    t_stat &lt;- coefs[2 ,3]\n    df &lt;- fm$df.residual\n    pt(t_stat, df, lower = FALSE) &lt; 0.05\n  } else {\n    NA\n  }\n}\n\nThe code below applies h_test_5() to each row of manip_df to calculate the proportion of firms for which the null is rejected for each value of (level, manip_type, measure).\n\n\n\n\n\n\n\nこのpower_plot_dataの作成には時間がかかるため、  以下の演習では、このコードを実行する必要はない。  処理時間に加えて、このコードはかなりのメモリを消費し、メモリは10GB以上必要となる。  したがって、RAMが約16GB未満の場合、このコードをスムーズに実行するには修正が必要かもしれない。\n\n\n\n\nplan(multisession)\n\npower_plot_data &lt;-\n  manip_df |&gt;\n  unnest(results) |&gt;\n  group_by(level, manip_type, measure) |&gt;\n  mutate(rej_null = future_map_lgl(model, h_test_5)) |&gt;\n  summarize(prop_reject = mean(rej_null, na.rm = TRUE), .groups = \"drop\") |&gt;\n  system_time()\n\n\nこのデータセットは、facet_grid()を使用して簡単にプロットでき、その結果は図16.1に示されている。\n\npower_plot_data |&gt;\n  ggplot(aes(x = level, y = prop_reject)) +\n  geom_line() +\n  facet_grid(measure ~ manip_type)",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#discussion-questions-1",
    "href": "chap16_em.html#discussion-questions-1",
    "title": "\n8  利益マネジメント\n",
    "section": "\n8.7 Discussion questions",
    "text": "8.7 Discussion questions\n\n\n図16.1の結果は、Dechow et al. (1995)の図4と比較してどうだろうか？\n\n\nB&Lに関連する上記のSECの報告書によると、「B&Lは、GAAPおよび同社独自の収益認識方針に違反して、1993会計年度に報告された純利益の少なくとも11%、つまり$17.6百万ドルの純利益の過大報告をもたらす$42.1百万ドルの収益を認識した。」  後のSECの報告書によると、B&Lの1994年の総資産は$2,457,731,000ドルであった（1993年の値がこれと大きく異なると仮定するのは合理的でないと思われる）。  この情報（およびSECの報告書に含まれる情報）に基づいて、B&Lの収益管理はDechow et al. (1995)の3つのカテゴリのどれに該当するか？  Dechow et al. (1995)の図4（または上記の図）のx軸に対する大きさはおおよそどれくらいか？  これらのデータポイントに基づいて、この程度の収益管理を検出する各モデルの確率のおおよその推定値はどれくらいか？\n\n\n上記で行われたパワー分析の研究に対する意味合いは何だと考えるか？  これらの意味合いは、Dechow et al. (1995)以降の利益マネジメントに関する広範な文献と一致しているか？  そうであれば、その理由を説明してください。そうでない場合、どのようにして矛盾を調整しますか？\n\n\nDechow et al. (1995, pp.201-202)における対応する説明と、上記のmanipulate()で実装された3つの収益管理の形式が正確に一致しているか？  そうでない場合、どちらのアプローチがより正しいように見えるか？  (負の収益率やゼロの場合に問題が発生します。Dechow et al. (1995)とmanipulate()はそのようなケースをどのように処理していますか？)",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#footnotes",
    "href": "chap16_em.html#footnotes",
    "title": "\n8  利益マネジメント\n",
    "section": "",
    "text": "See (https://en.wiktionary.org/wiki/operationalizable#English)↩︎\nAs in Chapter 15, we supplement data on comp.funda with SIC codes from comp.company.↩︎\nBecause we need lagged values for most analyses, in only collecting data from 1950, we will lose that first year from most analyses. It is unclear whether Dechow et al. (1995) collected data for 1949 to be able to use 1950 firm-years in their analysis, but it is unlikely to have much of an impact (there are few firms in the data for 1950) and it would be easy to tweak if we wanted to include 1950 in our analysis.↩︎\nOur sampling approach deviates from that in Dechow et al. (1995), where firm-years are selected (without replacement) subject to the constraint that “a firm-year is not selected if its inclusion in the random sample leaves less than ten unselected observations for the estimation period.” One important difference is that the approach in Dechow et al. (1995) could lead to a firm having two years of earnings management. There seems to be little upside in this, while our approach is much simpler to code and unlikely to impact results in a significant way.↩︎\nWe put fit_jones() and fit_mod_jones() outside the function for reasons that will become clear if you attempt the exercises.↩︎\naa↩︎\n777↩︎",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap01_Introduction.html",
    "href": "chap01_Introduction.html",
    "title": "\n1  はじめに\n",
    "section": "",
    "text": "1.1 本書の構成\n本書は4部に分かれています。\n第1部：基礎では，研究のためのコンピューティング，統計学，因果推論，および会計研究で一般的に使用されるデータセットの詳細など，さまざまなトピックをカバーしている。  本書のこの部分は，会計の博士課程の正式なカリキュラムには通常含まれていない内容をカバーしている。  たとえば，統計学や因果推論に関連する内容は，統計学や計量経済学の授業でカバーされることが多く，会計専門の授業ではカバーされないことが多い。  研究のためのコンピューティングやデータセットの詳細な調査に関する内容は，一般的に博士課程の授業では全くカバーされず，これらのスキルや知識は非公式に身につけることが一般的である。\n第1部：基礎では，事前の知識がほとんどないことを前提として，データ分析，統計学，因果推論の基本的な概念とスキルをカバーしている。\n第1部：基礎では，実証的な会計研究で頻繁に使用される主要なデータセットを紹介している。\n第1部は，本書の残りの部分の基礎を提供している。  読者や教員の好みによっては，第2部：資本市場研究に進むか，第3部：因果推論に進むかを選択することができる。  第3部の一部は，第2部でカバーされたスキルや概念を引用しているが，そのような場合にはそれぞれにフラグを立てている。\n第1部の内容は，いくつかの方法でカバーすることができる。  1つのアプローチは，独立した入門コースや「ブートキャンプ」でこの内容をカバーすることである。  読者は，第2章が実際には「R for Data Science」の重要な部分を参照していることに気づくだろう。これは独立したコース（ここでカバーされている内容と非常に補完的なもの）になりうるため，これらのスキルにクラス時間を割くことを惜しまないプログラムにとっては，十分な材料がある。\n別のアプローチとしては，第1部：基礎を自己学習のために学生に割り当てることが考えられる。後の部分に最も関連するときに選択された部分をカバーすることもあるだろう。  たとえば，本書の第2部：資本市場研究に基づいたコースの場合，第7章はデータベースを正しくリンクするという重要なトピックをカバーしており，博士課程の授業ではあまり遭遇しないが，第2部の内容に関連するときに背景作業として割り当てることができる。\n第2部：資本市場研究は，資本市場研究に焦点を当てた博士課程レベルのコースの基礎を提供している。  この部分だけでも，約8週間の授業に必要な材料が十分に提供されている。  10週間または12週間のコースの場合，教員は本書の他の部分から材料を引用するか，他の材料を使用して簡単に補足することができる。  第2部は，意図的により「古典的な」材料に焦点を当てているため，金融会計研究のより現代的な研究に焦点を当てた関連する材料を簡単に補完することができる。  第2部は，Fama et al. (1969)，Ball and Brown (1968)，Beaver (1968)などの1960年代の研究から始まり，Bernard and Thomas (1989)，Sloan (1996)，1980年代と1990年代の主要な益出し管理論文など，その後の数十年間の最も重要な研究をカバーしている。\n第3部:因果推論は，実証的な会計研究における因果推論に焦点を当てた博士課程レベルのコースの基礎を提供している。  第3部はより現代的な方向性を持ち，資本市場研究に焦点を当てていない。\nプログラムの学生のニーズに応じて，第1部の要素が必要に応じて引用される独立したコースとして第3部を教えることができる。  第19章のトピックは，第1部の材料を引用しており，因果図（第4章），標準誤差（第5章），データベースのリンク（第7章），正規表現の使用（第9章），および2段階回帰（第3章でカバーされた材料を引用）について詳しく説明している。\n第2部と第3部にはつながりがある（たとえば，第19章では，第15章と第16章でカバーされている計上額と益出し管理の指標について説明している）が，これらは第3部の前提条件として第2部を考慮するほどのものではない。  19章は利益マネジメントに焦点を当てているが，これは第2部の1章（第16章）のトピックである。  第3部の内容は，通常は会計の博士課程の後半にカバーされることが多いが，この材料はかなり自己完結的に提示されているため，博士課程の初期の学生にもアクセスしやすくなっている（第1部の材料を使ってギャップを埋めることもできる）。  第2部の前に第3部のほとんどをカバーすることで，学生がより現代的な視点で第2部の材料（主に古い論文）を読むことができるかもしれない。\n第4部：追加のトピックでは，マッチング，極端な値の処理，選択モデル，統計（機械）学習などのトピックに関する章が提供されている。  これらは重要なトピックであるが，第2部や第3部の材料ほど密接に関連しているとは考えていない。  教員は，本書の第2部や第3部に基づいたコースに第4部の章を簡単に組み込むことができる。また，本書に基づかないコースの独立した材料としても利用できる。",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめに</span>"
    ]
  },
  {
    "objectID": "chap01_Introduction.html#コンピューターの設定",
    "href": "chap01_Introduction.html#コンピューターの設定",
    "title": "\n1  はじめに\n",
    "section": "\n1.2 コンピューターの設定",
    "text": "1.2 コンピューターの設定\n\n# install.packages(\"pacman\") # first time only\npacman::p_load(DBI, MASS, MatchIt, RPostgres,\n    arrow, car, duckdb, farr, fixest, furrr,\n    glmnet, httr2, kableExtra, lmtest, modelsummary,\n    optmatch, pdftools, plm, rdrobust, robustbase,\n    rpart, rpart.plot, sandwich, tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめに</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#議論の質問と演習",
    "href": "chap16_em.html#議論の質問と演習",
    "title": "\n8  利益マネジメント\n",
    "section": "\n8.5 議論の質問と演習",
    "text": "8.5 議論の質問と演習\n\n\nDechow et al. (1995)は、Table 1の結果に対してどのような解釈を提供しているか。\n\n\nTable 16.1の結果とDechow et al. (1995)のTable 1の結果を比較して、どのような違いが重要であると思われるか。\n\n\nDechow et al. (1995)のTable 1の標準偏差列の値を他の統計量と比較しよう。 これらの違いは理にかなっていますか？それとも、基礎データに異常があることを示唆しているか？\n\n\nDechow et al. (1995)の表1の「earnings management」行の標準偏差列の値を、標準誤差行の平均列の値と比較しなさい。  これらの値の関係は何か？これらの値の関係はどのようになると予想されるのか？Table 16.1でも同様の関係を観察できるか？\n\n\nHealyモデル、DeAngeloモデル、産業モデルに焦点を当て、Table 16.2の棄却率をDechow et al. (1995)のTable 2で提示されたものと比較してください。  どのような違いが説明されるか？これらはTable 16.1の結果とDechow et al. (1995)のTable 1で報告された結果との違いに起因すると考えられるか？それとも、これらの違いには別の原因があると予想されるか？\n\n\nTable 16.3に報告されたbinom_test()の結果をどのように解釈できるか？ 各列を他の列とは独立して解釈するのは意味があるのか？\n\n\n\nfm2aの回帰の PART の係数がfm2の回帰から回復できることを確認しなさい。標準誤差は同じか？\n\n\n上記のコードを修正して、修正ジョーンズモデルについても同様のことが成立するか確認しなさい。\n\n\n上で説明したジョーンズモデルを「(異なる)修正ジョーンズモデル」と呼ぶ。  このモデルは、上記のfit_jones()で推定されたジョーンズモデルとどのように異なるか？fit_jones()からのジョーンズモデルを使用した場合、Salkever (1976)の同等性は成立するか？  成立する場合、なぜか？成立しない場合、ジョーンズモデルとSalkever (1976)アプローチの使用方法にどのように影響するか？  （たとえば、「(異なる)修正ジョーンズモデル」は、ジョーンズモデルと実質的に異なる結果を生じると予想されるか？）\n\n\n一段目と二段目に関連する問題は、HealyモデルまたはDeAngeloモデルのどちらか、または両方に適用されるか？  もしそうなら、Salkever (1976)アプローチを適用してこれらの問題に対処できるか？  そうでない場合、上記で実装されたHealyモデルとDeAngeloモデルのアプローチに対する「一段階」の同等物はあるか？\n\n\n上記で使用したコードを適応して、merged_sample_2とTable 16.2を作成することで、Dechow et al. (1995)のTable 3の同等物を作成しなさい。（チャレンジバージョン：Salkever (1976)のアプローチを実装しなさい。）\n\n\n上記で使用したコードを適応して、merged_sample_2とTable 16.2を作成することで、Dechow et al. (1995)のTable 4の同等物を作成しなさい。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#帰無仮説下の結果極端なパフォーマンス",
    "href": "chap16_em.html#帰無仮説下の結果極端なパフォーマンス",
    "title": "\n8  利益マネジメント\n",
    "section": "\n8.4 帰無仮説下の結果：極端なパフォーマンス",
    "text": "8.4 帰無仮説下の結果：極端なパフォーマンス\n\nDechow et al. (1995)のTable 3は、第2のサンプル（「極端な財務パフォーマンスを経験している企業年度からランダムに選択された1000企業年度のサンプル」）を使用した回帰の結果を示している。  Table 3はTable 2に類似しており、Table 16.2で平行な結果を提供した。  Dechow et al. (1995)のTable 3で報告された平行な分析の再現は、読者にとっての演習として残し、その目的に使用できるサンプルを生成するコードのみを提供する。  次のコードは、4つのステップで進行する。  最初に、earn_decilesを作成する。これには、サンプル基準を満たすすべての企業（つまり、test_sampleに含まれる企業）を利益十分位に並べる変数earn_decが含まれている。\n\nearn_deciles &lt;-\n  acc_data_raw |&gt;\n  semi_join(test_sample, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  mutate(earn = ib / lag(at)) |&gt;\n  ungroup() |&gt;\n  mutate(earn_dec = ntile(earn, 10)) |&gt;\n  select(gvkey, fyear, earn_dec)\n\n\n第2に、sample_2_firm_yearsを作成する。これは、最上位の利益十分位から企業年度を選択する（DeAngeloモデルには前年が必要なため、企業の最初の年ではないことが制約として設定されている）。  企業が最上位の利益十分位に複数の企業年度を持っている場合、そのうちの1つの企業年度がランダムに選択される。\n\nsample_2_firm_years &lt;-\n  earn_deciles |&gt;\n  filter(earn_dec == 10) |&gt;\n  select(gvkey, fyear) |&gt;\n  mutate(rand = rnorm(n = nrow(pick(everything())))) |&gt;\n  group_by(gvkey) |&gt;\n  filter(rand == min(rand), fyear &gt; min(fyear)) |&gt;\n  ungroup() |&gt;\n  top_n(1000, wt = rand) |&gt;\n  select(gvkey, fyear) |&gt;\n  mutate(part = TRUE)\n\n\n第3に、sample_2を作成する。これは、sample_2_firm_yearsに見つかる企業の企業年度をtest_sampleから引き出し、sample_2_firm_yearsのpartの値に基づいてpartがTRUEの企業年度を引き出し、partが欠落している場合（つまり、sample_2_firm_yearsに見つからない場合）はpartの値をFALSEに設定する。\n\nsample_2 &lt;-\n  test_sample |&gt;\n  semi_join(sample_2_firm_years, by = \"gvkey\") |&gt;\n  left_join(sample_2_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt;\n  mutate(part = coalesce(part, FALSE))\n\n\n最後に、merged_sample_2を作成する。これは、merged_sample_1のアナログであり、sample_2をacc_data_rawの基礎となる会計データとマージすることで作成される。\n\nmerged_sample_2 &lt;-\n  sample_2 |&gt;\n  inner_join(acc_data_raw, by = c(\"gvkey\", \"fyear\"))\n\n\nDechow et al. (1995)のTable 3は、実際には2つのサンプルを含んでいる。  1つのサンプルは上記と似ており、2つ目のサンプルは上記をベースにして、sample_2_firm_yearsの作成にfilter(earn_dec == 1)を使用している。  Dechow et al. (1995)の表4と似ているが、これは営業活動によるキャッシュフローの十分位に基づいている。 Dechow et al. (1995)のほとんどのサンプル期間ではキャッシュフロー計算書が要求されておらず、営業活動によるキャッシュフローは利益とアクルーアルを使用して計算された。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical Research in Accounting",
    "section": "",
    "text": "序論\n本書は、学部上級（「honours」）あるいは博士課程の入門レベルからスタートする財務会計研究の講義を提供している。  この講義の目的は、ほとんどの博士課程の講義と同じように、博士課程の学生がより上級の講義を取るために準備することや、自ら研究をすすめることができるようにすることである。  もう一つの目的は、コンサルティングやファイナンスといった他の領域で有用なスキルをもつ生徒を育てることである。  この2つ目の目的は、メルボルン大学の優秀学生と博士課程のジョイント講義の一部が起源である。 ここで優秀学生(honours students)は、研究を中心とした追加の1年間の学習を完了する学部生である。  優秀学生の一部は博士課程に進むが、ほとんどはコンサルティング、監査、公共サービスなどの業界での仕事を選択する。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#本書の特徴",
    "href": "index.html#本書の特徴",
    "title": "Empirical Research in Accounting",
    "section": "本書の特徴",
    "text": "本書の特徴\n\n本書の特徴のいくつかは、本書に基づいた講義が、従来の博士課程レベルの講義とは異なる点で区別されることを意味している。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#教育的な観点から論文の選択",
    "href": "index.html#教育的な観点から論文の選択",
    "title": "Empirical Research in Accounting",
    "section": "教育的な観点から論文の選択",
    "text": "教育的な観点から論文の選択\n\n会計を専攻する博士課程の講義の多くは、学生が自分自身の研究で埋めることができる文献の空白を見つけるのを助けるために、最新の論文に焦点を当てている。  このような講義は本書と補完的なものと考えているが、異なるアプローチを取っている。  会計研究のより基本的な理解を提供することを目指して、この講義で選ばれた論文は、会計研究の現状を確認しようとするのではなく、教育的な目的を重視して選択されている。  場合によっては、古い論文（例：Ball and Brown, 1968）を取り上げることもあるが、他の場合には、中心的なアイデアやアプローチを特徴とする最近の論文を使用している。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#データ解析スキルの組み込み",
    "href": "index.html#データ解析スキルの組み込み",
    "title": "Empirical Research in Accounting",
    "section": "データ解析スキルの組み込み",
    "text": "データ解析スキルの組み込み\n\n本書を特徴づける2番目の特徴は、ほとんどの博士課程コースとは違って、データ解析スキルに重点を置いていることである。  本書をデータ解析スキルに特化させることも可能であったが、これらのスキルは実際の研究課題に適用することで最もよく学ぶことができると考えている。  逆に、データを取得し、シミュレーションを実行し、研究プロセスの重要な要素により深く関わることができるようになることは、研究に対する理解を深めると考えている。  データ解析とコンピューティングスキルを体系的に講義の各段階に組み込んでいる。  実際には、研究コンピューティングスキルは研究者のツールキットの中心的な部分を占めているが、一般的には博士課程のカリキュラムでは無視されがちである。  一般的な考え方は、研究コンピューティングスキルは他の学生から、研究助手や教員との協力を通じて、などの形で非公式に獲得されるというものである。\n\n博士課程プログラムによっては、このアプローチがある程度は機能するかもしれない。  しかし、多くの博士課程プログラムでは、このような非公式な学習では学生を十分に準備することができない。  例えば、学生が教員との協力が非公式であり、学生がデータ解析を行う際に教員からの指導が限られているかない場合、明確で包括的な指導の機会は限られる。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#研究デザインと方法に対する強い重点",
    "href": "index.html#研究デザインと方法に対する強い重点",
    "title": "Empirical Research in Accounting",
    "section": "研究デザインと方法に対する強い重点",
    "text": "研究デザインと方法に対する強い重点\n\n会計研究は圧倒的に因果推論を行う実証的な学問であり、そのため、重要な研究トレーニングは研究デザインの問題に焦点を当てるべきである。  本書の第III部では、自然実験、回帰不連続デザイン、操作変数、固定効果など、因果推論について詳しく調べている。  しかし、これらの技術が保証された因果推論の可能性を提供するという一般的な信念には欠陥があることがわかるだろう。  講義全体を通じて、実世界の現象について推論を行うためのより広いツールセットを提供する。  この講義では、カバーする分析を実施するために必要なスキルを学生に提供することを目指している。  会計研究者による統計的および計量経済学的手法の理解は、一貫性や推定量の漸近分散の分析よりも、実際のシミュレーション分析によってより高い可能性で向上すると考えている。  このようなシミュレーションを実行するために必要なデータ解析スキルを組み込むことで、この講義が会計研究者が推定量の特性についてより慎重に考えるためのプラットフォームを提供することを期待している。  本書で行う分析は、本書に記載されているコードを使用して読者が実行できることを前提としている。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#前提条件",
    "href": "index.html#前提条件",
    "title": "Empirical Research in Accounting",
    "section": "前提条件",
    "text": "前提条件\n\nいくつかのトピックについての事前知識と、特定のコンピューティングリソースへのアクセスを前提としている。  これらの要件を最小限に抑えるよう努めている。\n\n\n会計とビジネスに関する知識。  会計に関しては、入門的な財務会計コースの内容についての理解と、会計を理解するためのビジネスに関する十分な理解を前提としている。\n\n\n\n統計学と計量経済学の事前学習  統計推論の要素や最小二乗（OLS）回帰についての基本的な知識があると有用である。  第3章から第5章ではこれらの要素についての入門的な資料を提供しているが、これは選択的なものであり、本書を読み進める際にこれらのトピックを復習するために教科書を使用すると役立つかもしれない。\n\n\n\n学術誌へのアクセス  講義では、学術誌の論文を広範に使用している。  学術機関の教員、研究者、学生であれば、所属する図書館を通じて使用する論文にアクセスできるはずである。  一部の大学は、卒業生が学術誌にアクセスするためのサービス（おそらく有料）を提供している。  残念ながら、論文にアクセスできない場合、第I部以降の本書を十分に活用することは難しいだろう。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#読者へのガイド",
    "href": "index.html#読者へのガイド",
    "title": "Empirical Research in Accounting",
    "section": "読者へのガイド",
    "text": "読者へのガイド\n\n本書は、上記の前提条件を満たしている読者にとっては、比較的独学で読むことができるように書かれている。  このような読者には、最初の数章を順番に読み進め、コードを実行し、演習を完了し、議論の質問について考えることをお勧めする。  ただし、演習や議論の質問の一部は微妙であり、これらについて議論するためのインストラクターや他の人がいると、この教材から最大の価値を得るのに役立つだろう。  本書が初心者以外のさまざまな読者、学習者、インストラクターにとって有用であることを願っている。  以下では、いくつかの仮想的な読者に対するアプローチについて議論する。\n\n\n研究デザインと因果推論に関連する問題についてもっと学びたい。  第2章と第4章に飛び込んで、その後第17章以降の章に進むことができるかもしれない。 \n研究デザインと因果推論に関連する問題についてもっと学びたいが、Rを学ぶ気はあまりない。  前の項目の仮想的な読者の計画がおそらく適している。  Rを学ぶことに興味がなくても、コードを実行することが理解を固めるのに役立つと考えており、コードが何をしているかが十分に明確であるため、コードを自分のコンピュータにコピーして貼り付けるだけで、何が起こっているかの要点を把握できるはずである。 \nRについて聞いたことがあり、もっと詳しく知りたい。  第2章と第3章では、基本的な内容をカバーしている。  ただし、SASやStataなどの他のソフトウェアに精通している場合、これらの章をスキップするのはかなり簡単かもしれない（上記の前提条件を満たした後）。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#謝辞",
    "href": "index.html#謝辞",
    "title": "Empirical Research in Accounting",
    "section": "謝辞",
    "text": "謝辞\n省略",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "index.html#スタイルに関するいくつかの注意",
    "href": "index.html#スタイルに関するいくつかの注意",
    "title": "Empirical Research in Accounting",
    "section": "スタイルに関するいくつかの注意",
    "text": "スタイルに関するいくつかの注意\n\n本書は、ほぼイギリス（したがってオーストラリア）の慣習に従っている。  7歳のときに受け取ったPocket Oxford Dictionaryの持続的な影響を反映して、私たちは「-ize」の綴りを使うことが多い（いずれにせよ、これはアメリカの読者にとってより馴染み深い）。  また、おそらくオックスフォードカンマをよく使っている。  私たちの選択の利点の一つは、アメリカ英語の規則に従う必要がないことである。つまり、コンマやピリオドを常に引用符の内側に置かなければならないという規則に従う必要がなく、代わりに（アメリカ英語以外の言語を話す人々がそれらを置く場所）自然な場所に置くことができる。これにより、一部のアメリカの読者には奇妙に見えるかもしれないが、それでも構わない。  （この点についてTidyverseの主要作者であるHadley Wickhamの意見に異論を唱えるのは難しい。 「それはアメリカ英語で最も愚かなルールであり、私はそれに従うことを拒否する」と彼は述べている。）  コードについては、RコードのTidyverseスタイルガイドに基づいているが、代入演算子（&lt;-）の後の最初の項目を新しい行に置くことが多い。",
    "crumbs": [
      "序論"
    ]
  },
  {
    "objectID": "chap15_accruals.html",
    "href": "chap15_accruals.html",
    "title": "\n7  アクルーアル\n",
    "section": "",
    "text": "7.1 Sloan (1996)\n本章では、アクルーアルのプロセスの研究への焦点を提供するために、Sloan (1996) を利用する。  アクルーアルへに注目することで、会計プロセスより理解するためにシミュレーション分析を用いる。 この章では、アクルーアルを営業キャッシュフローを超える利益の部分と定義する。1  最後に、いわゆるアクルーアル・アノマリー(accrual anomaly)の検討を行う。\nSloan (1996) は、いくつかの実務家がキャッシュフローではなくアクルーアルに依存する企業を特定することに基づいて投資アドバイスを提供していることを指摘している。  このような投資アドバイスは、資本市場が利益に「固執」し、キャッシュフローとアクルーアルの成分の特性の違いを認識しない傾向に基づいていると主張されている。  ある意味で、Sloan (1996) はそのような投資アドバイスとそれに基づく前提を厳密に評価している。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>アクルーアル</span>"
    ]
  },
  {
    "objectID": "chap15_accruals.html#sloan-1996",
    "href": "chap15_accruals.html#sloan-1996",
    "title": "\n7  アクルーアル\n",
    "section": "",
    "text": "7.1.1 ディスカッション問題\n\n以下のディスカッション質問は、Sloan (1996) を読むためのアプローチを提供します。\n\n\n\nH_1 のフォーマルな記述の前にこの資料を読みなさい。  Sloan (1996) は、利益の構成要素の差異の持続性に関してどのような理由を提供しているのか？  これらの理由が正しいことが、表3で提供されているH1の実証的な支持にとってどれほど重要か？  H1の実証的な支持がH2(i)にとってどれほど重要か？\n\n\n表4はどの仮説をテストしているのか？ 表4の結果を言葉でどのように解釈しているか？\n\n\n表5はどの仮説をテストしているのか？ 表5の結果を言葉でどのように解釈しているか？\n\n\n表6はどの仮説をテストしているのか？ 表6の結果を言葉でどのように解釈しているか？  Sloan (1996) の表6の結果と Bernard and Thomas (1989) の結果には類似点がある。  両論文は、ある変数(1996年のSloanのアクルーアル、1989年のBernard and Thomasの利益サプライズ)の十分位数に基づいて企業のポートフォリオを形成し、その後のポートフォリオのパフォーマンスを調査することを含んでいる。  ポートフォリオを形成するために使用される尺度以外に、表6を見て考えられる2つの論文の分析の重要な違いは何か？\n\n\nFigure 2 は(あれば)どの仮説と関連していますか？ Sloan (1996) によると、Figure 2 は何を示していますか？\n\n\nFigure 3 は(あれば)どの仮説と関連していますか？ Sloan (1996) によると、Figure 3 は何を示していますか？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>アクルーアル</span>"
    ]
  },
  {
    "objectID": "chap15_accruals.html#アクルーアルの測定",
    "href": "chap15_accruals.html#アクルーアルの測定",
    "title": "\n7  アクルーアル\n",
    "section": "\n7.2 アクルーアルの測定",
    "text": "7.2 アクルーアルの測定\n\nHribar and Collins (2002) は、Sloan (1996) で使用されているものに類似したアクルーアルの定義を含んでいる。  先行研究を参照して、彼らは次のように述べている(2002, p. 10)。\n\n\n具体的には、アクルーアル(ACC_{bs})は通常次のように計算される(便宜上、企業と時間の添字は省略されている)。 \nACC_{bs} = (\\Delta CA - \\Delta CL - \\Delta Cash + \\Delta STDEBT - DEP)\n\n\nここで\n\n\n\n\\Delta CA = 期間 t 中の流動資産の変化(Compustat #4) \n\n\n\\Delta CL = 期間 t 中の流動負債の変化(Compustat #5) \n\n\n\\Delta Cash = 期間 t 中の現金及び現金同等物の変化(Compustat #1) \n\n\n\\Delta STDEBT = 期間 t 中の流動負債に含まれる長期債務の短期債務の変化(Compustat #34) \n\nそして \\Delta DEP = 期間 t 中の減価償却費と無形資産償却費(Compustat #14)。\n\nすべての変数は、規模の違いを調整するために遅れた総資産( TA_{t-1} )で割っている。\n\n\n最初に尋ねるべきことは、「(例えば) ‘Compustat #4’ とは何か？」ということだろう。  2006年以前、Compustatのデータ項目は、Compustat #4やdata4などの数字を使って参照されていた。  そのため、古い論文ではそのような項目が参照されることがある。  幸いなことに、Wharton Research Data Services (WRDS) はこれらの項目から現在の変数への変換テーブルを提供しており、関連する変換は表15.1に示されている。\n\nTable 15.1: Translation of key pre-2006 Compustat items\n\nOld item\nCurrent item\nItem description\n\n\n\n#1\nche\nCash and Short-Term Investments\n\n\n#4\nact\nCurrent Assets—Total\n\n\n#5\nlct\nCurrent Liabilities—Total\n\n\n#14\ndp\nDepreciation and Amortization\n\n\n#34\ndlc\nDebt in Current Liabilities—Total\n\n\n\nHribar and Collins (2002) は、流動資産の変化から流動負債の変化を引いて現在のアクルーアルを計算することは間違っていると指摘している。「他の非営業イベント(例えば、合併、売却)が現在の資産と負債の勘定に影響を与えるが、利益への影響はない」と述べている。\n\n\n7.2.1 議論する問題\n\n\n上記の式では、なぜ \\Delta Cash が引かれているのか？\n\n\n上記の式では、なぜ \\Delta STDEBT が加算されているのか？\n\n\n合併や売却が「利益への影響がない」というのは本当だろうか？ 利益への影響の欠如は推定問題にとって重要だろうか？ 利益への影響がないが、営業キャッシュフローに影響を与える取引はあるだろうか？\n\n\nHribar and Collins (2002) (上記) と Sloan (1996) のアクルーアルの定義にはどのような違いがあるか？ どちらの定義がより理にかなっていると思いますか？ なぜですか？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>アクルーアル</span>"
    ]
  },
  {
    "objectID": "chap15_accruals.html#シミュレーション分析",
    "href": "chap15_accruals.html#シミュレーション分析",
    "title": "\n7  アクルーアル\n",
    "section": "\n7.3 シミュレーション分析",
    "text": "7.3 シミュレーション分析\n\nここでは、いくつかのシミュレーション分析を考える。  Sloan (1996) のH1の根拠をよりよく理解するための分析の一つの理由である。\n\nここでシミュレーション分析を行う第二の理由は、その提供する力を示すことである。  多くの文脈では、推定量の性質を導出したり、現象がどのように相互作用するかを理解することは非常に複雑である。  多くの研究者は、自分の分析を導くために直感に頼るが、そのような直感は信頼性に欠けることがある。  例えば、「FM-NW」法が時系列依存性と断面依存性の両方に対して頑健な標準誤差を提供するという考えは、強い直感的な魅力があるが、第5章でその直感が単純に間違っていることを見た。\n\n\n7.3.1 ベクトル\n\nシミュレーション分析では、これまでの章よりも、基本的なRの機能をより広範に使用する。  R for Data Science の第28章「基本Rのフィールドガイド」は、次のセクションのコードがわかりにくい場合に役立つかもしれません。  ここでは、現金フローをシミュレーションし、現金で商品を購入し、マークアップを加えて売却する単純な企業の会計を考慮している。\n\n\n7.3.2 シミュレーション関数\n\n前に見たように、分析では関数を自由に使用することが良いコーディングの習慣である。2  この場合、シミュレーションの中核を関数に埋め込む。\n\n以下のシミュレーション関数 get_data() は、単一の「企業」のデータの時系列を生成し、2つの引数を受け入れる。  get_data() の最初の引数は add_perc で、デフォルト値は0.03である。  add_perc の値は、貸倒引当金の額を決定する。  2番目の引数は n_years で、デフォルト値は20である。  n_years の値は、シミュレーションで生成されるデータの年数を決定する。\n\nシミュレーションは、さまざまなキャッシュフローとそれを表す財務諸表を生成する。  モデルの主要なドライバーは売上であり、自己回帰プロセスに従う。  t 期の売上を S_t と表すと、次のようになる。\n\nS_t - \\bar S = \\rho (S_{t-1} - \\bar S) + \\varepsilon _t\n\n\nここで， \\rho \\in (0,1) と \\varepsilon _t \\sim N(0, \\sigma^2) である。\n\n売上は、売上時に現金支出が必要とされると仮定される売買費用と、すべての売上が勘定に記載されると仮定される売掛金を推進する。  モデルは、回収、債権放棄、配当にも対応している。  当社のモデルには在庫はありません。\n\nシミュレーション関数では、「基本R」の機能をかなり使用している。  変数を生成するために mutate() を使用する代わりに、変数をベクトルとして返す $ 表記を使用して変数を参照する。  例えば、df |&gt; select(ni) は、単一の列を持つデータフレームを返す。  対照的に、df$ni は、同じ基礎データを取得するが、ベクトルとして取得する。  株主資本（se）を計算するために、初期値（ t=0 ）を beg_se に設定する。  次に、株主資本の期末残高を、初期株主資本に純利益を加え、配当を引いたものとして計算する。\n\nget_data &lt;- function(add_perc = 0.03, n_years = 20) {\n\n    # パラメータの設定\n    add_true   &lt;- 0.03\n    gross_margin &lt;- 0.8\n    beg_cash   &lt;- beg_se &lt;- 1500\n    div_payout &lt;- 1\n    mean_sale  &lt;- 1000\n    sd_sale    &lt;- 100\n    rho        &lt;- 0.9\n\n    # Generate sales as an AR(1) process around mean_sale\n    sale_err &lt;- rnorm(n_years, sd = sd_sale) # 売上の誤差を作成\n    sales &lt;- vector(\"double\", n_years) # 売上を格納するベクトルを作成\n    sales[1] &lt;- mean_sale + sale_err[1] # 平均売上高に誤差を加える\n    for (i in 2:n_years) {              # AR(1) 仮定に基づいて売上を生成\n      sales[i] = mean_sale + rho * ( sales[i-1] - mean_sale ) + sale_err[i]\n    }\n\n    # データを結合してdata.frameを作成\n    df &lt;- tibble(year = 1:n_years,\n                 add_perc = add_perc,\n                 sales,\n                 # 初期値は欠損値で\n                 writeoffs = NA, collect = NA,\n                 div = NA, se = NA, ni = NA,\n                 bde = NA, cash = NA)\n\n    # COGSは売上高の一定割合と仮定\n    df$cogs &lt;- (1 - gross_margin) * df$sales\n\n    # 売上は全て掛売上で，回収・貸倒損失は翌期\n    df$ar &lt;- df$sales\n\n    # 貸倒引当金addを設定\n    df$add &lt;- add_perc * df$sales\n\n    # 前期の値を設定\n    df$writeoffs[1] &lt;- 0\n    df$collect[1] &lt;- 0\n    df$bde[1] &lt;- df$add[1]\n    df$ni[1] &lt;- df$sales[1] - df$cogs[1] - df$bde[1]\n    df$div[1] &lt;- df$ni[1] * div_payout\n    df$cash[1] &lt;- beg_cash + df$collect[1] - df$cogs[1] - df$div[1]\n    df$se[1] &lt;- beg_se + df$ni[1] - df$div[1]\n\n    # 2年目からループ設定\n    for (i in 2:n_years) {\n      df$writeoffs[i] &lt;- add_true * df$ar[i-1]\n      df$collect[i] &lt;- (1 - add_true) * df$ar[i-1]\n      df$bde[i] = df$add[i] - df$add[i-1] + df$writeoffs[i]\n      df$ni[i] &lt;- df$sales[i] - df$cogs[i] - df$bde[i]\n      df$div[i] &lt;- df$ni[i] * div_payout\n      df$cash[i] &lt;- df$cash[i-1] + df$collect[i] - df$cogs[i] - df$div[i]\n      df$se[i] &lt;- df$se[i-1] + df$ni[i] - df$ni[i]\n    }\n\n    df\n}\n\n\nこのような関数を理解するためには、引数の値を設定することが役立つ（例：add_perc &lt;- 0.03; n_years &lt;- 20）し、コードの各行を一つずつ進めながら、その際に df などの変数の内容を確認することが役立つ。3\n\n1000年分のデータを生成しよう。\n\nset.seed(2021)\ndf_1000 &lt;- get_data(n_years = 1000)\n\nThe first 20 years are shown in Figure 15.1.\n\ndf_1000 |&gt;\n  filter(year &lt;= 20) |&gt;\n  ggplot(aes(x = year)) +\n  geom_line(aes(y = sales), colour = \"red\") +\n  geom_line(aes(y = mean(sales)), colour = \"blue\")\n\n\n次に、シミュレーションデータを生成するために使用できる add_perc パラメータの5,000個のランダム値を生成しよう。\n\nadd_percs &lt;- runif(n = 5000, min = 0.01, max = 0.05)\n\n\nadd_percs の各値に対してシミュレーションデータを生成し、これらのデータを res_list というリストに格納する。\n\nset.seed(2021)\n\nres_list &lt;-\n  map(add_percs, get_data) |&gt;\n  system_time()\n\n\nres_list の生成にはわずか数秒しかかからないが、get_data() の各反復が他の反復と独立しているため、さらに高速に行うために future パッケージと plan(multisession) を使用することができる。\n\nplan(multisession)\n\nres_list &lt;-\n  future_map(add_percs, get_data,\n             .options = furrr_options(seed = 2021)) |&gt;\n  system_time()\n\n\n次に、2つのデータフレームを作成する。  最初のデータフレーム（res_df）は、id フィールドを使用して、1つのシミュレーション実行を他の実行と区別するためにすべてのデータを1つのデータフレームに格納する。  これらの実行は、それぞれが他の実行と独立していると考えられるかもしれない。\n\nres_df &lt;- list_rbind(res_list, names_to = \"id\")\n\n\n結果をまとめやすくするために、get_coefs() を作成し、収益をその遅延値に回帰した際の係数として持続性を計算し、その値を返す。 これはSloan (1996) と似た仕様の関数である。\n\nget_coefs &lt;- function(df) {\n  fm &lt;-\n    df |&gt;\n    arrange(year) |&gt;\n    mutate(lag_ni = lag(ni)) |&gt;\n    lm(ni ~ lag_ni, data = _)\n\n  tibble(add_perc = mean(df$add_perc),\n         persistence = fm$coefficients[2])\n}\n\n\nget_coefs() を res_list に適用し、その結果を2番目のデータフレーム results に格納する。\n\nresults &lt;-\n  res_list |&gt;\n  map(get_coefs) |&gt;\n  list_rbind(names_to = \"id\")\n\n\n推定された持続性の値を add_perc の仮定値に対してプロットする。\n\nresults |&gt;\n  ggplot(aes(x = add_perc, y = persistence)) +\n  geom_point()\n\n\n\n7.3.3 練習問題\n\n\nシミュレーションされた財務諸表データを生成する際には、一般的に生成されたデータがいくつかの基本的な要件を満たしていることを確認することが重要となる。  これらのデータについて成立すると期待される基本的な関係は何か？df_1000のデータについては成立しているか？\n\n\n営業活動によるキャッシュフローと財務活動によるキャッシュフローの値を計算せよ。 \n\n\n\n図15.1から売上を生成する基礎プロセスの詳細はどの程度明確か？ より多くのデータを見ることが有用か？ (明らかに、定常プロセスを持つ企業の1000年分のデータを持つことは一般的ではない。)\n\n\n用いられるべき「正しい」add_perc の値は何か？  上記の結果からのプロットを使用して、その値からの離れた値と持続性の関係は何か？  これはあなたの直感と一致しているか？  何が起こっているのか？  add_perc に関連する会計のどの側面が非現実的に見えるか？  (ヒント: 0.03 の代わりにさまざまな値を使用して set.seed(2021); get_data(0.03) の変形を使用し、利益プロセスがどのように影響を受けるかを調べると役立つかもしれない。)\n\n\nシミュレーション分析は、Sloan (1996) のH1の根拠についての基本的な理由に言及していますか？  もしそうならなぜか？  そうでない場合、分析に欠けている要素は何か？  どのようにしてシミュレーションを変更して、欠落している要素を組み込むことができるか？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>アクルーアル</span>"
    ]
  },
  {
    "objectID": "chap15_accruals.html#replicating-sloan-1996",
    "href": "chap15_accruals.html#replicating-sloan-1996",
    "title": "\n7  アクルーアル\n",
    "section": "\n7.4 Replicating Sloan (1996)",
    "text": "7.4 Replicating Sloan (1996)\n\nSloan (1996) の実証分析のいくつかの要素をよりよく理解するために、分析の再現を行う。\n\nまず、データをまとめる。  最初に、分析で使用するテーブルに接続する。\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nfunda &lt;- tbl(db, Id(schema = \"comp\", table = \"funda\"))\ncompany &lt;- tbl(db, Id(schema = \"comp\", table = \"company\"))\nccmxpf_lnkhist &lt;- tbl(db, Id(schema = \"crsp\", table = \"ccmxpf_lnkhist\"))\nmsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"msf\"))\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nfunda &lt;- load_parquet(db, schema = \"comp\", table = \"funda\")\ncompany &lt;- load_parquet(db, schema = \"comp\", table = \"company\")\nccmxpf_lnkhist &lt;- load_parquet(db, schema = \"crsp\", table = \"ccmxpf_lnkhist\")\nmsf &lt;- load_parquet(db, schema = \"crsp\", table = \"msf\")\n\n\n\n\n\n次に、comp.fundaのサブセットを作成し、comp.companyからSICデータを追加して、その結果を funda_mod と呼ぶ。\n\n利用可能な場合は「歴史的」SICコード（sich）を提供する sich を使用して SICコードを構築しますが、sich が利用できない場合は comp.company で見つかる「ヘッダー」SICコード（sic）を使用します。4  何らかの理由で、comp.company では sic が文字変数であり、comp.funda では sich が整数です。  したがって、ここで2つのテーブルをマージする前に sic を整数に変換します。\n\nsics &lt;-\n  company |&gt;\n  select(gvkey, sic) |&gt;\n  mutate(sic = as.integer(sic))\n\nfunda_mod &lt;-\n  funda |&gt;\n  filter(indfmt == \"INDL\", datafmt == \"STD\",\n         consol == \"C\", popsrc == \"D\") |&gt;\n  left_join(sics, by = \"gvkey\") |&gt;\n  mutate(sic = coalesce(sich, sic))\n\n\n次に、Sloan (1996) と同じサンプル選択基準を適用する。  NYSE と AMEX の企業年（つまり、それぞれ exchg が 11 と 12 に等しいもの）および 1962 年から 1991 年の間の年に焦点を当てる。\n\nSloan (1996, p.1) は、「銀行、生命保険、損害保険会社については、運転アクルーアルを計算するために必要な財務諸表データがCompustatにはない」と述べている。  ただし、これらの企業が明示的に除外されているか（例：SICコードでフィルタリングされているか）、単にアクルーアルを計算するためのデータが利用可能であることを要求しているかどうかは明確ではない。  そのため、これらの企業を保持し、関連する指標変数（finance）を作成する。\n\n次のステップは、主要な変数の変化を反映する変数を作成することです。  これには lag() を使用できます。5\n\nacc_data_raw &lt;-\n  funda_mod |&gt;\n  filter(!is.na(at),\n         pddur == 12,\n         exchg %in% c(11L, 12L)) |&gt;\n  mutate(finance = between(sic, 6000, 6999),\n         across(c(che, dlc, txp), \\(x) coalesce(x, 0))) |&gt;\n  group_by(gvkey) |&gt;\n  window_order(datadate) |&gt;\n  mutate(avg_at = (at + lag(at)) / 2,\n         d_ca = act - lag(act),\n         d_cash = che - lag(che),\n         d_cl = lct - lag(lct),\n         d_std = dlc - lag(dlc),\n         d_tp = txp - lag(txp)) |&gt;\n  select(gvkey, datadate, fyear, avg_at, at, oiadp, dp, finance,\n         starts_with(\"d_\"), sic, pddur) |&gt;\n  mutate(acc_raw =  (d_ca - d_cash) - (d_cl - d_std - d_tp) - dp) |&gt;\n  ungroup() |&gt;\n  filter(between(fyear, 1962, 1991),\n         avg_at &gt; 0)\n\n\nデータの準備の最終ステップでは、Sloan (1996) で見つかる定義に従って、earn、acc、cfoのコア変数を計算し、earnの先行値を格納する変数を作成する（上記で使用した lag() 関数を補完するウィンドウ関数である lead() 関数を使用）、acc、earn、cfo、lead_earnのデシルを作成し、2桁のSICコードを作成し、最後に、金融企業とaccの値がない観測をフィルタリングする。\n\nacc_data &lt;-\n  acc_data_raw |&gt;\n  mutate(earn = oiadp / avg_at,\n         acc = acc_raw / avg_at,\n         cfo = earn - acc) |&gt;\n  group_by(gvkey) |&gt;\n  window_order(datadate) |&gt;\n  mutate(lead_earn = lead(earn)) |&gt;\n  ungroup() |&gt;\n  collect() |&gt;\n  mutate(acc_decile = ntile(acc, 10),\n         earn_decile = ntile(earn, 10),\n         cfo_decile = ntile(cfo, 10),\n         lead_earn_decile = ntile(lead_earn, 10),\n         sic2 = str_sub(as.character(sic), 1, 2)) |&gt;\n  filter(!finance, !is.na(acc))\n\n\n次のステップは、各企業年の株価収益データを収集することである。  GVKEY（Compustat）をPERMNO（CRSP）にリンクするために、第7章で見た ccm_link を使用する。\n\nccm_link &lt;-\n  ccmxpf_lnkhist |&gt;\n    filter(linktype %in% c(\"LC\", \"LU\", \"LS\"),\n           linkprim %in% c(\"C\", \"P\")) |&gt;\n    rename(permno = lpermno) |&gt;\n    mutate(linkenddt = coalesce(linkenddt, max(linkenddt, na.rm = TRUE))) |&gt;\n  select(gvkey, permno, linkdt, linkenddt)\n\n\nSloan (1996) に従い、（gvkey、datadate）の組み合わせをpermnoとリンクし、決算期間の終了後4か月から始まる12か月間の期間を指定する。\n\ncrsp_link &lt;-\n  acc_data_raw |&gt;\n  select(gvkey, datadate) |&gt;\n  inner_join(ccm_link,\n             join_by(gvkey, between(datadate, linkdt, linkenddt))) |&gt;\n  select(gvkey, datadate, permno) |&gt;\n  mutate(start_month = as.Date(floor_date(datadate + months(4L), \"month\")),\n         end_month = as.Date(floor_date(datadate + months(16L) - days(1L),\n                                        \"month\")),\n         month = floor_date(datadate, 'month'))\n\n\n次に、このウィンドウに対して各（gvkey、datadate、permno）の組み合わせに対して複利収益を計算する。\n\ncrsp_data &lt;-\n  msf |&gt;\n  inner_join(crsp_link,\n             by = join_by(permno, between(date, start_month, end_month))) |&gt;\n  group_by(gvkey, permno, datadate) |&gt;\n  summarize(ret = exp(sum(log(1 + ret), na.rm = TRUE)) - 1,\n            n_months = n(),\n            .groups = \"drop\") |&gt;\n  collect()\n\n\nSloan (1996, p.304) の表4では、異常収益が使用されており、「生の買いホールドリターンを取り、市場価値加重ポートフォリオの買いホールドリターンを引くことによって計算される。  サイズポートフォリオは、NYSEおよびAMEX企業の株式の時価総額デシルに基づいている。  上記で個々の企業の収益を取得したが、サイズポートフォリオのデータ、各ポートフォリオの収益と時価総額のカットオフを収集する必要がある。\n\nサイズポートフォリオのデータは、第11章で見たように、Ken Frenchのウェブサイトから取得される。  第11章で使用されたコードのようなコードは、farrパッケージに2つの関数 get_size_rets_monthly() と get_me_breakpoints() として含まれている。\n\nsize_rets &lt;- get_size_rets_monthly()\nsize_rets\n\n\nget_size_rets_monthly() によって返されるテーブルには、等加重ポートフォリオ（ew_ret）と価値加重ポートフォリオ（vw_ret）に基づく2つの収益の測定値を含む4つの列がある。  Sloan (1996) と同様に、vw_ret を使用する。\n\nme_breakpoints &lt;- get_me_breakpoints()\nme_breakpoints\n\n\nget_me_breakpoints() によって返されるテーブルは、特定の月において me_min と me_max の間の時価総額を持つ企業が割り当てられるサイズデシル（decile）を識別する。\n\nCRSPをcrsp_linkと結合するために、crsp.msfのdateの各値に対して変数monthを構築する。  これにより、datadateの値がcrsp.msfのdateの値と整合しないことによる非一致を防ぐ。\n\ncrsp_dates &lt;-\n  msf |&gt;\n  distinct(date) |&gt;\n  mutate(month = floor_date(date, 'month'))\n\n\n次のコードは、datadateの月に適用される時価総額とサイズのカットオフに従って、企業年（つまり、（permno、datadate）の組み合わせ）をサイズデシルに割り当てる。\n\nme_values &lt;-\n  crsp_link |&gt;\n  inner_join(crsp_dates, by = \"month\") |&gt;\n  inner_join(msf, by = c(\"permno\", \"date\")) |&gt;\n  mutate(mktcap = abs(prc) * shrout / 1000) |&gt;\n  select(permno, datadate, month, mktcap) |&gt;\n  collect()\n\nme_decile_assignments &lt;-\n  me_breakpoints |&gt;\n  inner_join(me_values,\n             join_by(month, me_min &lt;= mktcap, me_max &gt; mktcap)) |&gt;\n  select(permno, datadate, decile)\n\n\n各datadateとサイズデシルについて、以下のコードは、datadateの4か月後から始まる12か月間の累積収益を計算する。\n\ncum_size_rets &lt;-\n  me_decile_assignments |&gt;\n  select(datadate, decile) |&gt;\n  distinct() |&gt;\n  mutate(start_month = datadate + months(4),\n         end_month =  datadate + months(16)) |&gt;\n  inner_join(size_rets,\n             join_by(decile, start_month &lt;= month, end_month &gt;= month)) |&gt;\n  group_by(datadate, decile) |&gt;\n  summarize(ew_ret = exp(sum(log(1 + ew_ret), na.rm = TRUE)) - 1,\n            vw_ret = exp(sum(log(1 + vw_ret), na.rm = TRUE)) - 1,\n            n_size_months = n(),\n            .groups = \"drop\")\n\n\nこれで、サイズ調整収益を計算するために必要なデータが揃った。  単純な差として size_adj_ret を計算するために、crsp_data と me_decile_assignments、そして cum_size_rets を組み合わせるだけである。\n\nsize_adj_rets &lt;-\n  crsp_data |&gt;\n  inner_join(me_decile_assignments, by = c(\"permno\", \"datadate\")) |&gt;\n  inner_join(cum_size_rets, by = c(\"datadate\", \"decile\")) |&gt;\n  mutate(size_adj_ret = ret - vw_ret) |&gt;\n  select(gvkey, datadate, size_adj_ret, n_months, n_size_months)\n\n\n本稿の回帰分析では、Compustatの処理されたデータ（acc_data）をサイズ調整収益（size_adj_rets）の新しいデータと結合するだけである。\n\nreg_data &lt;-\n  acc_data |&gt;\n  inner_join(size_adj_rets, by = c(\"gvkey\", \"datadate\"))\n\n\n回帰分析を実行する前に、データを調査することが重要である。  有用な基準の1つは、Sloan (1996) の Table 1 に報告されている記述統計である。  Table 15.2 で見られる値が Sloan (1996) で報告されている値と類似していることにより、ある程度の保証が提供される。\n\nreg_data |&gt;\n  group_by(acc_decile) |&gt;\n  summarize(across(c(acc, earn, cfo), \\(x) mean(x, na.rm = TRUE)))\n\n\n\n7.4.1 Sloan (1996)の表2\n\nデータの非常に基本的なチェックを行った後、Sloan (1996) に見られるいくつかの回帰分析の類似物を作成することができる。\n\nTable 15.3 に示されている出力は、Sloan (1996) の Table 2 にある「プールされた」結果に対応している。\n\nfms &lt;- list(lm(lead_earn ~ earn, data = reg_data),\n            lm(lead_earn_decile ~ earn_decile, data = reg_data))\n\nmodelsummary(fms,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\", \"r.squared\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\n\nSloan (1996) の Table 2 のような「業界レベル」の分析を生成するために、業界ごとに回帰係数を生成する小さな関数 run_table_ind() を作成する。  第14章で見たように、関数に供給される sic2 の値を reg_data で見つかる変数 sic2 と区別するために !! を使用する。6\n\nrun_table_ind &lt;- function(sic2, lhs = \"lead_earn\", rhs = \"earn\") {\n  df &lt;-\n    reg_data |&gt;\n    filter(sic2 == !!sic2)\n\n  fm &lt;- lm(as.formula(str_c(lhs, \" ~ \", rhs)), data = df)\n\n  coefs &lt;- as_tibble(t(fm$coefficients))\n  names(coefs) &lt;- colnames(t(fm$coefficients))\n  bind_cols(sic2 = sic2, coefs)\n}\n\n\n関数 stats_for_table() は記述統計をまとめる。\n\nstats_for_table &lt;- function(x) {\n  qs &lt;- quantile(x, probs = c(0.25, 0.50, 0.75), na.rm = TRUE)\n\n  tibble(mean = mean(x, na.rm = TRUE),\n         q1 = qs[1], median = qs[2], q3 = qs[3])\n}\n\n\n最後に、summ_for_table() は run_table_ind() と stats_for_table() を呼び出し、サマリーテーブルを生成する。\n\nsumm_for_table &lt;- function(lhs = \"lead_earn\", rhs = \"earn\") {\n  reg_data |&gt;\n    distinct(sic2) |&gt;\n    pull() |&gt;\n    map(run_table_ind, lhs = lhs, rhs = rhs) |&gt;\n    list_rbind() |&gt;\n    select(-sic2) |&gt;\n    map(stats_for_table) |&gt;\n    list_rbind(names_to = \"term\")\n}\n\n\nTable 15.4 と Table 15.5 は、Sloan (1996) の Table 2 に報告されている「業界レベル」の結果に対応している。\n\nsumm_for_table(lhs = \"lead_earn\", rhs = \"earn\")\n\nsumm_for_table(lhs = \"lead_earn_decile\", rhs = \"earn_decile\")\n\n\nこれまでの結果は、Sloan (1996) の Table 2 と「質的に類似している」と言えるかもしれない。  従属変数が lead_earn である回帰分析における earn のプールされた係数の大きさが異なる点が主な違いであるかもしれない。  Sloan (1996) のTable 2では、 0.841 という係数が報告されており、これは業界レベルの回帰分析の平均係数（ 0.773 ）よりも著しく高い。  一方、本稿のプールされた分析と業界レベルの分析の平均係数はお互いに非常に近い。\n\n\n7.4.2 Sloan (1996)の表3\n\nSloan (1996) では、Table 15.3 の右辺の変数を貸借対照表とキャッシュフローのコンポーネントに分解している。  これらの分析を Table 15.6 で再現する。\n\nfms &lt;- list(lm(lead_earn ~ acc + cfo, data = reg_data),\n            lm(lead_earn_decile ~ acc_decile + cfo_decile, data = reg_data))\n\nmodelsummary(fms,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\", \"r.squared\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\n\nTable 15.7 と Table 15.8 は、Sloan (1996) の Table 3 に報告されている「業界レベル」の結果に対応している。  再び、Sloan (1996) で見られる結果と「質的に類似した」結果が得られている。\n\nsumm_for_table(lhs = \"lead_earn\", rhs = \"acc + cfo\")\n\nsumm_for_table(lhs = \"lead_earn_decile\", rhs = \"acc_decile + cfo_decile\")\n\n\n\n7.4.3 Pricing of earnings components\n\nSloan (1996) の Table 5 に報告されている分析の要素は、異常リターンを同時期の収益および遅れた収益のコンポーネントに回帰させることであり、私たちも同様に行う。\n\nmms &lt;- list(lm(size_adj_ret ~ lead_earn + acc + cfo,\n               data = reg_data),\n            lm(size_adj_ret ~ lead_earn_decile + acc_decile + cfo_decile,\n               data = reg_data))\n\n\nTable 15.9 は、この回帰分析の結果を示している。\n\nmodelsummary(mms,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\", \"r.squared\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\n\nSloan (1996) の表記法では、acc の係数は - \\beta \\gamma_1^* と表すことができ、これは \\beta （lead_earn に対する係数、つまり size_adj_ret とほぼ同時期の収益）と \\gamma ^*_1 （帳簿価額の暗黙の市場係数）の積をマイナスにしたものである。\n\n\\hat{\\beta} = 2.538 および \\hat{\\beta \\gamma _1^*} = 1.961 の推定値から、暗黙の推定値 \\hat{\\gamma}_1^* = 0.7726 を得る。  この推定値 \\hat{\\gamma }_1^* = 0.7726 は、\\hat{\\gamma }_1 = 0.6399 の推定値よりも高い。  しかし、これら2つの係数の差が統計的に有意であると結論付けることができるだろうか？\n\nこの問いに対する1つのアプローチは、市場回帰から推定された \\hat{\\beta \\gamma _1^*} = 1.961 が、\\hat{\\beta} \\times \\hat{\\gamma}_1 = 2.538 \\times 0.6399 = 1.624 によって暗示される値と統計的に異なるかどうかを評価することである。\n\nしかし、Mishkin (1983) が指摘しているように、この手順は「 \\hat{\\gamma}_1 の推定値に不確実性がないという暗黙の前提を含んでいる。  これにより、パラメータの標準誤差の一貫性のない推定値が生じ、したがって、仮定されたF分布を持たない検定統計量が得られる。  これは「不適切な推論につながる可能性がある…」 7 。\n\n上記の「不適切な推論」の問題を考慮すると、Mishkin (1983) は「反復重み付き非線形最小二乗法」（Sloan, 1996, p.1）を使用して方程式系を推定し、制約のない方程式系と制約のある方程式系（つまり、両方の方程式で \\gamma が等しいと制約された方程式系）の適合度を比較してF統計量を計算する。  Sloan (1996) は、表4および表5に報告されている分析でこの「Mishkin (1983)」テストを使用しているが、このアプローチは複雑さが大きい。8\n\n幸いなことに、Abel and Mishkin (1983) は、Mishkinテストと漸近的に等価であることを示すより簡単なアプローチを提案している。  このアプローチの直感は、遅れた収益のコンポーネント（負債計上額とキャッシュフロー）が株価リターンを予測する方法で過大評価されている場合、株価リターンを遅れた収益のコンポーネントに回帰させることでこれが明らかになるはずであるということである。  Kraft et al. (2007) は、MishkinテストとAbel and Mishkin (1983)が使用するアプローチについての追加の議論を提供している。9  実際には、このアプローチにより、回帰分析において同時期の収益を経由することなく、遅れた収益のコンポーネントを直接使用することができる。\n\nTable 15.10 の回帰結果は、Abel and Mishkin (1983) が提案したアプローチを適用して得られたものである。\n\neff &lt;- list(lm(size_adj_ret ~ acc + cfo, data = reg_data),\n            lm(size_adj_ret ~ acc_decile + cfo_decile, data = reg_data))\n\nmodelsummary(eff,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\", \"r.squared\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\n\nTable 15.10 から、前期のアクルーアルは異常リターンと負の関連があることがわかる。  この結果は、市場が負債計上額を過大評価しているということを示しており、これはあまりに高い持続性を前提としている。\n\n7.4.4 Exercises\n\nIn creating acc_data_raw, we used coalesce() to set the value of certain variables to zero when missing on Compustat. Does this seem appropriate here? Are the issues similar to those observed with regard to R&D in Chapter 8? It may be helpful to find some observations from recent years where this use of the coalesce() function has an effect and think about the issues in context of financial statements for those firm-years.\nCan you reconcile the results from the Abel and Mishkin (1983) test with those from the previous regressions? (Hint: Pay attention to sample composition; you may need to tweak these regressions.)\nThe equations estimated in Table 5 of Sloan (1996) could be viewed as a structural (causal) model. Can you represent this model using a causal diagram? In light of the apparent econometric equivalence between that structural model and the estimation approach used in Abel and Mishkin (1983), how might the coefficients from the structural model be recovered from the latter approach?\n\n\nこの批判は、Sloan (1996) の係数に因果関係の解釈が含まれていることを示唆している。  Kraft et al. (2007) の批判は、上記の因果関係図にどのように表現されるだろうか？  Kraft et al. (2007) の批判はどの程度説得力があると感じるだろうか？\nApart from the different data sources used, another difference between the simulation analysis earlier in this chapter and the regression analysis in Table 3 of Sloan (1996) is the regression model used. Modify the code below to incorporate the appropriate formulas for cash flow from operating activities (cfo) and accruals (acc). Then replicate the pooled analysis of Panel A of Table 3 of Sloan (1996) using the resulting sim_reg_data data frame. What do you observe?\n\n\nsim_reg_data &lt;-\n  res_df |&gt;\n  mutate(cfo = [PUT CALC HERE], acc = [PUT CALC HERE]) |&gt;\n  group_by(id) |&gt;\n  arrange(id, year) |&gt;\n  mutate(lag_cfo = lag(cfo),\n         lag_acc = lag(acc)) |&gt;\n  ungroup()\n\n\nWhich hypothesis does Figure 1 of Sloan (1996) relate to? What aspects of the plot make it easier or more difficult to interpret the results? The following code replicates a version of Figure 1 from Sloan (1996) using our simulated data. On the basis of Figures 15.3–15.5 and the arguments given in Sloan (1996), is H1 true in our simulated data? Given the other analysis above, is H1 true in our simulated data?\n\n\nyear_of_event &lt;- 10\n\ndecile_data &lt;-\n  sim_reg_data |&gt;\n  filter(year == year_of_event) |&gt;\n  mutate(cfo_decile = ntile(cfo, 10),\n         ni_decile = ntile(ni, 10),\n         acc_decile = ntile(acc, 10)) |&gt;\n  select(id, ends_with(\"decile\"))\n\nreg_data_deciles &lt;-\n  sim_reg_data |&gt;\n  inner_join(decile_data, by = \"id\")\n\n\n次のコードは、図15.3を生成する。\n\nreg_data_deciles |&gt;\n  filter(ni_decile %in% c(1, 10)) |&gt;\n  mutate(ni_decile = as.factor(ni_decile),\n         event_year = year - year_of_event) |&gt;\n  group_by(ni_decile, year) |&gt;\n  summarize(ni = mean(ni, na.rm = TRUE), .groups = \"drop\") |&gt;\n  ggplot(aes(x = year, y = ni, group= ni_decile, color = ni_decile)) +\n  geom_line()\n\n\n次のコードは、図15.4を生成する。\n\nreg_data_deciles |&gt;\n  filter(cfo_decile %in% c(1, 10)) |&gt;\n  mutate(cfo_decile = as.factor(cfo_decile),\n         event_year = year - year_of_event) |&gt;\n  group_by(cfo_decile, year) |&gt;\n  summarize(ni = mean(ni, na.rm = TRUE), .groups = \"drop\") |&gt;\n  ggplot(aes(x = year, y = ni, group = cfo_decile, color = cfo_decile)) +\n  geom_line()\n\n\n次のコードは、図15.5を生成する。\n\nreg_data_deciles |&gt;\n  filter(acc_decile %in% c(1, 10)) |&gt;\n  mutate(acc_decile = as.factor(acc_decile),\n         event_year = year - year_of_event) |&gt;\n  group_by(acc_decile, year) |&gt;\n  summarize(ni = mean(ni, na.rm = TRUE), .groups = \"drop\") |&gt;\n  ggplot(aes(x = year, y = ni, group = acc_decile, color = acc_decile)) +\n  geom_line()",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>アクルーアル</span>"
    ]
  },
  {
    "objectID": "chap15_accruals.html#アクルーアルアノマリー",
    "href": "chap15_accruals.html#アクルーアルアノマリー",
    "title": "\n7  アクルーアル\n",
    "section": "\n7.5 アクルーアル・アノマリー",
    "text": "7.5 アクルーアル・アノマリー\n\nSloan (1996) の Table 6 は、市場がアクルーアルを誤って価格設定していることから、異常リターンを生む取引戦略を示している。  このような戦略は一般的にアノマリーと呼ばれ、効率的市場仮説と矛盾しているように見える（10.1節を参照）。  Fama and French (2008, p. 1653) は、アノマリーを「平均株価リターンのパターンであり、…資本資産価格モデル(CAPM)で説明されない」と定義している。  Fama and French (2008) には、CAPMが市場リスクの真のモデルであるという概念が含まれているように思われ、Fama and French (2008) の定義の一般的なバージョンでは、CAPMを仮定された市場リスクの真のモデルに置き換えることになる。  Dechow et al. (2011, p. 23) は、「アクルーアル異常は実際には全く異常ではない」と主張している。  実際、アクルーアル異常を文書化した最初の研究は、それが存在すると予測していた。  アノマリーという用語は、既存の理論から逸脱する行動に通常予約されているが、Sloan (1996) が最初にアクルーアル異常を文書化したとき、彼はよく知られた理論を検証して、それが支持されたことを発見した。\n\nTable 6 は、年 t+1 ， t+2 ，および t+3 のポートフォリオリターンを提供しているが、上記の手順では年 t+1 のリターンのみを収集している。  したがって、Table 15.11 は Sloan (1996) の Table 6 の最初の列のみを再現している。\n\nfm &lt;-\n  reg_data |&gt;\n  group_by(fyear, acc_decile) |&gt;\n  summarize(size_adj_ret = mean(size_adj_ret, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  mutate(acc_decile = as.factor(acc_decile)) |&gt;\n  lm(size_adj_ret ~ acc_decile - 1, data = _)\n\nmodelsummary(fm,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\", \"r.squared\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\n\nhedge_ret &lt;- fm$coefficients[\"acc_decile1\"] - fm$coefficients[\"acc_decile10\"]\np_val &lt;- linearHypothesis(fm, \"acc_decile1 = acc_decile10\")$`Pr(&gt;F)`[2]\n\n\nヘッジポートフォリオリターン (hedge_ret) は 0.1322 で、p値は 4.4e-06 (p_val) です。\n\n\n7.5.1 ディスカッション問題2\n\n\nヘッジポートフォリオ回帰を推定する際、size_adj_ret = mean(size_adj_ret) を含めた理由は何ですか？  なぜこのステップが重要なのですか？\nGreen et al. (2011) say “the simplicity of the accruals strategy and the size of the returns it generates have led some scholars to conclude that the anomaly is illusory. Green et al. (2011)は、「アクルーアル戦略の単純さとそれが生み出すリターンの大きさが、一部の学者たちに、この異常は幻想であると結論付けさせた」と述べている。  たとえば，Khan (2008) や Wu et al. (2010) は，異常は誤ったリスクモデルや時間変動する割引率のq理論によって説明できると主張している。Desai et al. (2004) は，異常は異なる戦略に包含されているため欺瞞的であると結論付けている。Kraft et al. (2006) は，外れ値や先読みバイアスに帰する。Ng (2005) は，異常リターンは破産リスクへの高い露出の補償であると提案している。Zach (2006) は，アクルーアルと相関する企業特性がリターンパターンを引き起こすと主張している。  Sloan (1996) を見ると、上記の各論文で述べられている主張と矛盾すると思われる Sloan (1996) の証拠は何か？  上記の各論文で述べられている主張と矛盾すると思われる Sloan (1996) の証拠は何か？  Zach (2006) が代替の「原因」を主張するために提供する必要がある証拠は何か？\n\nGreen et al. (2011) は、Q1で引用された代替説明に対処しているか？  彼らはそうする必要があると思うか？\n\nGreen et al. (2011) が提供するヘッジファンドの役割に関する証拠はどの程度説得力があると思いますか？\n\n\nXie (2001, p.360)は、「Compustatのアイテム#308が利用できない1988年以前の企業年度については、次のように推定する…」と述べている。  なぜアイテム#308が1988年以前に利用できないのか？ Compustatの現在の#308に相当するものは何か？\n\nXie (2001) の p.361 にラベル付けされた実証モデル(式1)を調べなさい。  (これは、Jones (1991) の「ジョーンズモデル」であり、第16章で説明する。)  このモデルと残差を「異常アクルーアル」とラベル付けする際の暗黙の仮定は何か？  (ヒント: モデルの各構成要素を取り上げ、「通常の」アクルーアルの合理的なモデルとなる状況を特定しなさい。)\n\n「チャネルスタッフィング」とは何か？  (ヒント: Wikipediaにはこれについてのまともなエントリがある。)  チャネルスタッフィングが異常アクルーアルに与える影響は何か？  (ヒント: これを概念的に、および式(1)に関して考えてみなさい。)  これに答えるために、Xie (2001) で提供されている情報以上の情報が必要か？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>アクルーアル</span>"
    ]
  },
  {
    "objectID": "chap15_accruals.html#footnotes",
    "href": "chap15_accruals.html#footnotes",
    "title": "\n7  アクルーアル\n",
    "section": "",
    "text": "This is one definition that can be tightened and vary by context.↩︎\nWickham et al. (2023, p.441) suggest that “a good rule of thumb is to consider writing a function whenever you’ve copied and pasted a block of code more than twice.”↩︎\nIndeed, this is the process often used to create a function like this in the first place.↩︎\nRecall from Chapter 12 that a header variable is one where only the most recent value is retained in the database.↩︎\nWe introduced the lag() function in Chapters 7 and 8.↩︎\nChapter 19 of Wickham (2019) has more details on this “unquote” operator !!.↩︎\nNote that Mishkin (1983) is actually critiquing a different econometric procedure whereby residuals from the first regression are included in a version of the second, but the quoted criticism is equally applicable to the procedure we describe here.↩︎\nThis is apparent from inspection of the Stata .ado file provided by Judson Caskey to implement the Mishkin (1983) approach at https://sites.google.com/site/judsoncaskey/data.↩︎\nNote that Kraft et al.(2007) appear to assume that the OLS test used by Mishkin (1983) is the same as the test proposed in Abel and Mishkin (1983), but differences in these tests do not affect the substance of the discussion of Abel and Mishkin (1983).↩︎",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>アクルーアル</span>"
    ]
  },
  {
    "objectID": "chap01_Introduction.html#本書の構成",
    "href": "chap01_Introduction.html#本書の構成",
    "title": "\n1  はじめに\n",
    "section": "",
    "text": "第1章では，本書の概要，読書ガイド，およびコンピュータの設定手順について説明している。 \n\nこの本を最大限に活用するためには，データスキルが中心的な役割を果たすため，第2章では，Rの速習チュートリアル形式の紹介を提供している。 \n\n統計学や回帰分析についてほとんど知識がないことを前提としているため，第3章では，回帰分析の基本を紹介している。 \n\n第4章では，第3章を基に因果推論の要素を紹介している。 \n\n第5章では，統計的推論の基本を紹介している。これは実証的な会計研究の中核をなす部分である。\n\n\n\n\n第6章と第8章では，Compustatの紹介とWRDSを通じてデータにアクセスする方法を紹介している。 \n\n第7章では，異なるプロバイダーからのデータセットをリンクする方法に焦点を当て，Compustatからの財務諸表データをCRSPからの株価データとリンクする方法について説明している。 \n\n第1部は，後の章や（読者自身の）研究活動に役立つ追加のデータスキルを提供する第9章で締めくくっている。",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめに</span>"
    ]
  },
  {
    "objectID": "chap01_Introduction.html#コンピュータの設定",
    "href": "chap01_Introduction.html#コンピュータの設定",
    "title": "\n1  はじめに\n",
    "section": "\n1.2 コンピュータの設定",
    "text": "1.2 コンピュータの設定\nAssuming that you have the ability to install software and a WRDS account, setting up your computer so that you can run the code in this book is straightforward and takes just a few minutes. We list the required steps below and also provide a video demonstrating these steps online.\n\nDownload and install R. R is available for all major platforms (Windows, Linux, and MacOS) at https://cloud.r-project.org.\nDownload and install RStudio. An open-source version of RStudio is available on the Posit website.\nInstall the required packages from CRAN. CRAN stands for “Comprehensive R Archive Network” and is the official repository for packages (also known as libraries) made available for R. In this course, we will make use of a number of R packages. These can be installed easily by running the following code in RStudio.1\n\n\n# install.packages(\"pacman\") # first time only\npacman::p_load(\n    DBI, # Database Interface\n    MASS, # Modern Applied Statistics with S\n    MatchIt, # Propensity Score Matching\n    RPostgres, # PostgreSQL\n    arrow, # Apache Arrow\n    car, # Companion to Applied Regression\n    duckdb, # DuckDB\n    farr, # 著者のパッケージ\n    fixest, # 固定効果モデル\n    furrr, # 平行処理\n    glmnet, # ラッソ回帰\n    httr2, # HTTP\n    kableExtra,  # 作表\n    lmtest, # 統計検定\n    modelsummary, # モデルの要約\n    optmatch, # Propensity Score Matching\n    pdftools, #endregion PDF\n    plm, # 固定効果モデル\n    rdrobust,  # ロバスト推定\n    robustbase, # ロバスト推定\n    rpart, # 決定木\n    rpart.plot,  # 決定木\n    sandwich, # サンドイッチ推定量\n    tidyverse # データ処理\n    )\n\nNote that farr is an R package one of us created just for this course (Gow, 2022). (As the package is related to the course Financial Accounting Research at the University of Melbourne, farr stands for “Financial Accounting Research with R”.) Set up R to connect to the WRDS PostgreSQL database. To actually use much of the code from Chapter 6 on, you will need to tell R how to access WRDS data stored in its PostgreSQL database by running the following line within RStudio.\n\nSys.setenv(PGHOST = \"wrds-pgdata.wharton.upenn.edu\",\n           PGPORT = 9737L,\n           PGDATABASE = \"wrds\",\n           PGUSER = \"your_WRDS_ID\",\n           PGPASSWORD = \"your_WRDS_password\")\n\nObviously, you should replace your_WRDS_ID and your_WRDS_password with your actual WRDS ID and WRDS password, respectively. This code will need to be run each time you open RStudio to access WRDS data in the code examples below. But once you have run this code, you do not need to run it again during the same session (i.e., until you close and reopen RStudio).\nIf the only PostgreSQL database you access is the WRDS database, you could put the values above in .Renviron, a special file that is opened every time you open R (see here for more information on this file).2 The contents of this file would look something like this:\n\nPGHOST = \"wrds-pgdata.wharton.upenn.edu\"\nPGPORT = 9737L\nPGDATABASE = \"wrds\"\nPGUSER = \"your_WRDS_ID\"\nPGPASSWORD = \"your_WRDS_password\"\n\nWe discuss alternative approaches to setting up the WRDS database connection in Section 6.1, but we recommend this approach as it keeps the user-specific aspects of the code separate from the parts of the code that should work for everyone. By using environment variables, we ensure that the code in the book works for you if you copy it and paste it in your R console.\nNote that we have striven to make the code in each chapter independent of the code in other chapters. So, if you feel comfortable with using R and have fulfilled the requirements listed above, you could easily jump ahead to a chapter of interest and start running code.",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめに</span>"
    ]
  },
  {
    "objectID": "chap16_em.html#利益マネジメント尺度の評価",
    "href": "chap16_em.html#利益マネジメント尺度の評価",
    "title": "\n8  利益マネジメント\n",
    "section": "\n8.2 利益マネジメント尺度の評価",
    "text": "8.2 利益マネジメント尺度の評価\n\n自然な疑問として、Jones (1991)で使用されているような利益マネジメントの尺度がどれほどうまく機能するかということが挙げられる。  理想的な尺度は、利益マネジメントが存在するときにはそれを検出し、利益マネジメントが存在しないときにはそれを検出しない。  これにより、2つの疑問が生じる。  第1に、利益マネジメントが存在するときに、どのようにして特定の尺度が利益マネジメントを検出するか？  第2に、利益マネジメントが存在しないときに、特定の尺度がどのように振る舞うか？\n\nDechow et al. (1995) は、これらの条件に基づいて、先行研究からの5つの利益マネジメント尺度を評価している。  これらの尺度のそれぞれは、推定期間を使用して非裁量的アクルーアルのモデルを作成し、その後、推定された非裁量的アクルーアルとの差として、テスト期間の裁量的アクルーアルを測定する。  推定期間がt=1からt=Tまでであると仮定すると、これらの尺度は、企業iの年度\\tauについて以下のように定義される。\n\n\nHealyモデル（Healy、1985）は、推定期間中の平均総アクルーアルを非裁量的アクルーアルとして測定する。\n\n\nNDA_{i, tau} = \\frac{\\sum _{t=1}^{T} TA_{i,t}}{T}\n\n\nここで、TA_{i,t}は（以下も同様）遅延総資産でスケーリングされた総アクルーアルである。\n\n\nDeAngeloモデル（DeAngelo、1986）は、前期の総アクルーアルを非裁量的アクルーアルの尺度として使用する。\n\n\nNDA_{i, tau} = TA_{i,t}\n\n\n\nJonesモデル（Jones、1991）は、「企業の経済状況の変化が非裁量的アクルーアルに与える影響を制御しようとする」次のモデルを使用する。\n\n\nNDA_{i, tau} = \\alpha_1 (1/AT_{i, \\tau -1} ) + \\alpha_2 \\Delta REV_{i,\\tau} + \\alpha_3 PPE_{i, \\tau}\n\nここで AT_{i, \\tau -1} は企業 i の \\tau -1 時点の総資産、\\Delta REV _{i, \\tau} は \\tau 年度の売上高から \\tau - 1 年度の売上を引いたものを AT_{i, \\tau - 1} で基準化したもの、 PPE _{i, \\tau} は i 企業の \\tau 年度における有形固定資産を AT_{i, \\tau-1} で基準化したものである。\n\n\n\n修正Jonesモデル : Dechow et al. (1995)は、Jonesモデルの修正バージョンを考慮しており、「Jonesモデルが収益に対して裁量を行うときに裁量的アクルーアルを誤って測定するという推測される傾向を排除するために設計された」 (1995, p.199)。  このモデルでは、イベント期間中の非裁量的アクルーアルは次のように推定される。\n\n\nNDA_{i, \\tau} = \\alpha_1 (1/AT_{i, \\tau -1} ) +\n    \\alpha_2 ( \\Delta REV_{i,\\tau} - \\Delta REC_{i,\\tau} ) +\n    \\alpha_3 PPE_{i, \\tau}\n\nここで \\Delta REC_{i,\\tau} は、企業 i の \\tau 年度の売掛金から \\tau - 1 年度の売掛金を引いたものを AT_{i, \\tau - 1} で基準化したものである。\n\n\n\n産業モデル : 「非裁量的アクルーアルが時間とともに一定であるという仮定を緩和する。産業モデルは、非裁量的アクルーアルの決定要因の変動が、同じ業界の企業間で共通であると仮定している」 (1995)、p.199）。  このモデルでは、非裁量的アクルーアルは次のように計算される。\n\n\nNDA_{i, \\tau} = \\gamma _1 + \\gamma _2 \\text{median} (TA_{I, \\tau)})\n\nここで TA_{I,\\tau} は産業 I \\ (\\forall j \\in I) における全企業の TA_{j, \\tau } の値である。\n\n上記の各モデルでは、パラメータ（すなわち、(\\alpha _1, \\alpha _2, \\alpha _3)または(\\gamma _1, \\gamma _2)）は、推定期間中に企業ごとに推定される。\n\nDechow et al. (1995) は、異なる質問をテストするために設計された4つの異なるサンプルで分析を行う。  McNichols and Wilson (1988)のフレームワークを活用し、各サンプルの一部の企業年度に対して指標変数PARTを1に設定する。\n\n\n1000企業年度のランダムに選択されたサンプル \n\n極端な財務業績を経験している企業年度からランダムに選択された1000企業年度のサンプル \n\n1000企業年度のサンプルで、固定された既知のアクルーアル操作が導入される \n\nSECの執行措置に基づくサンプル\n\nここでは、Dechow et al. (1995) のある種の複製を行う。  最初の3つのサンプルを考慮するが、4番目のサンプルは省略する。 Data for our analysis come from two tables on Compustat: comp.funda and comp.company.2 本稿の分析のためのデータは、Compustatの2つのテーブル、comp.fundaとcomp.companyから取得される。2\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nfunda &lt;- tbl(db, Id(schema = \"comp\", table = \"funda\"))\ncompany &lt;- tbl(db, Id(schema = \"comp\", table = \"company\"))\n\n\n財務諸表データについては、第15章と同様にfunda_modを構築する。\n\nsics &lt;-\n  company |&gt;\n  select(gvkey, sic) |&gt;\n  mutate(sic = as.integer(sic))\n\nfunda_mod &lt;-\n  funda |&gt;\n  filter(indfmt == \"INDL\", datafmt == \"STD\",\n         consol == \"C\", popsrc == \"D\") |&gt;\n  left_join(sics, by = \"gvkey\") |&gt;\n  mutate(sic = coalesce(sich, sic))\n\n\nSloan (1996, p.293) は、「銀行、生命保険、損害保険会社」のデータを計算するためのデータが利用できないと述べているため、これらの企業（SICコードが6で始まる企業）を除外する。  Dechow et al. (1995) に従い、サンプルを1950年から1991年までの年に制限する。3  また、資産が欠損していない企業年度と、会計期間が12か月の企業年度に制限する(pddur == 12)。\n\nacc_data_raw &lt;-\n  funda_mod |&gt;\n  filter(!is.na(at), # ATの欠損値を除外\n         pddur == 12, # 会計期間が12か月の企業年度\n         !between(sic, 6000, 6999)) |&gt; # SICコードが6で始まる企業を除外\n  mutate( # che, dlc, sale, rectの欠損値を0で埋める\n    across(c(che, dlc, sale, rect), \\(x) coalesce(x, 0))\n  ) |&gt;\n  select(gvkey, datadate, fyear, at, ib, dp, rect,\n    ppegt, ni, sale, act, che, lct, dlc, sic) |&gt; # 変数を選択\n  filter(between(fyear, 1950, 1991)) |&gt; # 1950年から1991年までの年に制限\n  arrange(gvkey, fyear) |&gt; # gvkeyとfyearで並び替え\n  collect() # データを収集\n\n\nSloan (1996)やJones (1991)と同様に、Dechow et al. (1995) は、貸借対照表アプローチを使用してアクルーアルを測定する。  次の関数は、必要なCompustat変数を持つデータフレームを取り、各企業年度の総アクルーアルを計算し、結果のデータセットを返す。\n\ncalc_accruals &lt;- function(df) {\n  df |&gt;\n    group_by(gvkey) |&gt; # gvkeyでグループ化\n    arrange(datadate) |&gt; # datadateで並び替え\n    mutate(lag_at = lag(at),\n           d_ca = act - lag(act), # 流動資産変化\n           d_cash = che - lag(che), # 現金変化額\n           d_cl = lct - lag(lct), # 流動負債変化\n           d_std = dlc - lag(dlc), # 短期借入金変化\n           d_rev = sale - lag(sale), # 売上高変化\n           d_rec = rect - lag(rect)) |&gt; # る売掛金変化\n    ungroup() |&gt; # グループ化を解除\n    mutate(　# 総アクルーアルを計算\n      acc_raw =  (d_ca - d_cash - d_cl + d_std) - dp\n      )\n}\n\n\nJones (1991)と同様に、Dechow et al. (1995) は、企業レベルのデータを推定期間とテスト企業年度に分割し、企業ごとに利益マネジメントモデルを推定する。  Dechow et al. (1995) は、推定期間に少なくとも10年が必要であり、各サンプル企業は構築により1つのテスト企業年度を持つ。  これを実現するために、少なくとも11年間のデータを持つ企業からなる候補企業年度のサンプルを構築する。\n\ntest_sample &lt;-\n  acc_data_raw |&gt;\n  calc_accruals() |&gt; # 関数を適用\n  filter(# 条件を満たすデータを抽出\n    lag_at &gt; 0, sale &gt; 0, ppegt &gt; 0, !is.na(acc_raw),\n    !is.na(d_rev), !is.na(d_rec), !is.na(ppegt)) |&gt;\n  group_by(gvkey) |&gt; # gvkeyでグループ化\n  filter(n() &gt;= 11) |&gt; # 11観測値以上のデータを持つ企業年を抽出\n  ungroup() |&gt; # グループ化を解除\n  arrange(gvkey, fyear) |&gt; # gvkeyとfyearで並び替え\n  select(gvkey, fyear) # 変数を選択\n\n\nほとんどの分析は、1000社のランダムサンプルに焦点を当てる。  これらの1000社の各企業について、partをTRUEに設定する1つの会計年度を選択する。  DeAngeloモデルでは総アクルーアルの前期の値を使用するため、ランダム選択を最初の年以外の任意の年に制限する。\n\nset.seed(2022) # 乱数のシードを設定\n\nsample_1_firm_years &lt;-\n  test_sample |&gt;\n  mutate( # ランダムに1つの会計年度を選択\n    rand = rnorm( # 正規分布から乱数を生成\n      n = nrow(pick(everything())) # 行数分の乱数を生成\n      )) |&gt;\n  group_by(gvkey) |&gt;\n  filter(rand == min(rand),  # 最小の乱数を持つ行を抽出\n         fyear &gt; min(fyear) # 最初の年以外の行を抽出\n         ) |&gt;\n  ungroup() |&gt; # グループ化を解除\n  top_n(1000, wt = rand) |&gt; # 乱数の値が最小の1000行を抽出\n  select(gvkey, fyear) |&gt; # 変数を選択\n  mutate(part = TRUE) # partをTRUEに設定\n\n\nsample_1を作成するために、2つの結合を使用する。  最初の結合は、gvkeyによるsemi_join()であり、sample_1_firm_yearsで選択された企業年度を持つtest_sampleの企業年度を抽出する。  2番目の結合は、gvkeyとfyearによるleft_join()であり、sample_1_firm_yearsからのpart指標を該当する企業年度のデータに追加する。  ほとんどの企業年度はsample_1_firm_yearsには見つからないため、最終ステップでは、sample_1_firm_yearsに欠落している企業年度に対してpartをFALSEに設定するためにcoalesce()を使用する。\n\nsample_1 &lt;-\n  test_sample |&gt;\n  semi_join(sample_1_firm_years, by = \"gvkey\") |&gt; # セミ結合\n  left_join(sample_1_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt; # 左結合\n  mutate(part = coalesce(part, FALSE))\n\n\nサンプル企業年度のpartデータをCompustatデータのacc_data_rawと組み合わせてmerged_sample_1を作成する。4\n\nmerged_sample_1 &lt;-\n  sample_1 |&gt;\n  inner_join(acc_data_raw, by = c(\"gvkey\", \"fyear\")) # 内部結合\n\n\n観察された利益マネジメントの単純な研究を行う場合、利益マネジメントの尺度を計算してから分析に進むのが自然である。  しかし、ここでの分析では、Dechow et al. (1995) のように、自分で会計尺度を操作することになるため、利益マネジメントの尺度や総アクルーアルの尺度などの入力を再計算する必要がある。  このプロセスを容易にするために、以下の関数get_nda()に5つの利益マネジメント尺度の計算を埋め込む。5  summarize()の代わりにreframe()を使用していることに注意する。reframe()は、結果が各グループごとに1行であるとは限らないためである。\n\n# ジョーンズモデル\nfit_jones &lt;- function(df) {\n  fm &lt;- lm(acc_at ~ one_at + d_rev_at + ppe_at - 1,\n           data = df, model = FALSE, subset = !part)\n\n  df |&gt;\n    mutate(nda_jones = predict(fm, newdata = df),\n           da_jones = acc_at - nda_jones) |&gt;\n    select(fyear, nda_jones, da_jones)\n}\n\n# 修正ジョーンズモデル\nfit_mod_jones &lt;- function(df) {\n  fm &lt;- lm(acc_at ~ one_at + d_rev_alt_at + ppe_at - 1,\n           data = df, model = FALSE, subset = !part)\n  df |&gt;\n    mutate(nda_mod_jones = predict(fm, newdata = df),\n           da_mod_jones = acc_at - nda_mod_jones) |&gt;\n    select(fyear, nda_mod_jones, da_mod_jones)\n}\n\n# 非裁量的アクルーアルを計算\nget_nda &lt;- function(df) {\n\n  df_mod &lt;-\n    df |&gt;\n    calc_accruals() |&gt; # 総アクルーアルを計算\n    mutate(sic2 = str_sub(as.character(sic), 1, 2),\n           acc_at = acc_raw / lag_at,\n           one_at = 1 / lag_at,\n           d_rev_at = d_rev / lag_at,\n           d_rev_alt_at = (d_rev - d_rec) / lag_at,\n           ppe_at = ppegt / lag_at) |&gt;\n    group_by(sic2) |&gt; # sic2でグループ化\n    mutate(\n      acc_ind = median(if_else(part, NA, acc_at), na.rm = TRUE)) |&gt;\n    ungroup()\n  # ヒーリーのモデル\n  da_healy &lt;-\n    df_mod |&gt;\n    group_by(gvkey) |&gt;\n    arrange(fyear) |&gt;\n    mutate(nda_healy = mean(if_else(part, NA, acc_at), na.rm = TRUE),\n           da_healy = acc_at - nda_healy,\n           nda_deangelo = lag(acc_at),\n           da_deangelo = acc_at - nda_deangelo) |&gt;\n    ungroup() |&gt;\n    select(gvkey, fyear, part, nda_healy, da_healy, nda_deangelo,\n           da_deangelo)\n  # ジョーンズモデル\n  df_jones &lt;-\n    df_mod |&gt;\n    nest_by(gvkey) |&gt;\n    reframe(fit_jones(data))\n  # 修正ジョーンズモデル\n  df_mod_jones &lt;-\n    df_mod |&gt;\n    nest_by(gvkey) |&gt;\n    reframe(fit_mod_jones(data))\n\n  # 産業モデル\n  fit_industry &lt;- function(df) {\n    fm &lt;- lm(acc_at ~ acc_ind, data = df, model = FALSE, subset = !part)\n\n    df |&gt;\n      mutate(nda_industry = suppressWarnings(predict(fm, newdata = df)),\n             da_industry = acc_at - nda_industry) |&gt;\n      select(fyear, nda_industry, da_industry)\n  }\n  # 産業モデルのnda\n  df_industry &lt;-\n    df_mod |&gt;\n    nest_by(gvkey) |&gt;\n    reframe(fit_industry(data))\n\n  da_healy |&gt;\n    left_join(df_jones, by = c(\"gvkey\", \"fyear\")) |&gt;\n    left_join(df_mod_jones, by = c(\"gvkey\", \"fyear\")) |&gt;\n    left_join(df_industry, by = c(\"gvkey\", \"fyear\"))\n}\n\n\nget_nda()をメインサンプル（merged_sample_1）に適用してさらなる分析のためのreg_dataを作成するには、1行だけで済む。\n\nreg_data &lt;- get_nda(merged_sample_1)",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>利益マネジメント</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html",
    "href": "appA_LinearAlgebra.html",
    "title": "付録 A — 線形代数",
    "section": "",
    "text": "A.1 ベクトル\n行列は，線形代数として知られる数学の分野からきている。  線形代数のフルコースは，本書でカバーする範囲より多くの内容を提供している。  ここでは，本章で行列を用いる内容を理解するために必要はバックグラウンドを提供することのみを目指している。  行列とは何か，行列に対して行える操作，および有用な結果などの基本的な詳細に焦点を当てる。\n経済学の標準的な設定では， k 個の変数についてn個の観測値がある。  各観測値は，個人や企業，あるいは特定の時点での企業であるかもしれない。  本書の前の章では，観測値 i の変数が以下のように関連している可能性を考慮した。\ny_i = \\beta_0 + x_{1i} \\beta_1 + x_{2i} \\beta_2 + \\cdots + x_{ki} \\beta_k + \\varepsilon_i\nたとえば， y_i は特定の年の企業の収益性を表し，さまざまな x 変数は，その収益性に影響を与えると仮定される要因である（たとえば，資本ストックや市場集中度など）。  誤差項（\\varepsilon）は，変数 x_{1i} から x_{ki} が y_i の正確な値を決定しない場合に方程式が成立するようにする。  n 個の観測値があるため，実際には n 個の方程式がある。\n\\begin{aligned}\ny_1 &= \\beta_0 + x_{11} \\beta_1 + x_{21} \\beta_2 + \\cdots + x_{k1} \\beta_k + \\varepsilon_1 \\\\\ny_2 &= \\beta_0 + x_{12} \\beta_1 + x_{22} \\beta_2 + \\cdots + x_{k2} \\beta_k + \\varepsilon_2 \\\\\n& \\vdots \\\\\ny_n &= \\beta_0 + x_{1n} \\beta_1 + x_{2n} \\beta_2 + \\cdots + x_{kn} \\beta_k + \\varepsilon_n\n\\end{aligned}\n行列を使うと，この方程式系を簡潔に書くことができ，操作を簡潔に表現することができる。\n観測値には，売上高，利益，研究開発費，固定資産などのデータがあるかもしれない。これらのデータをベクトル(vector): y=(\\text{sales}, \\text{profit}, \\text{R\\&D}, \\text{fixed assets}) として配置することができる。  この y_i は n -tuple（ここでは n = 4）であり，これは要素の有限の順序付きリストである。より一般的な表現としては，y = (y_1, y_2, \\ldots, y_n) がある。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "chap10_FFJR.html",
    "href": "chap10_FFJR.html",
    "title": "\n3  FFJR\n",
    "section": "",
    "text": "3.1 効率的市場仮説\nFama et al. (1969)（以下、FFJR）の冒頭（簡潔なものではあるが）は、本論文の目的が市場の効率性（market efficiency）を明らかにすることであると確認している。  それまでの実証研究では、「連続する価格変化の独立性」が観察されることから市場の効率性を推測していた。  これに対し、FFJR は「特定の新しい情報に対する価格調整の速度」に着目している。 具体的には、「株式分割（stock split）に内在する情報（もし存在するならば）に対する市場の反応」を分析している。  本章では、まず効率的市場仮説(efficient markets hypothesis)の簡単な導入から始める。\nFFJRは最も初期のイベント・スタディ（event study）の一つを実施した研究である。  イベント・スタディは、効率的市場の概念、とりわけ Fama (1970) による「セミストロング型（semi-strong form）」効率性仮説と密接な関係があり、資本市場における会計情報の研究にも重要な示唆を与える。  したがって、本章ではFFJRを用いてイベント・スタディの基本概念を紹介し、第13章でより詳しく取り上げる。\nまた、本章では、CRSP（Center for Research in Security Prices）の重要な追加データセット（配当や株式分割に関連するデータなど）を紹介し、データやモデルを効率的に操作するためのR関数（例えば、tidyrパッケージのunnest()関数）も取り上げる。  FFJRをよりよく理解するために、株式分割と配当に関する背景情報も提供する。  続いて、FFJRの結果を再現（replication）し、読者向けの演習を用意する。 本章の最後には、本書のこの部分へのガイドを示す。\n資本市場研究の中心的なアイデアの1つは、効率的市場仮説(EMH)である。  Fama (1991)は、EMHを「証券価格がすべての利用可能な情報を完全に反映している」というシンプルな記述で定義している。  EMHは、おそらく社会科学全体で最も実証的に検証された命題である。\nFamaの定式化では、「完全に反映している」と「すべての利用可能な情報」の用語が多くの役割を果たしている。  ある情報が証券価格に完全に反映されているという概念の広く理解されている帰結の1つは、その情報に基づいて取引することでリスク調整済み利益を生み出す機会がないということである。1\nEMHは、少なくとも2つの理由で会計研究と実践にとって特に重要である。  第1に、会計情報は、EMHが検証される「すべての利用可能な情報」の一部であることが多い。  具体的には、Beaver (1998, p.136)が、会計利益は「投資コミュニティによって広く分析されている」と指摘している。  他の企業固有の変数は、アナリストや他の資本市場参加者によって利益よりも多くの注意を受けていない。  第2に、EMHが成立するかどうかは、会計情報の作成者、利用者、および規制当局にとって重要な意味を持つ。\nRichard Thalerは、EMHの2つの概念を特定し、「価格は正しい」(price is right)と「無料のランチはない」(no free lunch)という原則とラベルを付けている。  「価格は正しい」原則は、資産価格が利用可能な情報を「完全に反映」し、したがって「資源配分のための正確なシグナルを提供する」と述べている。  「無料のランチはない」原則は、市場価格を予測することは不可能であり、リスクを考慮した後に市場を上回る投資家を見つけることは非常に困難であると述べている。  「無料のランチはない」原則は、市場の先見の明を要求されることが少なく、実証的に検証可能な変種である。\nEMHのほとんどの実証的研究は、「無料のランチはない」原則を検証している。  しかし、Shiller (1984)は、「現実のリターンがほとんど予測不可能であるため、現実の株価は内在価値に近い」という主張が存在することを嘆き、「この効率的市場仮説の議論は、経済思想史上で最も注目すべき誤りの1つを表している」と述べている。  言い換えれば、一般的な誤謬は、2つの変種を混同し、その結果、「無料のランチはない」変種の証拠が「価格は正しい」理論を支持するとされる。\n「価格は正しい」理論は、企業の政策や規制のメリットを評価するためにイベント・スタディを使用する論文の基礎となることがよくある。  これら2つの原則を混同することに関するいくつかの問題については、第13章で議論する。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFJR</span>"
    ]
  },
  {
    "objectID": "chap10_FFJR.html#株式分割",
    "href": "chap10_FFJR.html#株式分割",
    "title": "\n3  FFJR\n",
    "section": "\n3.2 株式分割",
    "text": "3.2 株式分割\n\nFFJRとその設定については、明示的でなく暗黙的であり、50年以上後にこの分野に入る読者には明確でないかもしれないため、まず論文のいくつかの要素を明らかにする。\n\nWalker (2021, p.1)は，株式分割を「株主がすでに所有している株式の数に比例して、追加の株式を無償で発行すること」と説明している。  たとえば、2対1の株式分割は、企業が既存の株式に対して新しい株式を1株発行することで、発行済み株式の総数を倍増させる。\n\nファンダメンタルの観点からは、株式分割には経済的な影響がないように見えるかもしれない。  たとえば、1,000株を保有しているとし、その会社の内在価値が5億ドルで、発行済み株式が1,000万株であるとする。  私たちの株式は50,000ドルの価値があるだろう（1000 \\times 500 \\div 10）。  その後、会社が2対1の株式分割を行い、発行済み株式が2,000万株になったとすると、私たちは2,000株を保有し、50,000ドルの価値があるだろう（2000 \\times 500 \\div 20）。\n\nこのように、企業が株式分割を行う理由を尋ねるのは自然なことである。  ある説明は、株式分割が企業の内在価値に影響を与えないにもかかわらず、市場が効率的でないため、株式が株式数の変更に適切に調整されず、仮想的な株式保有が分割後に50,000ドル以上の価値になる可能性があると主張している。  もし経営者が企業の株価を高くしたいと考えているなら、株式分割を行う理由になるだろう。2\n\n\nしかし、市場効率性を仮定すると（株式分割の株価への影響を計算できることは、確かに効率性の控えめなレベルである）、株式分割の代替的な説明を見つける必要がある。  Walker (2021, p.2)は、「2つの主要な説明」を特定している。\n\n第1の説明は，情報シグナリング仮説であり，「経営陣が企業の前向きな見通しに関する非公開情報をシグナルするための手段として分割を利用する」と主張している（Walker, 2021, p.2）。  しかし，これは，非公開情報を持つ企業にとって有益であるかコストがかかるような分割の基礎となる効果がある場合にのみ説明となる。  たとえば、シグナリング均衡は、株式分割が見込みが良い企業にとってコストが低い場合に維持されるかもしれない。3  しかし、そのようなコストの違いを考慮する必要がある。\n\n\n第2の説明は、流動性仮説であり、「経営陣が株式分割を利用して株式の流動性を向上させる手段として利用する」と主張している（Walker, 2021, p.2）。  株式分割の流動性効果の詳細な説明に入ることは、この章の目的から遠ざかりすぎるため、ここでは目的に適したよりスタイライズされた説明を提供する。  この説明を、企業の流動性が特定の範囲で最大化されるときに、株価がその範囲にトレードされると、企業は株式を分割する傾向があるというゴルディロックスの株価理論としてラベル付けすることができる。  たとえば，企業の流動性が株価が40ドル前後のときに最大化されるが，実際には80ドル前後で取引されている場合，2対1の株式分割は流動性を向上させるだろう。\n\n流動性仮説は、情報シグナリング仮説で特定したギャップを埋めるかもしれない。  将来100ドルで取引されることを期待している企業（後で明らかになる内部情報による）は、2対1の株式分割を行いたいと考えるかもしれない。なぜなら、40ドルから50ドルに上昇する株式は流動性が向上するからである。  しかし、内部情報を持ち、将来40ドルに近い価格で取引されることを期待している企業（分割を行わない場合）は、2対1の株式分割を行うことに消極的であるだろう。なぜなら、その後は将来20ドルに近い価格で取引されることを期待しているからである。\n\nイベント研究の重要な要素の1つは、市場がイベント情報に反応すると予想される期間の特定である。  配当と同様に、株式分割は通常、有効日の前に発表される。  Fama et al. (1969, p.7)は、「分割の有効日が発生する月を月 0 と定義する」と述べている。  Fama et al. (1969, p. 9)は、サンプルからの「52の分割の無作為なサンプル」について、「分割の発表日と有効日の中央値は44.5日であった」と述べている。  株式分割に情報が含まれている場合、効率的な市場では、分割が発表されたときに反応することが期待される。  FFJRの分析は月次データを使用しているため、分割の中央値の発表日が有効日の44.5日前であることから、中央値の分割は月 -2 に発表されることを意味する。  FFJRは、分割の発表日の分布についての詳細を提供していないが、一部の分割は月 -1 に発表されるかもしれず、他の分割は月 -3 またはそれ以前に発生するかもしれない。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFJR</span>"
    ]
  },
  {
    "objectID": "chap10_FFJR.html#ディスカッション課題",
    "href": "chap10_FFJR.html#ディスカッション課題",
    "title": "\n3  FFJR\n",
    "section": "\n3.3 ディスカッション課題",
    "text": "3.3 ディスカッション課題\n\n以下の代替理論を考えてみよう。\n\n\n\n理論A: 企業は株価を一定の範囲内に保ちたいと考えている。  株価がある閾値を超えると、企業は分割を開始し、有効日の数週間前に発表するかもしれない。  企業は、分割を行うことで企業の見通しに関する非公開情報をシグナルするために使用しない。 \n\n\n理論B: 企業は、企業の見通しに関する非公開情報をシグナルするために分割を使用する。  企業は、有効日の数週間前に分割を発表する。\n\n\n\n理論C: 資本市場参加者は、分割の効果に完全に適応せず、ある程度前の株価にアンカーを置く傾向がある。  企業は、分割を行うことで企業の見通しに関する非公開情報をシグナルするために使用しない。\n\n分割が月 -2 に発表されたと仮定して、累積異常収益の予測される挙動の一連の指標プロット（例：手描き）を作成する。  発表日の変動が分割の有効日に対する影響はどうなるか？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFJR</span>"
    ]
  },
  {
    "objectID": "chap10_FFJR.html#配当政策",
    "href": "chap10_FFJR.html#配当政策",
    "title": "\n3  FFJR\n",
    "section": "\n3.4 配当政策",
    "text": "3.4 配当政策\n\nBlack (1976, p.9)は，ミラー・モディリアーニの理論によると企業が支払う配当は株価あるいは株式リターンに影響を与えない，と指摘している。  もちろん，ミラー・モディリアーニの理論の基礎となる仮定は，税効果と取引コストが存在しない，というものがる。  現実ではこれらの仮定は違反している一方で，この違反は一般的に配当の存在に対する説明を提供しない（たとえば，一般的に配当は税目的で優遇されない）。\n\nBlack (1976, p.10)は，配当に関する一般的な理解をまとめている。「何らかの理由で，経営者や取締役は配当を削減したくない。–&gt;  そのため，会社の見通しが一定期間にわたって高い配当を支持するのに十分であると感じる場合にのみ，配当を引き上げる。–&gt;  そして，見通しがすぐに回復する可能性が低いと考える場合にのみ，配当を削減する。」–&gt;  この理論は「行動的」であるが（これについてはThaler, 2015, p.166を参照）、経営者や取締役は内部情報を持っているため、この理論に従って行動すると、配当政策はその情報の一部を伝えることになる。\n\nFama et al. (1969, pp.2–3)は，配当に関するこの説明を支持している。「研究によると，一度配当が増加すると，大企業は最も極端な状況を除いて，それらを削減することに非常に消極的であることが示されている。–&gt;  取締役は，将来の利益が新しい高い率で配当を維持するのに十分であると確信しているときにのみ，配当を増やすことで，そのような配当削減に対するヘッジを行っているように見える。  したがって，配当の変更は，経営陣が企業の長期的な収益と配当支払いの潜在能力をどのように評価しているかに関する市場への重要な情報を伝えるものと見なすことができる。\n\n株式分割と配当の関係は，FFJRでは明確に説明されていない。  Fama et al. (1969, p. 2)は，「過去には，株式分割の大部分が直後に配当の増加に続いている」と述べている。  Fama et al. (1969, pp. 12–16)は，次の説明を提供している。「分割が発表されるか予想されると，市場はこれを（正しく）配当がすぐに大幅に増加する可能性が大幅に高まると解釈する。  （実際には，多くの場合，分割と配当の増加は同時に発表されることがある。）」\n\nこの説明にはいくつかの仮定が含まれており，何らかのシグナリングの物語に基づいているように見える。4  たとえば，なぜ企業は株式分割を使用して将来の配当増加をシグナルするのか？  また、分割と配当の増加を同時に発表するとき、企業は何をシグナルしているのか？\n\n\n3.4.1 ディスカッション課題\n\n\nFFJRのリサーチ・デザインでは、コントロールグループを利用しているか？もしそうなら、どのように利用しているか？  コントロールグループを導入するために使用できる代替手法は何か？\n\n\nFama et al. (1969, p. 9)は「この研究の最も重要な実証結果は、表2と表3、図2と図3にまとめられている」と述べている。  FFJRの表3は何を示しているか？（ヒント：p.11を見なさい）表3の提示は効果的だと思うか？\n\n\nFFJRの表2を考えてみよう。  それは表3よりも重要ですか？  表2と図2、図3の関係は何ですか？\n\n\n論文の仮説を検証するために使用される統計的検定は何か？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFJR</span>"
    ]
  },
  {
    "objectID": "chap10_FFJR.html#ffjrの再現",
    "href": "chap10_FFJR.html#ffjrの再現",
    "title": "\n3  FFJR\n",
    "section": "\n3.5 FFJRの再現",
    "text": "3.5 FFJRの再現\n\nここでは、FFJRのおおまかな再現を行う。[^5]\n\n使用するすべてのデータは、crspスキーマから取得する。  ここで新しいテーブルはdsedistであり、配当や株式分割を含む分配に関する情報が含まれている。\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nmsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"msf\"))\nmsi &lt;- tbl(db, Id(schema = \"crsp\", table = \"msi\"))\nstocknames &lt;- tbl(db, Id(schema = \"crsp\", table = \"stocknames\"))\ndsedist &lt;- tbl(db, Id(schema = \"crsp\", table = \"dsedist\"))\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nmsf &lt;- load_parquet(db, schema = \"crsp\", table = \"msf\")\nmsi &lt;- load_parquet(db, schema = \"crsp\", table = \"msi\")\nstocknames &lt;- load_parquet(db, schema = \"crsp\", table = \"stocknames\")\ndsedist &lt;- load_parquet(db, schema = \"crsp\", table = \"dsedist\")\n\n\n\n\n\nFama et al. (1969, p.3)は、「株式分割」を、少なくとも4株が以前に発行された株式に対して5株が配布される取引と定義している。  したがって、この分割の定義には、25％以上の株式配当が含まれる」。\n\nsplits &lt;-\n  dsedist |&gt;\n  filter(between(exdt, \"1927-01-01\", \"1959-12-31\"),\n         distcd %in% c(5523L, 5533L),\n         facshr &gt;= 0.25) |&gt;\n  group_by(permno, exdt) |&gt;\n  summarize(facshr = sum(facshr, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(split = TRUE)\n\n\nFama et al. (1969, p.3)は続けて、「このデータはニューヨーク証券取引所に上場されている普通株式のみをカバーしているため、テストに含めるためのルールとして、分割証券は分割の前後12か月間、少なくとも12か月間取引所に上場されている必要がある」と述べている。  1927年1月から1959年12月まで、ニューヨーク証券取引所でこれらの基準を満たす940回の分割が発生した。」  NYSEに上場している株式はexchcd == 1となり、普通株式はshrcdの最初の文字が1となっているものとする。  さまざまな日付のexchcdとshrcdのデータはcrsp.stocknamesにある。  以下のコードは、これらの基準を満たす証券のpermno値と日付範囲を持つテーブルを作成する。\n\nnyse_stocks &lt;-\n  stocknames |&gt;\n  filter(exchcd == 1,\n         str_sub(as.character(shrcd), 1L, 1L) == \"1\") |&gt;\n  select(permno, namedt, nameenddt)\n\n\nこのテーブルを使用して、これらの証券に焦点を当てて分割を行うことができる。\n\nnyse_splits_raw &lt;-\n  splits |&gt;\n  inner_join(nyse_stocks,\n             join_by(permno, between(exdt, namedt, nameenddt)))\n\n\nこれらのデータをcrsp.msfからのリターンデータと組み合わせる必要がある。  テーブルcrsp.msfには、permnoとdateでインデックス付けされた月次データが含まれており、dateは月の最終取引日である。  nyse_splits_rawの分割は一般的にcrsp.msfで見つかる日付には発生しないため、crsp.msfとnyse_splits_rawの観測値を整列させるために「月」変数を作成する必要がある。\n\nこのために、月の「インデックス」を含むmonth_indexesというテーブルを作成する。これは、月の前後に移動するための算術を行う際にも使用される。  この計算を行う際にwindow関数を使用する。 これについては、dplyrのドキュメントやR for Data Scienceの「データベース」章で説明されている。5  ここでの月の「インデックス」とは、crsp.msfとcrsp.msiにある月のシーケンスの配置を指す。6\n\nmonth_indexes &lt;-\n  msi |&gt;\n  mutate(month = as.Date(floor_date(date, 'month'))) |&gt;\n  window_order(month) |&gt;\n  mutate(month_index = row_number()) |&gt;\n  select(date, month, month_index) |&gt;\n  collect()\n\n\n実際には、市場価格純資産倍率を計算するためのデータを補完するためにfill()を使用したときに、window()関数を使用した。  この場合、dbplyrパッケージとPostgreSQL（window_order()を使用）を介して提供されるwindow関数の機能は、基本的なdplyrパッケージとtibbleデータフレーム（arrange()を使用）を介して提供されるものよりも強力であることがわかった。dbplyrバージョンでは、適用されるwindow_frame()を指定することができるため、以下で見るように、これを利用する。\n\n次に、分割データ（nyse_splits_raw）をRに取り込み、month_indexesとマージする。\n\nnyse_splits &lt;-\n  nyse_splits_raw |&gt;\n  mutate(month = as.Date(floor_date(exdt, 'month'))) |&gt;\n  collect() |&gt;\n  inner_join(month_indexes, by = \"month\") |&gt;\n  rename(ex_month_index = month_index) |&gt;\n  select(-namedt, -nameenddt, -date)\n\n\nNYSE株と欠損のないリターンを持つ月に制限されたcrsp.msfであるnyse_msfを構築する。さらに、month_indexという変数を追加する。\n\nnyse_msf &lt;-\n  msf |&gt;\n  filter(!is.na(ret)) |&gt;\n  inner_join(nyse_stocks,\n             join_by(permno, between(date, namedt, nameenddt))) |&gt;\n  collect() |&gt;\n  inner_join(month_indexes, by = \"date\") |&gt;\n  select(permno, month_index, date, ret)\n\n\n以下のコードは、nyse_splitsからの分割データとnyse_msfからのリターンデータをマージする。  分割とリターンの間の月数を測定する変数（month_rel_ex）を作成する。\n\nnyse_splitsの特定のpermnoに複数の分割（それぞれ異なるexdt）がある可能性があるため、nyse_splitsの行とnyse_msfの行との間には多対多の関係がある。  このような場合にdplyrが発行する警告を抑制するために、left_join()の引数としてrelationship = \"many-to-many\"を指定して、このようなマッチを予想していることを示す。\n\nsplit_return_data &lt;-\n  nyse_splits |&gt;\n  left_join(nyse_msf, by = \"permno\", relationship = \"many-to-many\") |&gt;\n  mutate(month_rel_ex = month_index - ex_month_index) |&gt;\n  select(permno, exdt, month_rel_ex, date, ret)\n\n\nただし、各（permno、exdt、month_rel_ex）の組み合わせに1行だけあることを確認したい。\n\nsplit_return_data |&gt;\n  count(permno, exdt, month_rel_ex) |&gt;\n  count(n_rows = n)\n\n\nFama et al. (1969, p.3)は、「分析に使用されるパラメータの信頼性の高い推定値を得るためには、分割日の周囲に少なくとも24か月連続して価格配当データが必要である」と任意に決定した。  実際にFama et al. (1969)は分割の前後12か月を見ているため、分割の月を含めると25か月となる。  split_sampleテーブルはこの制限を課し、split_return_dataとのsemi_join()を行うことで、サンプルの分割データを含むsplit_returnsを作成する。\n\nsplit_sample &lt;-\n  split_return_data |&gt;\n  filter(between(month_rel_ex, -12, 12)) |&gt;\n  group_by(permno, exdt) |&gt;\n  summarize(n_obs = n(), .groups = \"drop\") |&gt;\n  filter(n_obs == 25L) |&gt;\n  select(permno, exdt)\n\nsplit_returns &lt;-\n  split_return_data |&gt;\n  semi_join(split_sample, by = \"permno\")\n\n\nFama et al. (1969, p.4)のサンプルは、622の証券に対する940の分割から構成されている。  以下のコードの出力から、私たちのサンプルがわずかに大きいことが示唆されているが、これは1966年以降の基礎データの変更によるものかもしれない。\n\nsplit_sample |&gt; distinct(permno) |&gt; count() |&gt; pull()\n\nsplit_sample |&gt; count() |&gt; pull()\n\n\n我々は、FFJRの超過リターンを推定する基本的なアプローチに従う。  FFJRは、各株式の総リターンの対数を市場指数の対数の総リターンに回帰させ、残差を取ることで市場モデルを推定している。\n\nFama et al. (1969, p.4)で使用されている市場指数は、Fisher (1966)の表A1として提供されている，Fisherの組み合わせ投資パフォーマンス指数(combination investment performance index)である。  この表を使用するためには、機械可読形式で利用できないため、そこに含まれる数百の数値を入力する必要がある。  その代わりに、crsp.msiで提供されているCRSPの標準指数の1つを使用する。  crsp.msiには、2つの広く使用されている指数がある：vwretdは配当を含む時価総額加重リターンを提供し、ewretdは配当を含む等加重リターンを提供する。  時価総額加重指数は、ポートフォリオ形成時の各証券の時価総額に基づいてポートフォリオ内の各証券に重みを付ける一方、等加重指数は各証券に等しい重みを付ける。  基本分析では、vwretdに焦点を当てる。\n\nindex_returns &lt;-\n  msi |&gt;\n  select(date, vwretd, ewretd) |&gt;\n  collect()\n\n\nFama et al. (1969, pp.4-5)は，「特定化誤差」(specification error)を引き起こす非ゼロの超過リターンについて懸念を示し，「すべての証券について分割前の15か月と，配当減少に続く分割後の15か月を除外する」と述べている。  すべての証券について、分割前の15カ月は除外するが、分割後の月は除外しないことで、これを部分的に実施している、 というのも、分割時には入手できなかったであろう配当に関するデータが必要となり、その結果、さらなる先行問題(look-ahead issue)が生じるからである。\n\n(permno, date)の組み合わせは複数の分割に対して現れる可能性があるため、ある分割の15か月前にあるが、別の分割にはそうではない場合があるため、このような観測を横断してデータを集約する必要があることに注意する。  以下のコードは、その株式に対して15か月以内のいずれかの分割がある場合、(permno, date)の組み合わせが真である場合、excludeをtrueに設定する。\n\nomit_returns &lt;-\n  split_returns |&gt;\n  mutate(\n    # month_rel_exが-15から0の間の場合はtrue\n    exclude = between(month_rel_ex, -15, 0)\n    ) |&gt;\n  group_by(permno, date) |&gt;\n  summarize(\n    exclude = any(exclude), # excludeにTrueが1つでもあればTrue\n    .groups = \"drop\" # グループ化を解除\n     )\n\n\n以下のコードは、split_returnsからのデータにexcludeを追加する。  split_returnsからのリターンデータについては、(permno, date)の組み合わせが複数回現れる可能性があるため、month_rel_exの値が異なることがあるため、一意の値を選択する必要があることに注意する。  また、excludeがTRUEのデータセットから行を単純に削除しないことに注意する。これらの月の異常リターンを計算したいからである。  代わりに、subset = !excludeを指定することで、これらの観測を回帰分析から除外するだけである。\n\nsplit_returns_reg &lt;-\n  split_returns |&gt;\n  inner_join(omit_returns, by = c(\"permno\", \"date\")) |&gt;\n  select(permno, date, ret, exclude) |&gt;\n  distinct()\n\n\n次のステップは、permnoごとに回帰を推定することである。  FFJRと同様に、各株式の総リターンの対数を市場指数の総リターンの対数に回帰させることで、市場モデルを推定する。  以下のコードでは、R for Data Scienceの第1版の「多数のモデル」章のアイデアを広く活用しており、その章を読むと役立つかもしれません。7\n\n各permnoの値について1つのモデルを適合させたいため、permnoの値以外のすべてのデータを1つの列にまとめるためにnest()を使用する。  その後、purrrライブラリのmap()を使用して、各permno値に対して適合した線形（OLS）モデルを含む新しい列fitを作成することができる。  モデルにpredict()を適用すると、モデルを推定するために使用された観測値のみに対して適合値が得られるため、分割の15か月前の月に対して予測値が得られないことになる。  代わりに、predict関数の第2引数（newdata）としてすべての観測データを提供する必要がある。  これを行うために、purrrライブラリのmap2()を使用し、結果を変数predictedに格納する。\n\npredictedはリスト列であり、各要素は該当するpermnoに対して市場モデルを使用して予測された値のベクトルである。  tidyrパッケージのunnest()を使用して、各行が特定の（permno、date）のペアに関連するようにデータフレームを「展開」することができる。  最後に、予測値をlprの実際の値から引いて超過リターンを計算し、その結果をresidという列に格納する。\n\nabnormal_returns &lt;-\n  split_returns_reg |&gt;\n  left_join(index_returns, by = \"date\") |&gt;\n  mutate(lpr = log(1 + ret),\n         lm = log(1 + vwretd)) |&gt;\n  select(permno, date, lpr, lm, exclude) |&gt;\n  nest(data = !permno) |&gt;\n  mutate(fit = map(data, \\(x) lm(lpr ~ lm, data = x, subset = !exclude,\n                              na.action = \"na.exclude\"))) |&gt;\n  mutate(predicted = map2(fit, data, \\(x, y) predict(x, newdata = y))) |&gt;\n  unnest(cols = c(predicted, data)) |&gt;\n  mutate(resid = lpr - predicted) |&gt;\n  select(permno, date, resid)\n\n\nabnormal_returnsをnyse_splitsと結合するために、nyse_splitsのバージョンを作成し、startとend列を作成する。これらは、超過リターンを取得したい最初の月と最後の月のmonth_indexを示している（つまり、分割の30か月以内）。\n\nnyse_splits_join &lt;-\n  nyse_splits |&gt;\n  mutate(start = ex_month_index - 30,\n         end = ex_month_index + 30)\n\n\nabnormal_returnsをnyse_splitsと組み合わせることができるが、その前にmonth_indexesから各日付のmonth_indexを取得する必要がある。  abnormal_returnsをnyse_splitsと結合する際には、permnoとmonth_indexをstartとendの間で使用する。\n\ntable2_data &lt;-\n  abnormal_returns |&gt;\n  inner_join(month_indexes, by = \"date\") |&gt;\n  inner_join(nyse_splits_join,\n             join_by(permno, between(month_index, start, end))) |&gt;\n  mutate(month_gap = month_index - ex_month_index) |&gt;\n  select(permno, exdt, month_gap, resid)\n\n\n処理の前に，各分割（permnoとexdt）と相対月（month_gap）について1行だけあることを確認する。8\n\ntable2_data |&gt;\n  count(permno, exdt, month_gap) |&gt;\n  count(n_rows = n)\n\n\nこれで、Fama et al. (1969, p.13)の図2bのアナログである図10.1を作成するために必要なデータが揃った。\n\ntable2_data |&gt;\n  group_by(month_gap) |&gt;\n  summarize(all_u = mean(resid), .groups = \"drop\") |&gt;\n  arrange(month_gap) |&gt;\n  mutate(all_U = cumsum(all_u)) |&gt;\n  ggplot(aes(x = month_gap, y = all_U)) +\n  geom_point()\n\n\n\n3.5.1 配当データ\n\nFFJRの表2のほとんどの列とFFJRの図3のすべてのプロットには、配当データが必要である。  配当データはdsedistから収集する。9  普通配当は、distcdの1桁目が普通配当の1であることで、他の分配（清算配当や交換・再編成など）と区別される。\n\n特定の証券について特定の月に複数の配当が支払われることがあるため、（permno、month）で配当を集計する。\n\ndiv_months &lt;-\n  dsedist |&gt;\n  filter(str_sub(as.character(distcd), 1L, 1L) == \"1\") |&gt;\n  mutate(month = as.Date(floor_date(exdt, 'month'))) |&gt;\n  group_by(permno, month) |&gt;\n  summarize(divamt = sum(divamt, na.rm = TRUE), .groups = \"drop\")\n\n\n各月に対して，FFJRは「分割後12ヶ月に支払われた(分割前の株式1株当たり)配当合計を分割前12ヶ月に支払われた配当合計で割ったもの」を配当変化率と定義している。(Fama et al., 1969, p.8)。\n\nしたがって、各月tについて、月t-11から月tまでの配当と、月t+1から月t+12までの配当を合計したい。\n\n「分割前に四半期ごとに1株当たり80セントを支払っていた企業が、分割後に1株当たり45セント（つまり、元の株式1株当たり90セント）を支払った場合、配当を増やしたと見なすべきである」ということを考慮して、「分割前の株式1株当たり」で配当を計算するために注意が必要である。  msfのcfacshr変数を使用することで、必要な調整を行うことができる。10\n\n所与とした月に株式に配当が支払われない場合、divamtをゼロに設定する。これにより、株式がcrsp.msfにおいてNYSE株ではない月（この表にはない）と、NYSE株であるがその月に配当を支払わない月（つまり、divamtがゼロ）を区別することができる。11\n\nnyse_divs_raw &lt;-\n  msf |&gt;\n  inner_join(nyse_stocks, by = \"permno\") |&gt;\n  filter(between(date, namedt, nameenddt)) |&gt;\n  mutate(month = as.Date(floor_date(date, 'month'))) |&gt;\n  select(permno, date, month, cfacshr) |&gt;\n  left_join(div_months, by = c(\"permno\", \"month\")) |&gt;\n  mutate(divamt = coalesce(divamt / cfacshr, 0)) |&gt;\n  select(permno, month, divamt)\n\n\n次のコードは、上記で言及した配当変化率を計算するために必要な変数を集計する。すなわち、分割後12ヶ月に支払われた配当と分割前12ヶ月に支払われた配当である。\n\nこの場合，PERMNOごとにウィンドウを適用したいため，group_by(permno)を指定している。そして，各ウィンドウ内のデータを月で並べ替えるためにwindow_order(month)を指定している。次のステップでは、window_frame(from = -11, to = 0)を指定して、以降の計算でウィンドウに t-11 から t までの値を含めることを示している。  これにより、次のmutate()ステップで計算されるdiv_trailingとmths_trailingが考慮される。  na.rm = TRUEは常にSQL（このコードが最終的に変換されるもの）の場合であり、n()はウィンドウに含まれる行数を数える。\n\n次に、div_forwardとmths_forwardを計算するために、ウィンドウをwindow_frame(from = 1, to = 12)に再指定する。\n\n最後に、例えば、株式が過去12か月以内に上場したり、次の12か月で上場廃止されたりして、配当変化率の計算があまり意味をなさない場合を除外するために、filter(mths_trailing == 12, mths_forward == 12)を使用する。  不要になったグループ化group_by(permno)を除去するためにungroup()を使い，興味のある変数を選択し、データを収集している。\n\nnyse_divs &lt;-\n  nyse_divs_raw |&gt;\n  group_by(permno) |&gt;\n  window_order(month) |&gt;\n  window_frame(from = -11, to = 0) |&gt;\n  mutate(div_trailing = sum(divamt, na.rm = TRUE),\n         mths_trailing = n()) |&gt;\n  window_frame(from = 1, to = 12) |&gt;\n  mutate(div_forward = sum(divamt, na.rm = TRUE),\n         mths_forward = n()) |&gt;\n  filter(mths_trailing == 12, mths_forward == 12) |&gt;\n  ungroup() |&gt;\n  select(permno, month, div_trailing, div_forward) |&gt;\n  collect()\n\n\nNYSEのすべての（permno、month）の組み合わせについてdiv_trailingとdiv_forwardのデータがあるので、これらのデータを使用して、nyse_splitsの各分割月の配当変化率（div_ratio）を計算することができる。\n\nsplit_firm_dividends &lt;-\n  nyse_splits |&gt;\n  left_join(nyse_divs, by = c(\"permno\", \"month\")) |&gt;\n  mutate(div_ratio = if_else(div_trailing &gt; 0,\n                             div_forward / div_trailing, NA)) |&gt;\n  filter(!is.na(div_ratio)) |&gt;\n  select(permno, month, exdt, div_ratio)\n\n\nFama et al. (1969, p.8)は、「関連する期間中にNYSEで全証券が支払った配当平均に対する配当変化を測定する。…配当の「増加」は、分割株の配当変化率が取引所全体の比率よりも大きい場合と定義される。」  各月の全株式にわたるdiv_trailingとdiv_forwardの値を平均することで、「市場」の配当変化率（mkt_div_ratio）を計算する。\n\ndiv_mkt &lt;-\n  nyse_divs |&gt;\n  group_by(month) |&gt;\n  summarize(div_trailing = mean(div_trailing, na.rm = TRUE),\n            div_forward = mean(div_forward, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  mutate(mkt_div_ratio = if_else(div_trailing &gt; 0,\n                                 div_forward / div_trailing, NA))\n\n\n本稿では，分割が行われる各月について，分割レベルと市場レベルの配当変化率のデータを組み合わせて，分割を受ける株式の配当変化率がその月の市場の配当変化率よりも大きいかどうかを示す指標であるup_divを計算する。\n\nFama et al. (1969, p.9)は、配当増加の測定が完璧であるとは主張しておらず、「個々の証券の年間配当変化を分類するための簡単で便利な方法」として使用している。\n\ndividends_file &lt;-\n  split_firm_dividends |&gt;\n  inner_join(div_mkt, by = \"month\") |&gt;\n  select(permno, exdt, div_ratio, mkt_div_ratio) |&gt;\n  mutate(up_div = div_ratio &gt;= mkt_div_ratio)\n\n\n最後に，相対的な配当変化率のデータを超過リターンデータと組み合わせることができる。\n\ntable2_w_divs &lt;-\n  table2_data |&gt;\n  left_join(dividends_file, by = c(\"permno\", \"exdt\"))\n\n\nこれで、Fama et al. (1969, p. 15)の図3cのアナログである図10.2を作成することができる。\n\ntable2_w_divs |&gt;\n  filter(up_div) |&gt;\n  group_by(month_gap) |&gt;\n  summarize(u = mean(resid), .groups = \"drop\") |&gt;\n  arrange(month_gap) |&gt;\n  mutate(U = cumsum(u)) |&gt;\n  ggplot(aes(x = month_gap, y = U)) +\n  geom_point()\n\n\n図10.3は、Fama et al. (1969, p. 15)の図3dに相当するものである。\n\ntable2_w_divs |&gt;\n  filter(!up_div) |&gt;\n  group_by(month_gap) |&gt;\n  summarize(u = mean(resid), .groups = \"drop\") |&gt;\n  arrange(month_gap) |&gt;\n  mutate(U = cumsum(u)) |&gt;\n  ggplot(aes(x = month_gap, y = U)) +\n  geom_point()\n\n\n\n3.5.2 議論の問題と演習\n\n \n\n  \n\n\n\n\n\n\n\n\n「過去には、多くの株式分割が直後に配当増加に続いてきた。そして、他の証券と比較して、より大きな増加が経験されてきた。」 この主張を支持する証拠（あれば）は、FFJRによって提供されているか？ 上記の再現データの背後にあるデータには、この主張と一致する証拠が見られるか？\n以下の3つの代替理論を考えなさい。 観察された現象を最もよく説明するのは、これらの理論のうちどれかを区別するためのテストを提案しなさい。 FFJRが提案する理論と最も一致するのはどの理論か？ この理論をFFJRが提案する理論とよりよく一致させるためには、どのように修正する必要があるか？\n\n\n理論 A' : 企業は株価を一定の範囲内に保ちたいと考えている。 株価がある閾値を超えると、企業は分割を開始し、有効日の数週間前に発表するかもしれない。 企業は、企業の見通しに関する非公開情報を示すために分割を使用しない。 分割前の株価は、近々の配当増加を示唆する情報によって駆動されるかもしれない。\n理論 B' : 企業は、企業の見通しに関する非公開情報を示すために分割を使用する。 企業は、有効日の数週間前に分割を発表する。 企業は、企業の見通しに関する非公開情報を示すために配当変更を使用するかもしれない。\n理論 B'' : 企業は、将来の配当増加に関する非公開情報を示すために分割を使用し、これにより企業の見通しに関する非公開情報を示す。 企業は、有効日の数週間前に分割を発表する。\n\n\nFama et al. (1969)の17ページでは、「関連する配当の情報効果が適切に考慮されると、分割そのものは普通株式のリターンに対して純効果を持たない」と主張されている。  この文中の「per se」という言葉の意味が明確ですか？  FFJRは、この主張を支持するための説得力のある証拠を提供していますか？  今日利用可能なより豊富なデータを使用して、この主張をどのようにテストするかを説明してください。  FFJRで使用されたデータ以外のデータは何ですか？\n\n\n上記で生成された図は、FFJRのそれと比較してどのように異なるか？ どのような違いがあるか？\n\n\n上記の分析では、市場指数としてvwretdを使用しました。  上記のコードを変更して、代わりにewretdを使用してください。  結果の図に変化はありますか？  これらのプロットにとってより良い市場指数は何だと思いますか？ なぜですか？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFJR</span>"
    ]
  },
  {
    "objectID": "chap10_FFJR.html#会計における資本市場研究",
    "href": "chap10_FFJR.html#会計における資本市場研究",
    "title": "\n3  FFJR\n",
    "section": "\n3.6 10.5 会計における資本市場研究",
    "text": "3.6 10.5 会計における資本市場研究\n\n本書のこの章は、おそらく学術研究が現実の会計現象を理解するのに最も貢献している分野である資本市場研究に焦点を当てている。  この章の目的の1つは、会計の資本市場研究に関連する古典的なアイデアと論文についての堅実な導入を提供することである。\n\nこの章では、効率的な資本市場のアイデアを紹介し、資本市場研究の初期のイベント研究の1つであるFama et al. (1969)に焦点を当てた。  次の2章、第11章と第12章では、それぞれBall and Brown (1968)とBeaver (1968)という2つの画期的な論文について取り上げる。  最初の3章は、50年以上前の論文を研究しているが、それらは今日でも有効な研究デザインの基礎を優れた導入を提供していると考えている。\n\n第13章は、Fama et al. (1969)以来のイベント研究の進化についても含め、イベント研究についてさらに詳しく説明するために、前の3章に基づいて構築されている。  第14章では、資本市場における会計情報の価格設定における多くの研究された異常である決算発表後の株価ドリフト(Post-Earnings Annoucement Drift: PEAD)を調査する。  第15章では、負債の計測についてさらに詳しく説明し、財務会計の重要な要素であることと、シミュレーション分析をより詳しく探求する機会を提供する。  第16章では、数十年にわたり多くの会計研究の焦点となってきた利益マネジメント(earnings management)を探求し、研究における測定に関連する問題や統計的検定の力を理解する機会を提供する。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFJR</span>"
    ]
  },
  {
    "objectID": "chap10_FFJR.html#footnotes",
    "href": "chap10_FFJR.html#footnotes",
    "title": "\n3  FFJR\n",
    "section": "",
    "text": "EMHのもう1つの深い含意は、価格がある意味で「正しい」はずであるということである。↩︎\nいくつかの場合、経営者は、分割後に自分自身で株式を購入したい場合など、分割後の株価が低いほうが有利になるかもしれない。↩︎\nシグナリング理論については、Kreps (1990)の第17章を参照。↩︎\nEvidence that FFJR have a signalling story in mind comes from subsequent sentences in the paper.↩︎\nAlso see https://www.postgresql.org/docs/current/tutorial-window.html.↩︎\nNote that we are operating on remote data here, so we use window_order() instead of arrange() when using window functions. In fact, window_order() offers power that is not available with local data frames. In Chapters 6 and 8, we used dbplyr implicitly when connecting to databases through the dplyr package, but window_order() is not “re-exported” by the dplyr package and thus we needed to invoke library(dbplyr) above.↩︎\naaa08↩︎\naaa09↩︎\naaa10↩︎\naaa11↩︎\naaa12↩︎",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFJR</span>"
    ]
  },
  {
    "objectID": "chap11_BB1968.html",
    "href": "chap11_BB1968.html",
    "title": "\n4  Ball and Brown (1968)\n",
    "section": "",
    "text": "4.1 Ball and Brown (1968)の主要結果\n本章と次章では，「the Seminal Contributions to Accounting Literature Award」の最初の2つの受賞者であるBall and Brown (1968)とBeaver (1968)について説明する。\nBall and Brown (1968)は最初に「the Seminal Contribution to the Accounting Literature Award」を受賞し，その受賞理由は「過去30年間において，他のどの論文よりも引用されている論文はなく，会計研究の発展において重要な役割を果たしている」とされている。  しかし，Philip Brown (Brown, 1989)は1989年のJARカンファレンスでのプレゼンテーションで，この論文がThe Accounting Reviewに拒否されたことを振り返り，編集者が「Rayと私が実証的な部分を削減し，私たちが試みた論文と会計文献との間に構築しようとした『橋』を拡大することを望むなら，原稿を再検討する用意がある」と述べている。\nKothari (2001, p. 113)によると，「Ball and Brown (1968)とBeaver (1968)は，現在知られている実証的な資本市場研究の到来を告げた」としている。  それ以前の時期には，会計研究は主に理論的な学問であり，規範的研究に焦点を当てていた。つまり，さまざまな事象や取引について「正しい」または「最良の」会計処理方法に関心を持つ研究であった。  会計理論は規範的であるだけでなく，主に帰納的であった。つまり，一般的な原則から詳細な理論が導かれていた。\nBeaver (1998)は，あるアプローチを「『理想的な』純利益にはどのような特性があるべきか？」と問うものとして特定している。  この質問に対する1つの回答は，ある期間の会計収益は，その期間中の株主へのキャッシュフローの現在価値の変化（およびキャッシュ配当）を反映すべきであるというものである。  しかし，他の回答も存在した。  会計研究者は，望ましい特性の集合から始め，これらを使用して長期資産の減価償却、棚卸資産、またはリース資産の会計処理の「最良」のアプローチを導き出していた。  Kothari (2001)は，理論の「実証的な妥当性」にはほとんど重点が置かれていなかったと指摘している。\n同様の考え方は，基準設定主体の考え方に依然として浸透しており，基準設定主体は「概念フレームワーク」から詳細な会計基準を導出しようとしている。 概念フレームワークは，資産や負債などの広義の定義を概説し，基準設定主体が任意の状況で正しい会計処理方法を導き出すために使用できるとされている。\nしかし，Ball and Brown (1968)以降の時期において，これらのアプローチは学術研究においてほとんど捨てられている。  主に規範的で理論的な重点は，実証的で経験的な重点に置き換えられている。\n本章では，Ball and Brown (2019)をBall and Brown (1968)の読み方のガイドとして使用し，Nichols and Wahlen (2004)によるBall and Brown (1968)のレプリケーションを検討する。\nBall and Brown (1968)の最初の2ページは，（当時の）既存の会計文献について述べている。  この議論から，当時の（学術的な）常識は，会計数値は（おおよそ）意味がない（「27のテーブルと8つの椅子の違い」）というものであり，より意味のある会計システムを考案するためにさらなる研究が必要であるという印象を受ける。\nおそらく，この考え方は，Ball and Brown (1968)の帰無仮説に影響を与えている。  会計数値が意味を持たない場合，それらは合理的な行為者によって行われる経済的な意思決定とは何の関係もないはずである。  Ball and Brown (1968)は，株式リターンと期待外収益変化との関係を調査することで，この（帰無）仮説を検証しようとしている。  Ball and Brown (1968)は，「資本理論の最近の発展により，証券価格の挙動を有用性の操作的なテストとして選択することが正当化される」と主張している。\nBall and Brown (1968)によって提供される証拠は，市場がより広範な情報を合理的かつ効率的に利用する場合にのみ，椅子とテーブルを追加することの有用性を疑問視する批評家を納得させるかもしれない。  Ball and Brown (1968)の結果には，そのような合理性や効率性に依存しない説明もある。  第一に，市場は「27のテーブルから8つの椅子を引いたもの」に反応するかもしれないが，それは何をしているのかわからないためである。  第二に，市場は「27のテーブルから8つの椅子を引いたもの」が意味をなさないことを知っているかもしれないが，それ以上の情報がないために頼ることができない。  彼らが取り組もうとしている議論を考慮すると，株式リターンを使用して有用性のテストを行う際に，市場が（a）効率的であり，（b）収益以外の豊富な情報セットにアクセスできるという仮定は暗黙的に含まれているように思われる。\nBall and Brown (2019, p.414)は，Ball and Brown (1968)の3つの主要な結果を次のように特定している。「最も基本的な結果は，会計収益と株価リターンが相関していたことである。  … それにもかかわらず，年次の会計収益にはタイムリー性が欠けていた。  … 収益発表月の後，API（異常リターンを累積したもの）は同じ方向にドリフトし続けた。」\nBall and Brown (1968)の図1は，表5に提供されたデータを描いており，Ball and Brown (2019)が指摘した3つの主要な結果すべてを含んでいる。\nBall and Brown (1968)の図1に報告されているリターンは，月 -12 で各ポートフォリオを形成するために使用される収益変数が，月 0 以降にならないと信頼できないため，実現可能なポートフォリオではない。  しかし、仮にポートフォリオを理論的に実現可能にするメカニズム（例：タイムマシンや魔法の妖精）が存在すると仮定することはできる。  たとえば、もし妖精さんが各企業の翌年の収益が増加するのか減少するのかを， 12 ヶ月前に教えてくれるとすれば、利益がプラスであるというニュースを期待できる企業の株を買い（ロング）、マイナスとなるニュースを期待する企業の株を売る（ショート）ことが可能になる。  この仮想的な妖精さんは，提供する情報を非常に限定的にしている点に注意が必要である。  たとえば，利益がどれほど増加・減少したのかについてさらに詳細をしりたいかもしれないが，妖精さんは利益ニュースの符号だけを教えてくれる。\nさらに，ポートフォリオを形成する際にこの情報を使用する方法には暗黙の制約がある。  サイズや流動性などの他の要因に応じてポートフォリオのウェイトを調整する方が良いかもしれないが，Ball and Brown (1968)の図1に暗黙的に含まれるポートフォリオはこれを行っていない。  また，図1に表されているポートフォリオには，年間を通じてポートフォリオのウェイトを調整する機会もない。\nさまざまな情報源の相対的価値を評価する際，Ball and Brown (1968)は， TI ， NI ， II として示される3つのポートフォリオの構築方法を考慮している（Ball and Brown (2019)は II を AI として示しており，以下ではこの表記に従う）。\nこれらのメトリクスを使用して，Ball and Brown (1968, p.1) は「1年間に個々の企業に関するすべての情報のうち，半分以上がその年の収益数値に含まれている」と結論づけている。  … ただし，年次の収益報告書はタイムリーな媒体として高く評価されていない。なぜなら，その内容の大部分（約85〜90％）は，中間報告書を含むより迅速な媒体によって把握されているからである。」\n3番目の主要な結果は，表5に示されており，収益サプライズは，収益発表後2ヶ月までの各月において，APIと統計的に有意な程度で相関していることがわかる。\nこの結果は，後に収益発表後のドリフト（または単にPEAD）として知られるようになり，Ball and Brown (1968)にとって問題となっていた。彼らは，市場収益の「ピーク・アヘッド」と取引コストによる取引の遅延が一部説明できる可能性があると主張している。  PEADについては，第14章で詳しく検討する。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ball and Brown (1968)</span>"
    ]
  },
  {
    "objectID": "chap12_Beaver1968.html",
    "href": "chap12_Beaver1968.html",
    "title": "5  Beaver (1968)",
    "section": "",
    "text": "本章では，「the Seminal Contributions to Accounting Literature Award」の2番目の受賞者であるBeaver (1968)について説明する。  Beaver (1968)は，Ball and Brown (1968)に続いており，投資家が決算発表に反応するかどうかを検討している。\n\nBeaver (1968)は，投資家の反応を測定するために2つのアプローチを使用している。  最初のアプローチは，取引量を使用して市場の反応を測定し，2番目のアプローチは，リターン残差の2乗を使用している。ここで，リターン残差は，観測されたリターンと市場モデルを使用して適合されたリターンとの差である（1968年，p. 78）。\n\n本章では，Beaver (1968)自体を詳しく調査した後，Beaver (1968)を批判するBamber et al. (2000)を紹介する。  その後，より新しいサンプル期間を使用してBeaver (1968)の「レプリケーション」を行い，そのレプリケーションを使用してBeaver (1968)とBamber et al. (2000)によって提起された問題について考える。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Beaver (1968)</span>"
    ]
  },
  {
    "objectID": "chap11_BB1968.html#principal-results-of-ball-and-brown-1968",
    "href": "chap11_BB1968.html#principal-results-of-ball-and-brown-1968",
    "title": "\n3  Ball and Brown (1968)\n",
    "section": "",
    "text": "3.1.1 ディスカッション課題\n\n\nBall and Brown (1968)の研究課題は何か？  この論文は説得力がありますか？\n\n\nBall and Brown (1968, pp. 177–178)の参考文献について気づいたことは何ですか？\n\n\nBall and Brown (1968)の「もっとも基礎的な結果」が関係あるいは相関と Does this also mean that Ball and Brown (1968) is a “merely” descriptive paper according to the taxonomy of research papers outlined in Chapter 4. How might the results of Ball and Brown (1968) be represented in a causal diagram assuming that accounting information is meaningful and markets are efficient? Would an alternative causal diagram be assumed by a critic who viewed accounting information as meaningless?\nDescribe how Figure 1 of Ball and Brown (1968) supports each of principal results identified by Ball and Brown (2019).\nConsider the causal diagrams you created above. Do the results of Ball and Brown (1968) provide more support for one causal diagram than the other?\nCompare Figure 1 of Ball and Brown (2019) with Figure 1 of BB68. What is common between the two figures? What is different?\nWhat does “less their average” mean in the title of Figure 1 of Ball and Brown (2019)? What effect does this have on the plot? (Does it make this plot different from Figure 1 of BB68? Is information lost in the process?)\nBall and Brown (2019, p. 418) say “in this replication we address two issues with the BB68 significance tests.” Do you understand the points being made here?\nBall and Brown (2019, p. 418) also say “the persistence of PEAD over time is evidence it does not constitute market inefficiency.” What do you make of this argument?\nWhat is the minimum amount of information that our hypothetical genie needs to provide to enable formation of the portfolios underlying TI, NI, and II? What are the rules for construction of each of these portfolios?\nBall and Brown (1968) observe a ratio of NI to TI of about 0.23. What do we expect this ratio to be? Does this ratio depend on the information content of accounting information?\nConsider the paragraph in Ball and Brown (2019, p. 418) beginning “an innovation in BB68 was to estimate …”. How do the discussions of these results differ between Ball and Brown (1968) and Ball and Brown (2019)?\nConsider column (4) of Table 2 of Ball and Brown (2019). Is an equivalent set of numbers reported in BB68? What is the underlying investment strategy associated with this column (this need not be feasible in practice)?\n\n\nBall and Brown (2019)の6.3節の見出しは「『useful』は『meaningless』を否定するか？」である。  「『無意味』でない」ということは「『無用』でない」ということを意味すると思か？  Ball and Brown (1968)がこれらの観点でどのような問題（または事実）に取り組んでいるか。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ball and Brown (1968)</span>"
    ]
  },
  {
    "objectID": "chap11_BB1968.html#ball-and-brown-1968の主要結果",
    "href": "chap11_BB1968.html#ball-and-brown-1968の主要結果",
    "title": "\n4  Ball and Brown (1968)\n",
    "section": "",
    "text": "4.1.1 ディスカッション課題\n\n\nBall and Brown (1968)の研究課題は何か？  この論文は説得力がありますか？\n\n\nBall and Brown (1968, pp. 177–178)の参考文献について気づいたことは何ですか？\n\n\nBall and Brown (1968)の「最も基本的な結果」が関連性または相関に関するものであることを踏まえると、この論文が因果関係についての証拠を提供していないと言うのは正しいのか？  これは，第4章で概説された研究論文の分類に従うと，Ball and Brown (1968)が「単なる」記述的論文であるということを意味するでしょうか？[^4]  会計情報が意味を持ち，市場が効率的であると仮定した場合，Ball and Brown (1968)の結果はどのように因果図で表現されるでしょうか？  会計情報を無意味と見なす批評家は，どのような代替の因果図を仮定するでしょうか？\n\n\nBall and Brown (2019)が特定した主要な結果それぞれを，Ball and Brown (1968)の図1がどのように支持しているかを説明してください。\n\n\nBall and Brown (1968)の結果は，上で作成した因果関係図のうち、どちら一方をより強く支持しているといえるのか？\n\n\nBall and Brown (2019)の図1とBall and Brown (1968)の図1を比較しなさい。これら2つの図の共通点は何か？異なる点は何か？\n\n\nBall and Brown (2019)の図1のタイトルにある「less their average」とは何を意味するか？  このプロットにどのような影響を与えるか？ –&gt;  （このプロットはBall and Brown (1968)の図1と異なるものか？ このプロセスで情報が失われているのか？）\n\n\nBall and Brown (2019, p. 418)は「このレプリケーションでは，Ball and Brown (1968)の有意性検定に関する2つの問題に取り組んでいる」と述べている。ここで述べられているポイントを理解できますか？\n\n\nBall and Brown (2019, p. 418)はまた「PEADの時間経過に伴う持続性は，それが市場の非効率性を構成していないことの証拠である」と述べている。この議論についてどう思いますか？\n\n\n\nTI 、NI 、 II の基礎となるポートフォリオを形成するために，仮想的な妖精さんが提供する必要がある最小限の情報は何ですか？ これらのポートフォリオの構築にはどのようなルールがありますか？\n\n\nBall and Brown (1968)は，NI と TI の比率が約 0.23 であることを観察している。この比率はどのようになると予想されますか？この比率は会計情報の情報内容に依存するのでしょうか？\n\n\nBall and Brown (2019, p. 418)の「Ball and Brown (1968)のイノベーションは，…」という段落を考えなさい。 これらの結果に関する議論は，Ball and Brown (1968)とBall and Brown (2019)の間でどのように異なるか？\n\n\nBall and Brown (2019)の表2の列（4）を考えなさい。 Ball and Brown (1968)で同等の数値が報告されているか？ この列に関連付けられた基礎となる投資戦略は何か（これは現実に実現可能である必要はあるか）？\n\n\nBall and Brown (2019)の6.3節の見出しは「『useful』は『meaningless』を否定するか？」である。  「『無意味』でない」ということは「『無用』でない」ということを意味すると思か？  Ball and Brown (1968)がこれらの観点でどのような問題（または事実）に取り組んでいるか。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ball and Brown (1968)</span>"
    ]
  },
  {
    "objectID": "chap11_BB1968.html#ball-and-brown-1968の再現",
    "href": "chap11_BB1968.html#ball-and-brown-1968の再現",
    "title": "\n4  Ball and Brown (1968)\n",
    "section": "\n4.2 Ball and Brown (1968)の再現",
    "text": "4.2 Ball and Brown (1968)の再現\n\nこの章では，Nichols and Wahlen (2004)に倣って，Ball and Brown (1968)の最新のレプリケーションを行う。\n\n利益とリターンのデータは，それぞれCompustatとCRSPから取得する。\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnec#| t(RPostgres::Postgres(),\n                bigint = \"integer\",\n                check_interrupts = TRUE)\n\nmsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"msf\"))\nmsi &lt;- tbl(db, Id(schema = \"crsp\", table = \"msi\"))\nccmxpf_lnkhist &lt;- tbl(db, Id(schema = \"crsp\",\n                             table = \"ccmxpf_lnkhist\"))\nstocknames &lt;- tbl(db, Id(schema = \"crsp\",\n                         table = \"stocknames\"))\n\nfunda &lt;- tbl(db, Id(schema = \"comp\", table = \"funda\"))\nfundq &lt;- tbl(db, Id(schema = \"comp\", table = \"fundq\"))\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nmsf &lt;- load_parquet(db, schema = \"crsp\", table = \"msf\")\nmsi &lt;- load_parquet(db, schema = \"crsp\", table = \"msi\")\nccmxpf_lnkhist &lt;- load_parquet(db, schema = \"crsp\",\n                               table = \"ccmxpf_lnkhist\")\nstocknames &lt;- load_parquet(db, schema = \"crsp\",\n                               table = \"stocknames\")\n\nfunda &lt;- load_parquet(db, schema = \"comp\", table = \"funda\")\nfundq &lt;- load_parquet(db, schema = \"comp\", table = \"fundq\")\n\n\n\n\n\n\n4.2.1 発表日とリターンデータ\n\nBall and Brown (1968)の収益発表日を取得するためには，かなりのデータ収集作業が必要であった。  幸いなことに，Ball and Brown (2019)で議論されているように，四半期Compustat(comp.fundq)には，1971年以降の利益発表日のデータがある。  Ball and Brown (1968)と同様に，我々は四半期と12月31日の年度末を持つ企業に関心がある。  これらの日付を月次CRSP(crsp.msf)のデータと整合させる必要があるため，annc_month変数を作成する。\n\nannc_events &lt;-\n  fundq |&gt;\n  filter(indfmt == \"INDL\", datafmt == \"STD\",\n         consol == \"C\", popsrc == \"D\") |&gt;\n  filter(fqtr == 4, fyr == 12, !is.na(rdq)) |&gt;\n  select(gvkey, datadate, rdq) |&gt;\n  mutate(annc_month = as.Date(floor_date(rdq, unit = \"month\")))\n\n\nBall and Brown (1968)やNichols and Wahlen (2004)が行うように，各利益発表時点（t）の t-11 月から t+6 月までのリターンを集計するためには，それらの月に関連付けられたCRSPの日付値が必要となる。\n\nannc_eventsとCRSPの月次株式ファイル（crsp.msf）の日付とのリンクを提供するtd_linkというテーブルを作成する。\n\n最初のステップは，月次CRSPの日付を順序付け，各月に対応する「取引日」値（td）を割り当てるテーブル（crsp_dates）を作成することである。最初の月には1，2番目の月には2，といった具合である。  crsp.msfの日付値はcrsp.msiの日付値と整合しているため，後者（はるかに小さい）のテーブルを使用することができる。\n\ncrsp_dates &lt;-\n  msi |&gt;\n  select(date) |&gt;\n  window_order(date) |&gt;\n  mutate(td = row_number()) |&gt;\n  mutate(month = as.Date(floor_date(date, unit = \"month\")))\n\ncrsp_dates |&gt; collect(n = 10)\n\nWe want to construct a table that allows us to link earnings announcements (annc_events) with returns from crsp.msf Because we are only interested in months where returns are available, we can obtain the set of potential announcement months from crsp_dates. The table annc_months has each value of annc_month and its corresponding annc_td from crsp_dates, along with the boundaries of the window that contains all values of td within the range (t-11, t+6), where t is the announcement month.\n\nannc_months &lt;-\n  crsp_dates |&gt;\n  select(month, td) |&gt; \n  rename(annc_month = month, annc_td = td) |&gt;\n  mutate(start_td = annc_td - 11L,\n         end_td = annc_td + 6L)\n\nannc_months |&gt; collect(n = 10)\n\n\n次に，annc_monthsをcrsp_datesと結合して，td_linkテーブルを作成する。\n\ntd_link &lt;-\n  crsp_dates |&gt;\n  inner_join(annc_months, join_by(between(td, start_td, end_td))) |&gt;\n  mutate(rel_td = td - annc_td) |&gt;\n  select(annc_month, rel_td, date)\n\n\n以下は，1つのannc_monthのデータである。\n\ntd_link |&gt; \n  filter(annc_month == \"2001-04-01\") |&gt; \n  collect() |&gt; \n  print(n = Inf)\n\n\nCompustatの収益発表日とCRSPのリターンを結びつけるために，ccm_link（第7章で使用）を使用する。\n\nccm_link &lt;-\n  ccmxpf_lnkhist |&gt;\n  filter(linktype %in% c(\"LC\", \"LU\", \"LS\"),\n         linkprim %in% c(\"C\", \"P\")) |&gt;\n  rename(permno = lpermno) |&gt;\n  mutate(linkenddt = coalesce(linkenddt, max(linkenddt, na.rm = TRUE))) |&gt;\n  select(gvkey, permno, linkdt, linkenddt) \n\n\nNichols and Wahlen (2004)は，NYSE，AMEX，NASDAQに上場している企業に焦点を当てており，それぞれexchcd値が1，2，3に対応している。  各企業の各時点でのexchcdの値はcrsp.stocknamesにある。  Nichols and Wahlen (2004)に倣って，1988年から2002年までの財務年度のデータを取得する。\n\nrets_all &lt;-\n  annc_events |&gt; \n  inner_join(td_link, by = \"annc_month\") |&gt;\n  inner_join(ccm_link, by = \"gvkey\") |&gt;\n  filter(annc_month &gt;= linkdt, annc_month &lt;= linkenddt) |&gt;\n  inner_join(msf, by = c(\"permno\", \"date\")) |&gt;\n  collect() |&gt;\n  inner_join(\n    stocknames |&gt; \n      filter(exchcd %in% c(1, 2, 3)) |&gt;\n      collect(), \n    join_by(permno, between(date, namedt, nameenddt))) |&gt;\n  select(gvkey, datadate, rel_td, permno, date, ret) |&gt;\n  filter(between(year(datadate), 1987L, 2002L))\n\n\nわかりやすくするために，(t-11, t+6)のウィンドウ内の各月にリターンを持つ企業に焦点を当て，full_panelテーブルがこれらの企業を特定する。\n\nfull_panel &lt;-\n  rets_all |&gt; \n  group_by(gvkey, datadate) |&gt; \n  mutate(n_obs = n()) |&gt; \n  ungroup() |&gt; \n  filter(n_obs == max(n_obs)) |&gt;\n  select(gvkey, datadate)\n\n\nrets &lt;-\n  rets_all |&gt;\n  semi_join(full_panel, by = c(\"gvkey\", \"datadate\")) \n\n\n他の初期の論文（例：Beaver, 1968; Fama et al., 1969）とは異なり，Ball and Brown (1968)は既知の交絡事象による観測値を除外していないことに注意されたい。[^5]\n\n\n4.2.2 サイズポートフォリオリターンのデータ\n\nBall and Brown (1968)は異常リターンに焦点を当て，企業固有の係数を持つ市場モデルを推定し，残差リターンを推定する基礎として，これを API として示している。  市場モデルからの残差の使用は，生のリターンが使用された場合に生じるクロスセクションの相関に関する懸念に対処している。  Ball and Brown (1968)は，約10%のリターンが産業要因によるものであるが，これが推論に与える影響は小さいと結論づけている。\n\n一方，Nichols and Wahlen (2004)は，異常リターンの尺度としてサイズ調整リターンを使用している。  サイズ調整リターンを計算するために，（第9章で見たように）Ken Frenchのウェブサイトから2種類のデータを取得する。\n\nまず，規模の十分位リターンのデータを取得する。  Ken Frenchのウェブサイトは，月次と年次の加重と等加重のポートフォリオ・リターンに対するコンマ区切りのテキストファイルを提供している。\n\nt &lt;- \"Portfolios_Formed_on_ME_CSV.zip\"\nurl &lt;- str_c(\"http://mba.tuck.dartmouth.edu\",\n             \"/pages/faculty/ken.french/ftp/\",\n             \"Portfolios_Formed_on_ME_CSV.zip\")\nif (!file.exists(t)) download.file(url, t)\n\n\nダウンロードしたテキストファイルを確認すると，このファイルにはいくつかのデータセットが含まれていることがわかる。  月次リターンを取得し，時価総額加重リターンと等加重リターンの両方のデータを抽出する。  等加重リターンのデータは、Equal Weight Returns – Monthlyという文字列で始まる行から始まり、Value Weight Returns – Annualという文字列で始まる行の数行手前で終わっている。\n\n# Determine breakpoints (lines) for different tables\ntemp &lt;- read_lines(t)\nvw_start &lt;- str_which(temp, \"^\\\\s+Value Weight Returns -- Monthly\")\nvw_end &lt;- str_which(temp, \"^\\\\s+Equal Weight Returns -- Monthly\") - 4\n\new_start &lt;- str_which(temp, \"^\\\\s+Equal Weight Returns -- Monthly\")\new_end &lt;- str_which(temp, \"^\\\\s+Value Weight Returns -- Annual\") - 4\n\n\nこれらの区切り行を特定したら，次の関数を使用してデータセットを読み込み，関連するデータテーブルを適切に整理することができる。  このテキストファイルではNA値は-99.99として表されており，日付はyyyymmの形式であり，これをyyyy-mm-01の形式の日付に変換し，monthとする。\n\n元のデータは，5パーセンタイルごとにリターンがある「ワイド」形式で提供されているが，データを「ロング」形式に再配置し，十分位（つまり10パーセンタイルごと）のみを保持し，デシルラベルをLo 10，Dec 2，…，Dec 9，Hi 10から1，2，…，9，10に変更する。\n\nread_data &lt;- function(start, end) {\n\n  Sys.setenv(VROOM_CONNECTION_SIZE = 500000)\n  \n  fix_names &lt;- function(names) {\n    str_replace_all(names, \"^$\", \"date\")\n  }\n\n  read_csv(t, skip = start, n_max = end - start,\n           na = \"-99.99\",\n           name_repair = fix_names,\n           show_col_types = FALSE) |&gt;\n    mutate(month = ymd(str_c(date, \"01\"))) |&gt;\n    select(-date) |&gt;\n    pivot_longer(names_to = \"quantile\",\n                 values_to = \"ret\",\n                 cols = -month) |&gt;\n    mutate(ret = ret / 100,\n           decile = case_when(quantile == \"Hi 10\" ~ \"10\",\n                              quantile == \"Lo 10\" ~ \"1\",\n                              str_detect(quantile, \"^Dec \") ~\n                                sub(\"^Dec \", \"\", quantile)),\n           decile = as.integer(decile)) |&gt;\n    filter(!is.na(decile)) |&gt;\n    select(-quantile)\n}\n\n\nこの関数を適用して関連データを抽出し，これらを1つのデータフレームsize_retsに結合することができる。\n\nvw_rets &lt;- \n  read_data(vw_start, vw_end) |&gt;\n  rename(vw_ret = ret)\n\new_rets &lt;- \n  read_data(ew_start, ew_end) |&gt;\n  rename(ew_ret = ret)\n\nsize_rets &lt;-\n  ew_rets |&gt;\n  inner_join(vw_rets, by = c(\"month\", \"decile\")) |&gt;\n  select(month, decile, everything())\n\nsize_rets\n\n\n  \n\n\n\nThe second set of data we need to get from Ken French’s website is data on the cut-offs we will use in assigning firms to decile portfolios in calculating size-adjusted returns.\nAgain the original data come in a “wide” format with cut-offs at every fifth percentile, so again we rearrange the data into a “long” format, retain only the deciles (i.e., every tenth percentile), and rename the decile labels from p10, p20, …, p90, and p100, to 1, 2, …, 9, and 10.6 Also, we are only interested in the cut-offs for December in each year and use filter() to retain only these.\n\nme_breakpoints_raw &lt;- \n  read_csv(t, skip = 1, \n           col_names = c(\"month\", \"n\",\n                         str_c(\"p\", seq(from = 5, to = 100, by = 5))),\n           col_types = \"c\",\n           n_max = str_which(temp, \"^Copyright\") - 3) |&gt;\n  mutate(month = ymd(str_c(month, \"01\"))) |&gt;\n  select(-ends_with(\"5\"), -n) |&gt;\n  pivot_longer(cols = - month,\n               names_to = \"decile\",\n               values_to = \"cutoff\") |&gt;\n  mutate(decile = str_replace(decile, \"^p(.*)0$\", \"\\\\1\")) |&gt;\n  mutate(decile = as.integer(decile)) \n\nFinally, we organize the data to facilitate their use in joining with other data. Specifically, we create variables for the range of values covered by each decile (from me_min to me_max). We specify the minimum value for the first decile as zero and the maximum value for the tenth decile to infinity (Inf).",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ball and Brown (1968)</span>"
    ]
  },
  {
    "objectID": "appC_Computing.html",
    "href": "appC_Computing.html",
    "title": "付録 C — 研究のためのコンピュータ概論",
    "section": "",
    "text": "C.1 Languages\nこの付録では，実証的会計研究で最も一般的に使用される統計プログラミング言語に焦点を当てて，研究コンピューティングの概要を簡単に説明する。  また，本書で強調しているアプローチを説明するのに役立つデータ管理に関連するいくつかの基本的なアイデアについても議論する。\n実際のデータ解析を含むコースを開発する際の欠点は，少なくとも1つの統計プログラミング言語の資料が必要であることである。  このコースでは，ツールとしてRと，ある程度間接的にPostgreSQLに焦点を当てている。  この選択が思われるほど制限的ではないと考えている。  ただし，この選択を説明する前に，主要な代替案について議論することが理にかなっている。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>研究のためのコンピュータ概論</span>"
    ]
  },
  {
    "objectID": "appC_Computing.html#languages",
    "href": "appC_Computing.html#languages",
    "title": "付録 C — 研究のためのコンピュータ概論",
    "section": "",
    "text": "C.1.1 SAS\n\nWikipediaによると，「SASは，データ管理，高度な分析，多変量分析，ビジネスインテリジェンス，犯罪捜査，予測分析のためにSAS Instituteによって開発された統計ソフトウェアスイートである」。  SASは，一部の理由で，会計およびファイナンスの研究の中心であり続けており，Wharton Research Data Services（WRDS，発音は「ワーズ」）で使用されているためである。  WRDSは，データセットをSASデータ形式で提供しており，歴史的には，WRDSは研究コンピューティングサーバーでSASを主要なプログラミング言語として提供していた。\n\nSASは，研究者に多くの利点を提供している。\n\n\n豊富な統計ツール\nデータセットを準備するために（PROC SQLを介して）SQLを使用できる\nデータ処理にディスク指向のため，大規模なデータセットを処理できる\nWRDSサーバーを介してほとんどの研究者が利用できる\n\n\nしかし，SASにはいくつかの欠点がある。\n\n\n他の言語のユーザーにとっては比較的馴染みのない専門的なプログラミング言語\nWindowsとLinux（つまり，MacOSではない）に限定されたプロプライエタリソフトウェア\n限られたデータ型のセット\nテキストデータの処理に関する制限\n一部の重要な機能（例：グラフ）は，ほとんどのユーザーにはかなりアクセスしにくいように見える\n\n\n\nC.1.2 Stata\n\nWikipediaによると，「Stataは，1985年にStataCorpによって作成された汎用統計ソフトウェアパッケージである。  そのほとんどのユーザーは，経済学，社会学，政治学，生物医学，疫学などの研究分野で働いている。\n\nStataは，研究者に多くの利点を提供している。\n\n\n経済学者やファイナンス，会計などの下流分野の研究者にとって特に豊富な統計ツール\nすべてのエンドユーザーオペレーティングシステム（Windows，MacOS，Linux）で利用可能\n友好的なインターフェースとコマンドベースの指向は，多くのワークフローに適している\n\n\nしかし，Stataにはいくつかの欠点がある。\n\n\nプロプライエタリシステム（つまり有料）\nインメモリ指向のため，大規模なデータセットには適していない\n1つのインメモリデータセットへの歴史的な制限は，データの準備を難しくする\nStataユーザーが不透明なコードを生成する傾向がある\n\n\n\nC.1.3 Python\n\nWikipediaによると，「Pythonは，インタプリタ型，高水準，汎用プログラミング言語である」。  統計分析や計量経済学のために特に設計されたわけではないが，Pythonはデータサイエンスコミュニティの中心となっており，Pythonを使用して会計研究のための研究コンピューティングタスクを実行することが可能になるパッケージが多数存在している。\n\nPythonは，研究者に多くの利点を提供している。\n\n\n完全な機能を備えたコンピュータプログラミング言語として，Pythonを使用してほとんどすべてのことができる\n統計学習などの分野において特に豊富な統計ツール\nすべてのエンドユーザーオペレーティングシステム（Windows，MacOS，Linux）で利用可能\n\n\nしかし，Pythonにはいくつかの欠点がある。\n\n\n一般的なコンピューティング言語としての起源から，データ解析に最適化されていない設計選択肢がいくつかある\n比較的非技術的なユーザーにとってセットアップが少し難しい\n\n\n\nC.1.4 PostgreSQL\nPostgreSQL is in some ways out of place in this list, as it is not a programming language or statistical software package. But given that much of this course uses PostgreSQL, we discuss it briefly here. According to its website, “PostgreSQL is a powerful, open source object-relational database system with over 35 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.” For our purposes, the relevant fact is that PostgreSQL is a powerful system for storing data in a way that it can be processed, combined, and used in analysis.\nThere are a number of similar database systems, including SQLite, MySQL, and Oracle. We use PostgreSQL in part because we make extensive use of WRDS data, which is made available in a PostgreSQL database, and also because PostgreSQL is a very robust database system offering perhaps the strongest feature set among the main open-source alternatives.\nStarting from Chapter 6, we make extensive use of data sources from the PostgreSQL supplied by WRDS. WRDS essentially provides data in two forms: as SAS data files (accessible via SFTP) and as tables in a PostgreSQL database.\nChapter 2 provides an extended tutorial on R with an emphasis on the so-called Tidyverse. Using dbplyr, tables in a (possibly remote) database can be represented as a kind of data frame that allows us “to use remote database tables as if they are in-memory data frames by automatically converting dplyr code into SQL” (we explain what a data frame is in Chapter 2). In this way, the dbplyr package allows us to deploy the skills we learn in Chapter 2 to the manipulation and analysis of data stored in relational databases almost seamlessly. The best way to understand these ideas is through hands-on use, for which we provide guidance in the chapters beginning with Chapter 6.\n\n\nC.1.5 R\n\nWikipediaによると，「Rは，統計計算とグラフィックスのためのプログラミング言語およびフリーソフトウェア環境である」。\n\nSASやStataと比較して，Rはオープンソースで無料であるという利点がある。  SASと同様に，ほとんどすべての統計手続きやデータ管理タスクをサポートするためのパッケージがほとんど無限にあるように思われる。  Rの欠点は，Stataと同様に，デフォルトのアプローチがインメモリデータ解析を使用していることである。  しかし，後述するように，ディスク上のデータ操作と解析を容易にするパッケージを使用することで，この欠点を克服することができる。\n\n\nC.1.6 Why R?\n\n本書には多くのRコードが含まれており，読者はなぜこの本でRを選んだのかと尋ねるかもしれないが，このセクションでその答えを提供する。\n\nもっと基本的な質問は，どのソフトウェアパッケージを選ぶのかということである。  我々は，研究論文や研究手法だけでなく，それらの方法を適用して論文の結果を生み出すために必要な計算手順を詳細に学ぶことには，教育上の重要な利点があると考えている。  たとえば，読者が調整できる実際のコードを持っていることで，異なるアプローチを使用した場合に結果がどのように異なるかを読者が理解することが容易になる。  本書に含まれる多くの複製を生成するには，読者が本書でカバーされているすべての内容について行うことができる可能性は低いため，特に研究コンピューティングに新しい読者にとっては，かなりの時間を要する。\n\n本書にコードを含めることを決定した後，なぜ特にRを選んだのか。  別のアプローチとして，複数の言語でコードを含める方法がある。  たとえば，「The Effect: An Introduction to Research Design and Causality」（Huntington-Klein, 2021）は，Python，R，Stataのコードを提供している。  そして，「Causal Inference: The Mixtape」（Cunningham, 2021）は，RとStataのコードを提供している。\n\nこれらの2つの本と当書の間には，2つの重要な違いがある。  第1に，本書のすべての分析を再現するためのコードを含めることが重要だと考えた。  これに対して，「The Effect」と「The Mixtape」は，それらに含まれる分析の一部を生成するためのコードのみを含んでいる。  これらの本のすべての分析にコードを含めることは，おそらく長さと複雑さが大幅に増加し，複数の言語で行うことはさらに増加するだろう。\n\n第2に，「The Effect」と「The Mixtape」は，複雑なデータステップをほとんどスキップし，小規模なデータセットを使用している。  これに対して，通常，分析を生成するために必要なすべてのデータステップを実行する。  また，CRSPやCompustatなどの（比較的！）大規模なデータセットを使用しており，データを使用している統計解析パッケージに取り込む「物流」に注意を払う必要がある。\n\nこの点において，本書で使用しているアプローチは，これらのデータセットに特に低コストでアクセスする方法を提供しており，6.1節で詳しく説明する。  たとえば、Stataを使用して同様の結果を得るには，ほとんどの場合，SQLを使用した大規模な「データステップ」（SAS用語を使用）に続いて，結果のデータフレームを使用したデータ解析が必要となる。  ここで使用されているdplyrベースのアプローチは，理解しやすいコードを生成すると考えている。[^2]\n\n最後に，本書はQuartoで書かれており，Rコードと最もシームレスに動作するソフトウェアパッケージである。  StataやSASを使用してこの本を作成することは，はるかに困難であったであろう。\n\n本書はRを使用しているが，SAS，Stata，Pythonの熱心なユーザーにも興味を持ってもらえると考えている。  第1に，Rの学習に最小限の投資をすることで，本書から多くのことを得ることができると思われる。  コードを疑似コードのように見ても理解できるように努めており（この点において，Rは，SASやStataなどに比べて優れていると考えている），多くのことを得ることができる。  コードを実行せずに本を読むこともできるが，練習問題のいくつかを行うためにコードを実行することを強くお勧めする。  初期設定を行った後（1.2節を参照），本書のすべての分析は，コードをRにコピー＆ペーストすることで生成することができ，本書は，各章のコードが他の章のコードと独立して実行できるように書かれている（ただし，章の後半のコードは，章の前半のコードに依存する場合がある）。\n\n第2に，Stataの熱心なユーザーであるとしても，Rに投資することは価値があると考えている。  Stataの熱心なユーザーであっても，共著者が使用する可能性があるSASを読む能力から利益を得ることができるだろうし，論文の著者がSASコードを提供することもある。  Rの人気が高まるにつれて，Rを読む能力はより重要になる可能性がある。  すでにRは，統計学の多くの分野（Pythonと共に）や機械学習などで一種の共通語となっている。\n\n最後に，「Xの熱心なユーザー」である研究者は，おそらく絶滅危惧種である。  新興研究者のほとんどは，キャリア全体で複数の統計言語を使用する必要があると考えられ，Rはそのうちの1つである。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>研究のためのコンピュータ概論</span>"
    ]
  },
  {
    "objectID": "appC_Computing.html#data-management",
    "href": "appC_Computing.html#data-management",
    "title": "付録 C — 研究のためのコンピュータ概論",
    "section": "C.2 Data management",
    "text": "C.2 Data management\n\n\nC.2.1 データストレージのための望ましい条件\n\n\nディスク上のデータへの高速なランダムアクセス  時折，メモリ内データの処理の代替手段を比較するベンチマークを見ることがある。  しかし、データは主にディスク上にあり，これらのデータに迅速にアクセスできる能力が必要となる。  多くの場合，データセットの観測値の一部のみが必要となる。  たとえば，いくつかの日付のマイクロソフトの株価データが必要となる場合がある。  これらのデータは，WRDS PostgreSQLデータベースのcrsp.dsfにあり，そのシステムから非常に迅速に取得できる。  一方，一部のデータストレージシステムでは，これらの行を取得するためにcrsp.dsfの全体をロード（または少なくともスキャン）する必要がある。\n\n\n\nどのソフトウェアパッケージからもアクセス可能なデータ  理想的には，データはR，Stata，Python，Perlなどのほとんどのソフトウェアからアクセス可能である。  これを行うことは，データの共同作業や多言語プログラミングにおいて微妙だが重要な利点がある。  たとえば，Webデータからデータセットを準備するのに助けが必要なStataの信者かもしれない。  これを行うスキルを持つ人々を見つけることは，Pythonの専門家と協力するためにStataスキルを持っていない必要がない場合にははるかに簡単である。  データがStataを介してアクセスできる限り，Stataユーザーは，データがPythonまたはStataを使用して作成されたかどうかについてはあまり気にしないだろう。  または，いくつかの複雑なPerlコードを書いてデータセットを生成し，時折，ソースデータの変更に対応するためにコードを微調整する必要があるかもしれない。  データセットがどのシステムからもアクセス可能な形式で保存されている限り，コード全体をまだコーディングできる形式に変換する必要はない。\n\n\n\nどこからでもアクセス可能なデータ  理想的には，データはインターネット接続があればどこからでもアクセスできるようになっているべきである（2020年代には，これは基本的にどこでもということである）。  たとえば，ボストンの研究者がカリフォルニア，オーストラリア，シンガポールの共著者とデータを共有するのは簡単であるべきである。\n\n\n\n集中処理の可能性  かつてほど重要ではなくなったが，データ処理と分析を自分のラップトップよりも強力なコンピュータに移動できる能力は，今日でも依然として有用である。  本文では，Rコードをローカルで実行している場合でも，データ処理の重い部分をWRDS PostgreSQLサーバーで実行するケースがいくつかある。  SASはRSUBMITと同様の機能を提供している。\n\n\n\n幅広いデータ型のサポート  理想的には，利用可能なデータ型には，文字列，浮動小数点数，整数，大きな整数，日付，タイムゾーン付きのタイムスタンプ，JSON，XMLなどが含まれるべきである。  一方，テキストファイルは単なるテキストであり，データ型はデータを読み込む際に推論されるか，別のファイルで指定される。  他方，上記にリストされているすべてのデータ型とそれ以上のデータ型を提供するPostgreSQLなどのシステムがある。\n\n\n\nテキストデータを容易に処理できる能力  テキストデータは，近年の研究において重要なデータソースとして浮上している。  実際には，良好なテキスト処理能力は，UnicodeとUTF-8でのエンコードをサポートすることを意味することが多い。  Stataによると，「Unicodeは，あなたが今読んでいる単語の文字などをコンピュータがエンコードする現代的な方法です。」  さらに，テキストフィールドは「ワイド」であり，データストレージアプローチは，その有用性を制限する制約を課さないようにする必要がある。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>研究のためのコンピュータ概論</span>"
    ]
  },
  {
    "objectID": "appB_SQL.html",
    "href": "appB_SQL.html",
    "title": "付録 B — SQL入門",
    "section": "",
    "text": "B.1 What are SQL and dplyr?\nThis brief appendix aims to serve two groups of users. The first group comprises those who have followed the material in the main part of the book and would like a quick introduction to SQL. The second group comprises those who know SQL (say, SAS’s PROC SQL) would like a quick introduction to the dplyr-based approach to R that we use in this book.\nSQL is a specialized language for manipulating and retrieving tabular data used by almost all modern database systems.\nThe R package dplyr is a core part of the Tidyverse and perhaps the package we use the most in this book. From the Tidyverse website:\nPrior to the advent of the dplyr in 2014, most users of R would have used base R functions and operators, such as subset(), $, and [. However, dplyr provides a much more consistent framework for manipulating data that is easier to learn, especially for users familiar with SQL. This is unsurprising given that SQL provides something like a “grammar of data manipulation” of its own. In fact, each dplyr verb has an SQL equivalent keyword or concept, as seen in Table B.1, which provides a translation table of sorts between dplyr and SQL.",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>SQL入門</span>"
    ]
  },
  {
    "objectID": "appB_SQL.html#what-are-sql-and-dplyr",
    "href": "appB_SQL.html#what-are-sql-and-dplyr",
    "title": "付録 B — SQL入門",
    "section": "",
    "text": "dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names\nfilter() picks cases based on their values\nsummarize() reduces multiple values down to a single summary\narrange() changes the ordering of the rows\n\n\n\nSQL translation of key dplyr verbs\n\n\ndplyr verb (R)\n\nSQL equivalent\n\n\n\n|&gt;\nFROM\n\n\nselect()\nSELECT\n\n\nfilter()\nWHERE\n\n\ngroup_by()\nGROUP BY\n\n\narrange()\nORDER BY\n\n\nmutate()\nused-defined columns\n\n\nsummarize()\nused-defined aggregate columns\n\n\n\n\n\n\nPostgreSQL\nparquet\n\n\n\nIn this appendix, we use the following packages:\n\nlibrary(DBI)\nlibrary(dplyr)\n\nAs in earlier chapters, we set up a database connection that we can use within R. We also create remote data frames for each of the two tables from WRDS that we use in this appendix: crsp.dsf and crsp.dsi.\n\ndb &lt;- dbConnect(RPostgres::Postgres())\n\ncrsp.dsf &lt;- tbl(db, Id(table = \"dsf\", schema = \"crsp\"))\ncrsp.dsi &lt;- tbl(db, Id(table = \"dsi\", schema = \"crsp\"))\n\n\n\nIn this appendix, we use the following packages:\n\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(farr)\n\nAs in earlier chapters, we set up a database connection that we can use within R. We also create remote data frames for each of the two tables from WRDS that we use in this appendix: crsp.dsf and crsp.dsi. A wrinkle with our parquet-based approach to DuckDB is that to access the tables from SQL, we need to create named tables that refer to the underlying parquet files. The create_view() function does this. Note that we use CREATE VIEW rather than CREATE TABLE so that the data are not read into memory (this is costly with crsp.dsf).\n\ncreate_view &lt;- function(conn, table, schema = \"\", \n                         data_dir = Sys.getenv(\"DATA_DIR\"))  {\n  dbExecute(conn, paste0(\"CREATE SCHEMA IF NOT EXISTS \", schema))\n  file_path &lt;- file.path(data_dir, schema, paste0(table, \".parquet\"))\n  df_sql &lt;- paste0(\"CREATE VIEW \", schema, \".\", table, \" AS \",\n                   \"SELECT * FROM read_parquet('\", file_path, \n                   \"')\")\n  DBI::dbExecute(conn, dplyr::sql(df_sql))\n}\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\ncrsp.dsi &lt;- create_view(db, table = \"dsi\", schema = \"crsp\") \ncrsp.dsf &lt;- create_view(db, table = \"dsf\", schema = \"crsp\")",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>SQL入門</span>"
    ]
  },
  {
    "objectID": "appB_SQL.html#sql-terms-select-and-from",
    "href": "appB_SQL.html#sql-terms-select-and-from",
    "title": "付録 B — SQL入門",
    "section": "\nB.2 SQL terms SELECT and FROM\n",
    "text": "B.2 SQL terms SELECT and FROM\n\nLet’s begin with a basic SQL query. (Note that, throughout this appendix, we only display the first five records when there are more than five records returned by a query. Note that a record is SQL terminology for what we might call a row in the context of an R data frame.)\n\nSELECT date, vwretd, ewretd\nFROM crsp.dsi\n\nThe query above\n\nextracts the data in three columns (date, vwretd, ewretd) (the first line)\nfrom the table named crsp.dsi (the second line)\n\nWhile the syntax differs, SQL’s SELECT operates very much like select() from dplyr.\nTranslating this into dplyr code using the pipe operator (|&gt;), it’s easy to see that the order of presentation is one of the big differences between SQL and dplyr.\nTo use dplyr we first need to set up the table on which to operate; once we’ve done so we can see that SQL’s FROM is implicit in the |&gt; operator.\n\ncrsp.dsi |&gt;\n  select(date, vwretd, ewretd)",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>SQL入門</span>"
    ]
  },
  {
    "objectID": "appB_SQL.html#sql-where",
    "href": "appB_SQL.html#sql-where",
    "title": "付録 B — SQL入門",
    "section": "\nB.3 SQL WHERE\n",
    "text": "B.3 SQL WHERE\n\nThe filter() verb from dplyr corresponds to WHERE in SQL. Note that WHERE goes after the FROM clause in SQL, though in practice the query optimizer will execute the filter implied by the WHERE clause before executing other elements of a query (it would be wasteful to perform calculations on data that are going to be filtered out later on).\n\nSELECT date, vwretd, ewretd\nFROM crsp.dsi\nWHERE date = '2015-01-02'",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>SQL入門</span>"
    ]
  },
  {
    "objectID": "chap09_import.html",
    "href": "chap09_import.html",
    "title": "\n2  データを読み込む\n",
    "section": "",
    "text": "2.1 (見かけ上) 非表形式データの読み込み\nこれまでのデータは、farrパッケージまたはWRDSから取得してきた。  ほとんどの場合、WRDSデータはきれいな長方形であり、 N 個の観測値と K 個の変数があり、データ型が割り当てられている。1  実際には、研究者はしばしばインターネットなどの他のソースからデータを取得し、そのようなデータはしばしばかなり乱雑である。  この章では、そのようなソースからデータをインポートする方法について紹介する。\nWhile this chapter is fairly task-oriented, we think that it serves to reinforce two deeper ideas.\nWe also introduce regular expressions. Briefly speaking, a regular expression is a sequence of characters that define a pattern which can be used in a kind of search (or search-and-replace) on steroids. Regular expressions are very useful when working with data in many contexts.2 The chapters on strings and regular expressions in R for Data Science provide excellent introductions that complement material in this chapter. We recommend that you refer to those chapters as you work through this chapter.",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>データを読み込む</span>"
    ]
  },
  {
    "objectID": "chap09_import.html#見かけ上-非表形式データの読み込み",
    "href": "chap09_import.html#見かけ上-非表形式データの読み込み",
    "title": "\n2  データを読み込む\n",
    "section": "",
    "text": "2.1.1 Fama-French産業定義\n\n最初に探求するタスクは、Fama-French産業分類のデータを収集することである。これは、ファイナンスや会計研究において広く使用されており、数百種類あるSICコードを、分析のためのより少ない数の産業グループにマッピングするために役立つ。3  –&gt; 例えば、企業を48、12、または5つの産業グループに分類することが考えられる。\n\nFama-French産業分類の基本データは、Tuckビジネススクールのケン・フレンチのウェブサイトから入手可能である。\n\n複数の分類が存在し、5つの産業から始まり、10、12、17、30、38、48、そして最終的に49の産業に分けられる。データはzip形式のテキストファイルとして提供されている。例えば、48の産業分類データは、このページにある「Download industry definitions」と表示されたリンクをクリックすることで見つけることができる。\n\nそのリンク先のファイルをダウンロードして解凍すると、テキストエディタやExcelで開くことができる。ファイルの最初の10行は以下の通りである：\n1 Agric Agriculture 0100-0199 Agricultural production - crops 0200-0299 Agricultural production - livestock 0700-0799 Agricultural services 0910-0919 Commercial fishing 2048-2048 Prepared feeds for animals\n2 Food Food Products 2000-2009 Food and kindred products 2010-2019 Meat products\nLooking at the second row, we interpret this as saying that firms with SIC codes between 0100 and 0199 are assigned to industry group 1 (let’s call this field ff_ind), which has a label or short description (ff_ind_short_desc) Agric and a full industry description (ff_ind_desc) of Agriculture.\nOne approach to this task might be to write a function like the following (this one is woefully incomplete, as it only covers the first two lines of data above):\n\nget_ff_ind_48 &lt;- function(sic) {\n  case_when(sic &gt;= 100 & sic &lt;= 199 ~ 1,\n            sic &gt;= 200 & sic &lt;= 299 ~ 1)\n}\n\nWhile tedious and time-consuming, this is perfectly feasible. In fact, this is essentially the approach taken in code you can find on the internet (e.g., SAS code here or here or Stata code here).\nHowever, doing this would only solve the problem for the 48-industry grouping. And it certainly could not be described as particularly robust to, for example, changes in Fama-French industry definitions.4\nPart of the solution that we use below recognizes that the data are really tabular in nature. A relational database purist would likely look at the data above as representing two tables. One table relates Fama-French industries to short and full industry descriptions. The first two rows in this table would look something like this:\n\n\nff_ind\nff_ind_short_desc\nff_ind_desc\n\n\n\n1\nAgric\nAgriculture\n\n\n2\nFood\nFood Products\n\n\n\nThe second table would relate Fama-French industries to ranges of SIC codes, and the first few rows of this table would look something like this:\n\n\n\n\n\n\n\n\nff_ind\nsic_min\nsic_max\nsic_desc\n\n\n\n1\n100\n199\nAgricultural production - crops\n\n\n1\n200\n299\nAgricultural production - livestock\n\n\n1\n700\n799\nAgricultural services\n\n\n1\n910\n919\nCommercial fishing\n\n\n\nTo keep things simple for this exercise, we will disappoint the purists and make a single table with all six fields: ff_ind, ff_ind_short_desc, ff_ind_desc, sic_min, sic_max, sic_desc.5\nSo how do we make this table? One approach to this task might be to download the linked file, unzip it, open it up in some program (e.g., Excel), and then massage the data manually into the desired form. But this would have the same issues as the approach above.\nWe can do better by using R and tools from the Tidyverse package. The first step is to download the data. While one can easily do this manually, but we want to automate this process as much as possible. And we probably don’t have any reason to keep the .zip file once we have used it. R provides two functions that we can use here: download.file() and tempfile(). The tempfile() function creates a random file name in a location that will be cleaned up by our system automatically once we’re no longer using it.\n\nt &lt;- tempfile(fileext = \".zip\")\nurl &lt;- str_c(\"http://mba.tuck.dartmouth.edu/pages/faculty/\",\n             \"ken.french/ftp/Siccodes48.zip\")\n\nHere t is file65404704f67c.zip, which is random except for the .zip extension, something we need for our code to recognize the supplied file as a zipped file. The download.file() function downloads the file at url and saves it as t.\n\ndownload.file(url, t)\n\nIf you look at Ken French’s website, you will see that all the industry-definition files have URLs that follow a certain pattern, with just the number of industry groups (in this case, 48) changing. Recognizing this, we can rewrite the code above as follows:\n\nind &lt;- 48\nt &lt;- tempfile(fileext = \".zip\")\nurl &lt;- str_c(\"http://mba.tuck.dartmouth.edu\",\n             \"/pages/faculty/ken.french/ftp/Siccodes\", ind, \".zip\")\ndownload.file(url, t)\n\n\nここでは、stringrパッケージのstr_c()関数がすべての部分を結合している。6\nFrom visual inspection, we can see that our text file is a fixed-width format text file. So to read the data, we will use the function read_fwf() from the readr package.\nThere are two required arguments to read_fwf(): file and col_positions. From the help for read_fwf() (type ? readr::read_fwf in the R console to see this), we see that col_positions refers to “Column positions, as created by fwf_empty(), fwf_widths() or fwf_positions(). If the width of the last column is variable (i.e., we have a ragged fixed-width format file), we can supply the last end position as NA.” We can also see that fwf_widths() is itself a function: fwf_widths(widths, col_names = NULL).\nGiven that we have a very simple file, we can identify the column widths pretty easily. Manually adding a “ruler” of sorts at the top of the file, we can see below that the first column covers columns 1-3, the second column covers 4-10, and the third column starts at 11.\n123456789-123456789-123456789-123456789-123456789-… 1 Agric Agriculture 0100-0199 Agricultural production - crops 0200-0299 Agricultural production - livestock 0700-0799 Agricultural services So we have widths of 3, 7, and we can use NA to have R figure out the width of the last column. The first two columns should be named ff_ind and ff_ind_short_desc, but the third column is problematic, as some rows provide information on ff_ind_short_desc and some rows provide data that we will eventually put into sic_min and sic_max; so let’s call that column temp for now. Finally, as the first column contains integer values, while the other two are text columns, we can supply a string to the option col_types argument of read_fwf() to ensure that the columns are read as those types.\n\nt |&gt;\n  read_fwf(col_positions = fwf_widths(\n    c(3, 7, NA),\n    c(\"ff_ind\", \"ff_ind_short_desc\", \"temp\")\n    ), col_types = \"icc\") \n\n\n次のステップは、問題のあるtemp列を処理することである。  ff_indがNAであれば、tempには（何かが含まれていれば）SICコードの範囲が含まれているが、ff_indがNAでない場合、tempにはff_ind_descに格納したい値が含まれていることがわかる。  mutateとif_elseステートメントを使用して、適切な列にデータを抽出することができる（一時的にSICコードの範囲をsic_rangeと呼ぶことにする）。そして、その後、temp列を削除することができる。7\n\nt |&gt;\n  read_fwf(\n    col_positions = fwf_widths(\n        c(3, 7, NA),\n        c(\"ff_ind\", \"ff_ind_short_desc\", \"temp\")),\n    col_types = \"icc\") |&gt;\n  mutate(ff_ind_desc = if_else(!is.na(ff_ind), temp, NA),\n         sic_range = if_else(is.na(ff_ind), temp, NA)) |&gt;\n  select(-temp)\n\nWe are getting closer. Now, we see that the issue is that our sic_range column does not line up with the other three columns. To solve this, we can use fill() from the tidyr package. The fill function accepts arguments for the columns to “fill”. In this case, we can fill missing values with the previous non-missing value.\n\nt |&gt;\n  read_fwf(\n    col_positions = fwf_widths(\n        c(3, 7, NA), c(\"ff_ind\", \"ff_ind_short_desc\", \"temp\")),\n    col_types = \"icc\") |&gt;\n  mutate(ff_ind_desc = if_else(!is.na(ff_ind), temp, NA),\n         sic_range = if_else(is.na(ff_ind), temp, NA)) |&gt;\n  select(-temp) |&gt;\n  fill(ff_ind, ff_ind_short_desc, ff_ind_desc)\n\nAt this point, we have no further use for the rows where sic_range is NA, so we can filter them out.\n\nt |&gt;\n  read_fwf(\n    col_positions = fwf_widths(\n        c(3, 7, NA), c(\"ff_ind\", \"ff_ind_short_desc\", \"temp\")),\n     col_types = \"icc\") |&gt;\n  mutate(ff_ind_desc = if_else(!is.na(ff_ind), temp, NA),\n         sic_range = if_else(is.na(ff_ind), temp, NA)) |&gt;\n  select(-temp) |&gt;\n  fill(ff_ind, ff_ind_short_desc, ff_ind_desc) |&gt;\n  filter(!is.na(sic_range))\n\nThe last issue to address is the column sic_range. We want to split that into three target columns (sic_min, sic_max, and sic_desc). To do this, we can use extract() from the tidyr package. The two required arguments for extract() are col, the column from which data are being extracted, and into, the columns that will get the data.\n\nextract()のregex引数を使用すると、データを分割するために使用される正規表現を指定できる。  正規表現については、後の章で詳しく説明するため、ここで何が起こっているか完全に理解できなくても問題ない。  今のところ、各括弧のペア((と))に含まれる部分が各フィールドにキャプチャされるものであることを知っておくだけで十分である。  最初の括弧は[0-9]+を囲んでおり、これは「1つ以上の数字文字」を表すと読むことができる。  したがって、0100-0199 Agricultural production - cropsの場合、これは0100に一致する。  これに続くのは、-であり、これは0100の後の-に一致する。  次の括弧は再び「1つ以上の数字文字」であり、0199をキャプチャする。  これに続くのは\\\\s*である。  \\\\sは「任意のスペース」を表し、*は「0個以上」を意味するため、\\\\s*は「0個以上のスペース」を意味し、一致するがキャプチャされない。  最後に、(.*)$がある。 The . represents “any character”, so .* means “zero or more of any character”, which are captured as the third variable. この.は「任意の文字」を表し、.*は「任意の文字の0個以上」を意味し、これが3番目の変数としてキャプチャされる。  $は「文字列の終わり」を意味し、これは正規表現の先頭にある^と対応しており、「文字列の開始」を意味する。  ^と$を合わせることで、分析対象が文字列全体であることを確認する。8\n\n引数convert = TRUEは、extract()に抽出されたフィールドのデータ型を適切な型（例えば、sic_minとsic_maxに整数）に変換するように求める。\n\nt |&gt;\n  read_fwf(col_positions = fwf_widths(\n    c(3, 7, NA), c(\"ff_ind\", \"ff_ind_short_desc\", \"temp\")),\n           col_types = \"icc\") |&gt;\n  mutate(ff_ind_desc = if_else(!is.na(ff_ind), temp, NA),\n         sic_range = if_else(is.na(ff_ind), temp, NA)) |&gt;\n  select(-temp) |&gt;\n  fill(ff_ind, ff_ind_short_desc, ff_ind_desc) |&gt;\n  filter(!is.na(sic_range)) |&gt;\n  extract(sic_range, \n          into = c(\"sic_min\", \"sic_max\", \"sic_desc\"),\n          regex = \"^([0-9]+)-([0-9]+)\\\\s*(.*)$\",\n          convert = TRUE) \n\nLastly, we can put all of the above into a function. But, as we do so, let’s take a gamble that the same code will work for any of the Fama-French industry classifications if we only change the URL. To do this, we use an argument ind that reflects the industry grouping of interest and inserts that in the URL. Here we use the str_c() function from the stringr package to create the URL using the value supplied as ind.\n\nget_ff_ind &lt;- function(ind) {\n  t &lt;- tempfile(fileext = \".zip\")\n  url &lt;- str_c(\"https://mba.tuck.dartmouth.edu/pages/\",\n               \"faculty/ken.french/ftp/Siccodes\", ind, \".zip\")\n  download.file(url, t)\n\n  t |&gt;\n    read_fwf(col_positions = fwf_widths(c(3, 7, NA),\n                                        c(\"ff_ind\", \"ff_ind_short_desc\",\n                                          \"temp\")),\n             col_types = \"icc\") |&gt;\n    mutate(ff_ind_desc = if_else(!is.na(ff_ind), temp, NA),\n         sic_range = if_else(is.na(ff_ind), temp, NA)) |&gt;\n    select(-temp) |&gt;\n    fill(ff_ind, ff_ind_short_desc, ff_ind_desc) |&gt;\n    filter(!is.na(sic_range)) |&gt;\n    extract(sic_range, \n            into = c(\"sic_min\", \"sic_max\", \"sic_desc\"),\n            regex = \"^([0-9]+)-([0-9]+)\\\\s*(.*)$\",\n            convert = TRUE) \n}\n\nWe can test out for 48-industry classification above (just to make sure we didn’t mess up what we already had working):\n\nget_ff_ind(48)\nget_ff_ind(5)\nget_ff_ind(12)\n\n\n2.1.2 練習問題\n\nFollow the steps below to produce a data set where each column has the appropriate data type.\n\nGo to the MSCI GICS website.\nGet the link to the file under historical GICS structures that is “Effective until Sep 28, 2018”.\nUse this link and the tempfile() and download.file() functions to download the linked file.\nUse read_excel() from the readxl package to read the downloaded file.\nIdentify any variables that need to be handled like temp in the Fama-French data set above and process accordingly.\nUse the fill() function from the tidyr package to fill in rows as necessary.\nMake sure that each column has the appropriate data type.\n\n\n\nHints:\n\nYou may find it helpful to look at the Excel file so you can see how the data are structured.\nThe read_excel() function has skip and col_names arguments that you will probably want to use.\nYour final column names should be sector, sector_desc, ind_group, ind_group_desc, industry, industry_desc, sub_ind, sub_ind_desc, and sub_ind_details.\nThe following code snippets might be useful:\n\nfilter(!is.na(sub_ind_details))\nfill(sector:sub_ind_desc, .direction = “down”)\nmutate(across(where(is.numeric), as.integer))",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>データを読み込む</span>"
    ]
  },
  {
    "objectID": "chap09_import.html#extracting-data-from-messy-formats",
    "href": "chap09_import.html#extracting-data-from-messy-formats",
    "title": "\n2  データを読み込む\n",
    "section": "\n2.2 9.2 Extracting data from messy formats",
    "text": "2.2 9.2 Extracting data from messy formats\nSometimes data are provided in formats even messier than fixed-width text files. For example, we may want to extract data (perhaps tabular data) from a PDF. While the data may appear tabular to our eyes, the reality is that PDFs retain very little information about the structure of data, as the PDF format is a lightweight way of making a computer or printer present text for consumption by humans, not statistical software packages.\nIn this section, we will examine a case study in extracting tabular data from a PDF. Our initial focus will be on extracting data about money “left on the table” by firms in initial public offerings (IPOs). These data are provided by Jay Ritter at the University of Florida in a PDF found here.\nIn this case, we will use pdf_text() from the pdftools package. From the help for pdf_text(), we learn that “the pdf_text function renders all text boxes on a text canvas and returns a character vector of equal length to the number of pages in the PDF file.” We also learn that the function accepts an argument for the “pdf file path or raw vector with pdf data”. Since the above URL actually provides an absolute file path for the PDF, we can simply call pdf_text(url) to download the PDF from the url and convert it to a text representation in R.\n\nurl &lt;- str_c(\"https://site.warrington.ufl.edu/ritter/files/\", \n             \"money-left-on-the-table.pdf\")\n\nLooking at the PDF, we see that the first page is text and the table starts on page 2. We can omit the first page returned by pdf_text(url) below by appending [-1].",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>データを読み込む</span>"
    ]
  },
  {
    "objectID": "chap09_import.html#整形されていない形式からデータを抽出する",
    "href": "chap09_import.html#整形されていない形式からデータを抽出する",
    "title": "\n2  データを読み込む\n",
    "section": "\n2.2 整形されていない形式からデータを抽出する",
    "text": "2.2 整形されていない形式からデータを抽出する\n\n時には、データが固定幅のテキストファイルよりもさらに乱雑な形式で提供されることがある。  例えば、PDFからデータ（おそらく表形式のデータ）を抽出したい場合がある。  データは私たちの目には表形式に見えるかもしれないが、PDF形式はデータの構造についてほとんど情報を保持していない。PDF形式は、コンピュータやプリンタがテキストを人間が消費するために提示するための軽量な方法であり、統計ソフトウェアパッケージのためのものではない。\n\nこのセクションでは、PDFから表形式のデータを抽出するケーススタディを調査する。  最初の焦点は、新規株式公開（IPO）において企業が「テーブルに残した」金額に関するデータを抽出することである。  これらのデータは、フロリダ大学のJay Ritterによって提供されており、ここで見つけることができるPDFに記載されている。\n\nこの場合、pdftoolsパッケージのpdf_text()を使用する。  pdf_text()のヘルプから、pdf_text関数はテキストキャンバス上のすべてのテキストボックスをレンダリングし、PDFファイルのページ数と同じ長さの文字ベクトルを返す、ということがわかる。  また、関数は「PDFファイルのパスまたはPDFデータを含むrawベクトル」の引数を受け入れることがわかる。  上記のURLは実際にPDFの絶対ファイルパスを提供しているため、pdf_text(url)を呼び出すことで、R内でPDFをダウンロードし、テキスト表現に変換することができる。\n\nurl &lt;- str_c(\"\n  https://site.warrington.ufl.edu/ritter/files/\", \n  \"money-left-on-the-table.pdf\")\n\n\nPDFを見ると、最初のページはテキストであり、表は2ページ目から始まる。  以下のpdf_text(url)で返される最初のページを[-1]で省略することができる。\n\noutput &lt;- pdf_text(url)[-1]\n\n\npdf_text(url)またはpdf_text(url)[-1]の出力は、ここに表示するにはスペースが多すぎるが、直前のコードを実行してから、それぞれの代替案を呼び出すと、2番目の関数がかなり規則的に見えるデータを返すことがわかる。  この場合、readrのread_lines()を使ってテキストを実行することが理にかなっている。  こうして行い、結果をtempに保存して、ここでより詳しく調べることができる。9\n\ntemp &lt;- \n  output |&gt;\n  read_lines()\n\n\nいくつかの選択された行を見てみよう。  1行から5行は表の上部を表し、残念ながら列名が2行に分割されている。10\n\nprint_width &lt;- 70\nstr_sub(temp[1:5], 1, print_width)\n\nRather than trying to deal with the column-names-spread-over-two-rows issue with code, we can just manually specify the column names and skip the first two rows when we import the data. For now, we merely create variables to reflect those choices; we will use these variables later.\n\ncol_names &lt;-  c(\n  \"amount_left_on_table\", \"company\", \"ipo_date\", \"offer_price\", \"first_close_price\", \n  \"shares_offered\", \"ticker\")\nskip_rows &lt;- 2\n\nWhile the top portion of the table looks like it might be able to work with read_fwf(), once we look at the lines 56 through 62 at the bottom of the first page and top of the second pages (pp. 2–3 of the original PDF), we can see that they are not aligned.\n\nstr_sub(temp[56:62], 1, print_width)\n\nThe same is true of lines 114 through 120 at the bottom of the second page and top of the third pages (pp. 3–4 of the original PDF).\n\nstr_sub(temp[114:120], 1, print_width)\n\n\n最後に、461行から463行には、表に含まれていないフッターが含まれている。\n\nstr_sub(temp[458:464], 1, print_width)\n\n\n行の先頭にSource:があるかどうかを調べることで、これを示すことができる。  文字列の先頭に一致させるためには、正規表現を^でアンカーする必要がある。11  461行の1つ前までだけ読み込みたいので、最初にskip_rowsをスキップする。  したがって、読み込みたい最大行数を以下のように計算できる。\n\nmax_rows &lt;- str_which(temp, \"^Source:\") - 1 - skip_rows\n\nAlso, as we will use Tidyverse tools to munge the data, we will find it useful to put the data in a tibble, albeit one with just a single column, which we call temp.\n\nritter_data_raw &lt;-\n  output |&gt;\n  read_lines(skip = skip_rows, n_max = max_rows) |&gt;\n  tibble(temp = _)\n\nComing as it does from a PDF, there are some messy elements of the data.\nFirst, there are empty rows, which we can detect by matching on ^$ (i.e., the start and end with nothing in between) and we will want to filter out these rows.\n\nritter_data_raw |&gt; \n  filter(str_detect(temp, \"^$\")) \n\nSecond, there are rows with just spaces and page numbers, which we can match with:\n\nritter_data_raw |&gt; \n   filter(str_detect(temp, \"^\\\\s+\\\\d+$\")) \n\nSo we want to filter out these rows too:\n\nritter_data_raw &lt;- \n  output |&gt;\n    read_lines(skip = 2, n_max = max_rows) |&gt;\n    tibble(temp = _) |&gt;\n    filter(!str_detect(temp, \"^$\"), \n           !str_detect(temp, \"^\\\\s+\\\\d+$\"))\n\nThe next step will be, as before, to use extract() from tidyr and a regular expression to arrange the data into columns. However, the regular expression that we will need to use will be a bit more complicated than the one above. If it weren’t for the second column, we could use a function from the readr package such as read_delim() that is designed to read delimited text data, such as comma-separated values (CSVs), tab-separated values, or (most relevant here) values separated by spaces.\n\n不幸なことに、2番目の列（company）にはスペース（例：United Parcel Service*）が含まれているため、これは3つの列として読み込まれる（少なくともこの行に関しては）。12\nFortunately, company is the only column with embedded spaces and it is followed by a column (ipo_date) that is strictly six digits (it has the form yymmdd, where yy is the last two digits of the year, mm are the month digits, and dd represents the date of the month). So we can use this to effectively “delimit” the company column from the rest of the data.\n\n最初の列（amount_left_on_table）にはスペース以外の文字が含まれており、これは[^\\\\s]として表すことができる。  ここで、^は、その後に続く\\\\sを否定するために機能し、これは正規表現でスペースを一般的に表す方法である。  したがって、[^\\\\s]+は「1つ以上のスペース以外の文字」を示し、これを括弧で囲む（つまり、([^\\\\s]+)）ことで、一致する文字をキャプチャすることができる。  2番目の列にはほとんど何でも含まれる可能性がある（つまり、正規表現の用語で.である）、これに続く1つ以上のスペースを(.+)\\\\s+として表すことができる。  3番目の列（ipo_date）は、6桁の数字をキャプチャする正規表現でキャプチャできる（再び、キャプチャしない1つ以上のスペースに続く）：([0-9]{6})\\\\s+。  次の4つの列は最初の列と同じように、これらをキャプチャするために([^\\\\s]+)を使用し、最後の列以外は1つ以上のスペース（\\\\s+）に続く。  最初には明らかでない問題の1つは、いくつかの行がスペースで始まっているが、すべてがそうではないということである。  これらのスペースをキャプチャしたくはないが、それらが存在することを許可したいので、これを行うために^\\\\s*を使用して「行の先頭に続く0個以上のスペース」を表すことができる。\n\n\nregex &lt;- str_c(\"^\\\\s*\",          # Start string (perhaps followed by spaces)\n               \"([^\\\\s]+)\\\\s+\",  # Non-space characters (followed by spaces)\n               \"(.+)\\\\s+\",       # Any characters, which may include spaces \n                                 #  (followed by spaces)\n               \"([0-9]{6})\\\\s+\", # Six digits (followed by spaces)\n               \"([^\\\\s]+)\\\\s+\",  # Non-space characters (followed by spaces)\n               \"([^\\\\s]+)\\\\s+\",  # Non-space characters (followed by spaces)\n               \"([^\\\\s]+)\\\\s+\",  # Non-space characters (followed by spaces)\n               \"([^\\\\s]+)\",      # Non-space characters \n               \"$\")              # End of string\n\n\nこれをextract()を通して実行できる。\n\nritter_data_raw |&gt;\n    # Here we use the regular expression to split the data into columns\n    extract(temp, col_names, regex) \n\n\nもうすぐだ。  次に、amount_left_on_table、offer_price、first_close_price、shares_offeredのフィールドを数値に変換したい。  このタスクには、readrのparse_number()関数が最適である。  across()を使用して、複数の列（この場合、これらの4つの列のall_of()）に単一の関数parse_number()を適用し、デフォルトでは、これらの列で以前に見つかった値が置き換えられる。  また、ipo_dateを実際の日付に変換したいので、lubridateパッケージのymd()を使用してこれを行うことができる。  この段階で、結果をデータフレームritter_dataに保存する。\n\nritter_data &lt;-\n  ritter_data_raw |&gt;\n  # Here we use the regular expression to split the data into columns\n  extract(temp, col_names, regex) |&gt;\n  # Finally, fix up the data types of the columns\n  mutate(across(all_of(c(\"amount_left_on_table\", \"first_close_price\",\n                         \"offer_price\", \"shares_offered\")), \n                parse_number),\n         ipo_date = ymd(ipo_date),\n         company = str_trim(company)) \n\nritter_data\n\n\nこの段階で、NA値を持つ行があるかどうかを確認する必要がある。NA値を持つ行が存在する場合、解析の問題が考えられる。\n\nritter_data |&gt;\n  filter(if_any(.cols = everything(), .fns = is.na))\n\n\nなし！\n\n2.2.1 9.2.1 Exercises\n\nこのPDFで、Ritterは「*でマークされたIPOは国際トランシェも持っていた（おそらく他のものも）」と述べている。  この「*」が存在する場合にこれを削除し、国際トランシェを示す追加の列intl_trancheを作成するコードを書きなさい。  （ヒント：extract()関数をここで使用できる。）  最初の段階では、この関数を使用して、into = c(\"company\", \"intl_tranche\")とregex = \"^(.*?)(\\\\*?)$\"を使用するかもしれない。)  .*?の?が何をしているのか説明せよ。  ここに説明がある。  この?なしで正規表現は機能するか？\\\\*?は何に一致するか？）\n\nRitter defines money left on the table as “the difference between the closing price on the first day of trading and the offer price, multiplied by the number of shares sold.” Can you calculate this from the data provided? Is the calculated amount (amount) equal to the amount in amount_left_on_table in each case? What explains the differences? (Hints: There will be more than one reason. You may find it helpful to calculate ratio = amount / amount_left_on_table and to focus on differences of more than 1% with filter(abs(ratio - 1) &gt; 0.01).)\n\n\n次の正規表現はそれぞれ何にマッチするか？何をキャプチャするか？\n\n\n\"^\\\\s*\"\n\"(.+)\\\\s+\"\n\"([^\\\\s]+)\\\\s+\"\n\"([0-9]{1,2}/[0-9]{4})\\\\s+\"\n\"([0-9,]+)\\\\s+\"\n\n\n「The Customer Knows Best: The Investment Value of Consumer Opinions」のオンライン付録には、Amazon.comでの顧客レビューを持つ企業のリストを示すTable OA.1が含まれている。  上記のritter_dataと同様のアプローチを使用して、次のカラムを持つデータフレームhuang_dataを作成する。 \n\n\n\n\nmonthsとreviewsは数値であるべき。 \n\n\nstartとendは日付であるべき（月と年のみが指定されている場合は、月の最初の日を使用する）。 \n\n上記の部分的な正規表現に単一の正規表現を組み合わせることで、これを解決できる。 (この法法で、str_c()を使用してritter_dataの部分的な正規表現を組み合わせた方法と同様にすることができる。)\n\n\n次のコードを使用して、URLがSDCからの企業の合併と買収に関する観測値のサンプルを提供している場合、データの初回インポートを作成する。13  ma_sdcのデータを見るときに最初に見る問題は何か？  （ヒント：最初の5行を確認しなさい。）  この問題に対処するためにコードを適応させなさい。（ヒント：適切な設定を得るために、skip引数に異なる値を試してみる必要があるかもしれません。）\n\n\ncol_names &lt;- c(\"date_announced\", \"date_effective\", \"tgt_name\",\n               \"tgt_nation\", \"acq_name\", \"acq_nation\",\n               \"status\", \"pct_of_shares_acq\", \"pct_owned_after_transaction\",\n               \"acq_cusip\", \"tgt_cusip\", \"value_of_transaction_mil\",\n               \"acq_prior_mktval\", \"tgt_prior_mktval\",\n               \"acq_nation_code\", \"tgt_nation_code\")\n\nurl &lt;- str_c(\"https://gist.githubusercontent.com/iangow/\",\n             \"eb7dfe1cd0913821429bdf0566465d41/raw/\",\n             \"358d60a4429f5747abc61f8acc026d335fc165f3/sap_sample.txt\")\nma_sdc_file &lt;- tempfile()\ndownload.file(url, ma_sdc_file)\n\n\nma_sdc_cols &lt;- fwf_empty(ma_sdc_file, col_names = col_names)\nma_sdc &lt;- read_fwf(ma_sdc_file, col_positions = ma_sdc_cols)\n\nOpen the file found at url in your browser (browseURL(url) will help here) and locate the row containing the word Coffey. What do you see there as relates to the variable status? How does this compare with what you see in status if you filter using tgt_name == “Coffey International Ltd”)? What do you think has happened here? How can setting a value for n in fwf_empty() help here? (Hint: Using which(ma_sdc$status == “Unconditi”) might help here.)\nUsing an appropriate function from the lubridate package, fix the variables date_announced and date_effective so that they have type Date.\nWhat are the minimum and maximum values of date_announced and date_effective? What explains missing values (if any) here?\nWhat do you observe about acq_cusip and tgt_cusip? Can you write some code to check that these variables have been read in correctly? (Hint: The function str_length() might be useful here.)",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>データを読み込む</span>"
    ]
  },
  {
    "objectID": "chap09_import.html#further-reading",
    "href": "chap09_import.html#further-reading",
    "title": "\n2  データを読み込む\n",
    "section": "\n2.3 9.3 Further reading",
    "text": "2.3 9.3 Further reading\nChapter 7 of R for Data Science provides an introduction to importing data, including functions such as read_csv() and read_fwf(). Chapter 14 in R for Data Science covers strings and Chapter 15 covers regular expressions.\nThe topic of regular expressions is surprisingly deep and regular expressions are useful in more contexts than might be apparent when first learning about them. Friedl (2006) provides a deep treatment of the topic. Goyvaerts and Levithan (2009) is full of examples of solutions to common cases.",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>データを読み込む</span>"
    ]
  },
  {
    "objectID": "chap09_import.html#footnotes",
    "href": "chap09_import.html#footnotes",
    "title": "\n2  データを読み込む\n",
    "section": "",
    "text": "Some WRDS data sets have “incorrect” data types and additional work is needed to address these cases.↩︎\nRegular expressions are available in pretty much every package, include Python, R, PostgreSQL, and SAS. Learning on one platform largely carries over to any other. Stata’s support for regular expressions is much weaker than the other platforms’.↩︎\nAccording to https://siccode.com, “Standard Industrial Classification Codes (SIC Codes) identify the primary line of business of a company. It is the most widely used system by the US Government, public, and private organizations.”↩︎↩︎\nWhile Fama-French industry definitions might not change very often, we will see other benefits from a more robust and general approach below.↩︎\nWe justify this approach using the fact that these data sets are not large by any stretch and the assumption that, in general, only one of ff_ind, ff_ind_short_desc and ff_ind_desc actually gets used in practice.↩︎\nNote that an alternative to str_c() would be paste0() from base R; we use str_c() here because the stringr functions seem easier to learn as a group than their base R equivalents.↩︎\nThroughout this section, we will repeat code, but in practice, we would simply add additional lines as we work through the code. This is simply a way to represent the idea of working through the data interactively, something that the Tidyverse makes very easy.↩︎\nIt seems that omitting ^ and $ has no effect in this case.↩︎\nAgain, you may find it useful to inspect the full contents of temp yourself. We don’t do that here due to space constraints.↩︎\nWe only print the first 70 characters of each row to keep the output on the page.↩︎\nNote that in other contexts, such as inside [ and ], ^ will act as a kind of “not” operator.↩︎\nIf the values in company had been quoted, e.g., as “United Parcel Service*“, then reading the data as space-delimited would work. But these are “wild” data from a PDF and such niceties cannot be expected.↩︎\nWe messed with these data, so these cannot be used for research! But they are a realistic representation of an actual data set.↩︎",
    "crumbs": [
      "基礎",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>データを読み込む</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html#ベクトル",
    "href": "appA_LinearAlgebra.html#ベクトル",
    "title": "付録 A — 線形代数",
    "section": "",
    "text": "A.1.1 ベクトルの演算\nここで2つのベクトル x = (x_1, x_2, \\ldots, x_n) と y = (y_1, y_2, \\ldots, y_n) があるとする。\n\n同じ長さのベクトルは加算できる。\n\nx + y = (x_1 + y_1, x_2 + y_2, \\ldots, x_n + y_n)\n\nまた減算もできる。\n\nx - y = (x_1 - y_1, x_2 - y_2, \\ldots, x_n - y_n).\n\nベクトルは実数でスカラー倍することもできる。\n\n\\lambda y = (\\lambda y_1, \\lambda y_2, \\ldots, \\lambda y_n).\n\n\n定義 A.1 (内積) 2つの n 次元ベクトル x と y の内積は、x \\cdot y で表され、次のように定義される。\n\nx \\cdot y = x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n = \\sum_{i=1}^{n} x_i y_i.",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html#行列",
    "href": "appA_LinearAlgebra.html#行列",
    "title": "付録 A — 線形代数",
    "section": "A.2 行列",
    "text": "A.2 行列\n\n行列は実数の長方形の配列である。\n\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1k} \\\\\na_{21} & a_{22} & \\cdots & a_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mk}\n\\end{bmatrix}\n\n\n行列は通常大文字で表記され（例：\\boldsymbol{A}），行列の一般的な要素は a_{ij} と表記される。  行列は，一般的な要素とその次元を用いて [a_{ij}]_{m \\times k} と表現することもできる。\n2つの重要な行列は，ゼロだけで構成されるゼロ行列(null matrix) \\boldsymbol{0} と対角要素が1 (i_{kk} = 1, \\forall k) で、非対角要素がゼロ (i_{jk} = 0, \\text{for} \\forall j \\not = k) あるサイズ n の単位行列(identity matrix) \\boldsymbol{I}_n (あるいは単に \\boldsymbol{I} )である  \\boldsymbol{I} と \\boldsymbol{A} が乗算可能である場合（たとえば，両方とも n \\times n の正方行列である場合），$ = $ および $ = $ となる。したがって，単位行列（いくつかの点では，\\boldsymbol{I} は数値 1 の行列版である）という用語がある。\n\n行列の各行または列はベクトルとして考えることができるため、m \\times k 行列は m \\ k-ベクトル（行）または k \\ m -ベクトル（列）として見ることができる。\n\nA.2.1 行列の演算\n2つの行列 \\boldsymbol{A} = [a_{ij}]_{m \\times k} と \\boldsymbol{B} = [b_{ij}]_{m \\times k} があるとする。 そして、これらの行列を足すことができる。\n\n\\boldsymbol{A} + \\boldsymbol{B} = [a_{ij} + b_{ij}]_{m \\times k}\n\n実数 $$を行列に掛けることもできる。\n\n\\lambda \\boldsymbol{A} = [\\lambda a_{ij}]_{m \\times k}\n\n\n行列の積(matrix multiplication)は，第1行列の列数が第2行列の行数と等しい場合に定義される。\n行列 \\boldsymbol{A} = [a_{ij}]_{m \\times l} と \\boldsymbol{B} = [b_{jk}]_{l \\times n} があるとして、要素 c_{ik} をもつ m \\times n 行列 \\boldsymbol{A} \\boldsymbol{B} は次のように定義される。\n\n\\boldsymbol{A} \\boldsymbol{B} = \\left [\n    c_{ik} := \\sum_{j=1}^{l} a_{ij} b_{jk} \\right ]_{m \\times n}\n\n\nあるいは c_{ik} = a_i \\cdot b_k とも書ける。ここで，a_i は \\boldsymbol{A} の i 番目の行であり，b_k は \\boldsymbol{B} の k 番目の列である。  \\boldsymbol{A} と \\boldsymbol{B} の乗算には，乗算可能であることが必要である。  特に，\\boldsymbol{A} \\boldsymbol{B} が存在するためには \\boldsymbol{A} の列数は \\boldsymbol{B} の行数と等しい必要がある。  \\boldsymbol{A} の行数が \\boldsymbol{B} の列数と等しくない場合，\\boldsymbol{B} \\boldsymbol{A} は存在しない（\\boldsymbol{A} \\boldsymbol{B} と等しくなることはない）。\n\n定義 A.2 (転置) 行列 \\boldsymbol{B} = \\left[ b_{ij} \\right]_{n \\times m} は，行列 \\boldsymbol{A} = \\left[ a_{ij} \\right]_{m \\times n} の転置(transpose)と呼ばれ（\\boldsymbol{A}^{\\mathsf{T}} と表記される），すべての i \\in \\{1, 2, \\dots, m\\} およびすべての j \\in \\{1, 2, \\dots, n\\} に対して b_{ij} = a_{ji} が成り立つ。\n\n定義 A.3 (正方行列) 行数と列数が同じである行列を正方行列と呼ぶ。\n\n定義 A.4 (対称) 正方行列は，すべての i, j に対して a_{ij} = a_{ji} である場合に対称である。明らかに，\\boldsymbol{A} が対称行列である場合，\\boldsymbol{A} = \\boldsymbol{A}^{\\mathsf{T}} である。\n\n定義 A.5 (行列の逆) m \\times m の正方行列 \\boldsymbol{A} は，\\boldsymbol{A}^{-1} と表記される行列が存在し，\\boldsymbol{A}^{-1} \\boldsymbol{A} = I_m および \\boldsymbol{A} \\boldsymbol{A}^{-1} = I_m が成り立つ場合，逆行列を持つという。ここで，I_m は m \\times m の単位行列を表す。逆行列を持つ行列は，逆行列可能または非特異と言われる。\n\n逆行列の性質\n\n\n\\boldsymbol{A} の逆行列が存在する場合，それは一意である。 \n\\alpha \\neq 0 かつ \\boldsymbol{A} が逆行列可能である場合，(\\alpha \\boldsymbol{A})^{-1} = 1/\\alpha \\boldsymbol{A}^{-1}。 \n\\boldsymbol{A} と \\boldsymbol{B} がともに逆行列可能な m \\times m 行列である場合，(\\boldsymbol{A}\\boldsymbol{B})^{-1} = \\boldsymbol{B}^{-1} \\boldsymbol{A}^{-1}。\n\n\nここでは，転置に関するいくつかの有用な結果を示す。まず，2つの正方行列 \\boldsymbol{A} と \\boldsymbol{B} が逆行列可能である場合，(\\boldsymbol{A}\\boldsymbol{B})^{\\mathsf{T}} = \\boldsymbol{B}^{\\mathsf{T}} \\boldsymbol{A}^{\\mathsf{T}} が成り立つ。\n\n\\begin{aligned}\n(\\boldsymbol{A}\\boldsymbol{B})^{\\mathsf{T}} &= [ab_{ij}]^{\\mathsf{T}} \\\\\n&= \\left[ ab_{ji} \\right] \\\\\n&= \\left[ \\sum_{k=1}^n a_{jk} b_{ki} \\right] \\\\\n&= \\left[ \\sum_{k=1}^n (\\boldsymbol{B}^{\\mathsf{T}})_{ik}  (\\boldsymbol{A}^{\\mathsf{T}})_{kj} \\right] \\\\\n&= \\boldsymbol{B}^{\\mathsf{T}} \\boldsymbol{A}^{\\mathsf{T}}\n\\end{aligned}\n\n\n次に，正方行列 \\boldsymbol{A} が逆行列可能である場合，\\left(\\boldsymbol{A}^{\\mathsf{T}}\\right)^{-1} = \\left(\\boldsymbol{A}^{-1}\\right)^{\\mathsf{T}} が成り立つ。\n\n\\begin{aligned}\n\\boldsymbol{A} \\boldsymbol{A}^{-1}  &= \\boldsymbol{I} \\\\\n\\left(\\boldsymbol{A}^{-1}\\right)^{\\mathsf{T}} \\boldsymbol{A}^{\\mathsf{T}} &= \\boldsymbol{I} \\\\\n\\left(\\boldsymbol{A}^{-1}\\right)^{\\mathsf{T}} \\boldsymbol{A}^{\\mathsf{T}} \\left(\\boldsymbol{A}^{\\mathsf{T}}\\right)^{-1} &= \\left(\\boldsymbol{A}^{\\mathsf{T}}\\right)^{-1} \\\\\n\\left(\\boldsymbol{A}^{-1}\\right)^{\\mathsf{T}} &= \\left(\\boldsymbol{A}^{\\mathsf{T}}\\right)^{-1}\n\\end{aligned}\n\n\n定義 A.6 (対角行列) 正方行列 \\boldsymbol{A} が，a_{ij} = 0, \\forall i \\neq j である場合，対角行列である。言い換えれば，対角行列のすべての非対角要素はゼロである。\n\n定義 A.7 (線形独立) \\{x_1, x_2, \\dots, x_r\\} が n \\times 1 ベクトルの集合であるとする。これらのベクトルが線形独立であるとは，次の条件が成り立つ場合に限る。つまり\n\n\\alpha_1 x_1 + \\alpha_2 x_2 + \\dots + \\alpha_r x_r = 0 \\tag{A.1}\n\n\nとなる場合，\\alpha_1 = \\alpha_2 = \\dots = \\alpha_r = 0 である。式 (A.1) がすべてゼロでないスカラーの集合に対して成り立つ場合，\\{x_1, x_2, \\dots, x_r\\} は線形従属(linearly dependent)である。\n\n定義 A.8 (行列のランク) \\boldsymbol{A} を m \\times k 行列とする。行列 \\boldsymbol{A} のランクは，\\boldsymbol{A} の線形独立な列の最大数である。\\boldsymbol{A} が m \\times k であり，\\boldsymbol{A} のランクが k である場合，\\boldsymbol{A} は完全列ランクを持つ。\\boldsymbol{A} が m \\times k であり，m \\geq k の場合，そのランクは最大で k である。\n\nランクのいくつかの性質は次の通りである。\n\n\n\\boldsymbol{A} のランクは \\boldsymbol{A}^{\\mathsf{T}} のランクと等しい。 \n\\boldsymbol{A} がランク k の k \\times k 正方行列である場合，それは非特異である。\n\n\n定義 A.9 (恒等行列) 行列 \\boldsymbol{A} が \\boldsymbol{A} \\boldsymbol{A} = \\boldsymbol{A} という性質を持つ場合，恒等行列(idempotent)である。\n\n定義 A.10 (射影行列) 行列 \\boldsymbol{X} が与えられた場合，\\boldsymbol{X} の射影行列は \\boldsymbol{P_X} と表記され，次のように定義される。\n\n\\boldsymbol{P_X} = \\boldsymbol{X}(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\mathsf{T}}\n\n\n次の式は，\\boldsymbol{P_X} が恒等行列であることを示している。\n\n\\boldsymbol{P_X} \\boldsymbol{P_X} = \\boldsymbol{X}(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\mathsf{T}} = \\boldsymbol{X}(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\mathsf{T}} = \\boldsymbol{P_X}\n\n\nまた，\\boldsymbol{P_X} が対称であることに注意する。つまり，\\boldsymbol{P_X} とその転置が等しいことを示す。\n\n\\begin{aligned}\n\\boldsymbol{P_X}^{\\mathsf{T}} &= \\left(\\boldsymbol{X}(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\mathsf{T}}\\right)^{\\mathsf{T}} \\\\\n  &= \\boldsymbol{X} \\left((\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{-1}\\right)^{\\mathsf{T}} \\boldsymbol{X}^{\\mathsf{T}} \\\\\n  &= \\boldsymbol{X} \\left((\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{\\mathsf{T}}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\\\\n  &= \\boldsymbol{X} (\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X})^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\\\\n  &= \\boldsymbol{P_X}\n\\end{aligned}\n\n\nこの分析では，上で議論した転置に関する2つの結果を使用した。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html#a.3-the-ols-estimator",
    "href": "appA_LinearAlgebra.html#a.3-the-ols-estimator",
    "title": "付録 A — 線形代数",
    "section": "A.3 A.3 The OLS estimator",
    "text": "A.3 A.3 The OLS estimator\nThe classical linear regression model assumes that the data-generating process has y = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} where \\boldsymbol{\\varepsilon} \\sim IID(0, \\sigma^2 \\boldsymbol{I}), where y and \\boldsymbol{\\varepsilon} are n-vectors, \\boldsymbol{X} is an n \\times k matrix (including the constant term), \\boldsymbol{\\beta} is a k-vector, and \\boldsymbol{I} is the n \\times n identity matrix.\nAs discussed in Chapter 3, the ordinary least-squares (OLS) estimator is given by:\n\n\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} y\n\nHere we can see that we can only calculate \\hat{\\boldsymbol{\\beta}} if \\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X} is invertible, which requires that it be of rank k. This requires that no one column of \\boldsymbol{X} is a linear combination of the other columns of \\boldsymbol{X}.\nAssuming \\mathbb{E}[\\boldsymbol{\\varepsilon} | \\boldsymbol{X}] = 0, we can derive the following result:\n\n\\begin{aligned}\n\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}} \\right] &= \\mathbb{E}\\left[\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}} | \\boldsymbol{X} \\right] \\right] \\\\\n&= \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} (\\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) | \\boldsymbol{X} \\right] \\\\\n&= \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\boldsymbol{\\beta}  | \\boldsymbol{X} \\right]  + \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{\\varepsilon} | \\boldsymbol{X} \\right] \\\\\n&= \\boldsymbol{\\beta} + \\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\mathbb{E}\\left[ \\boldsymbol{\\varepsilon} | \\boldsymbol{X} \\right] \\\\\n&= \\boldsymbol{\\beta}\n\\end{aligned}\n\n\nこれは，これらの仮定のもとで \\hat{\\boldsymbol{\\beta}} が不偏であることを示している。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html#a.4-further-reading",
    "href": "appA_LinearAlgebra.html#a.4-further-reading",
    "title": "付録 A — 線形代数",
    "section": "A.4 A.4 Further reading",
    "text": "A.4 A.4 Further reading\n\nこの付録は，行列と線形代数のほんの一部を取り上げているに過ぎない。  多くの計量経済学の教科書には，ここで提供した内容を超える線形代数の入門的なスケッチがある。  Davidson and MacKinnon (2004) の第1章や Wooldridge (2000) の付録 D は，ここで提供されている結果やそれ以上の内容をカバーしている。  Chiang (1984) や Simon and Blume (1994) などの数理経済学の標準的な入門書は，線形代数のイントロダクションを提供している。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html#a.3-ols推定量",
    "href": "appA_LinearAlgebra.html#a.3-ols推定量",
    "title": "付録 A — 線形代数",
    "section": "A.3 A.3 OLS推定量",
    "text": "A.3 A.3 OLS推定量\n\n古典的線形回帰モデルは，データ生成過程が y = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} であると仮定している。ここで \\boldsymbol{\\varepsilon} \\sim IID(0, \\sigma^2 \\boldsymbol{I}) であり，y と \\boldsymbol{\\varepsilon} は n-ベクトル，\\boldsymbol{X} は n \\times k 行列（定数項を含む），\\boldsymbol{\\beta} は k-ベクトル，\\boldsymbol{I} は n \\times n の単位行列である。\n\n第3章で議論したように，最小二乗（OLS）推定量は次のように与えられる。\n\n\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} y\n\n\nここで，\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X} が逆行列可能である場合にのみ \\hat{\\boldsymbol{\\beta}} を計算できることがわかる。これには，\\boldsymbol{X} の各列が他の列の線形結合でないことが必要である。\n\n\\mathbb{E}[\\boldsymbol{\\varepsilon} | \\boldsymbol{X}] = 0 と仮定すると，次の結果を導出できる。\n\n\\begin{aligned}\n\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}} \\right] &= \\mathbb{E}\\left[\\mathbb{E}\\left[\\hat{\\boldsymbol{\\beta}} | \\boldsymbol{X} \\right] \\right] \\\\\n&= \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} (\\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) | \\boldsymbol{X} \\right] \\\\\n&= \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\boldsymbol{\\beta}  | \\boldsymbol{X} \\right]  + \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{\\varepsilon} | \\boldsymbol{X} \\right] \\\\\n&= \\boldsymbol{\\beta} + \\left(\\boldsymbol{X}^{\\mathsf{T}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\mathsf{T}} \\mathbb{E}\\left[ \\boldsymbol{\\varepsilon} | \\boldsymbol{X} \\right] \\\\\n&= \\boldsymbol{\\beta}\n\\end{aligned}\n\n\nこれは，これらの仮定の下で \\hat{\\boldsymbol{\\beta}} が不偏であることを示している。ただし，\\mathbb{E}[\\boldsymbol{\\varepsilon} | \\boldsymbol{X}] = 0 という仮定は，いくつかの状況では強いものであることに注意する。  たとえば，Davidson and MacKinnon は「時系列データの文脈では，[この]仮定は非常に強いものであり，しばしば快適に行うことができない」と指摘している。  そのため，多くの教科書では，\\mathbb{E}[\\boldsymbol{\\varepsilon} | \\boldsymbol{X}] = 0 をより弱い仮定に置き換え，不偏性の代わりに一致性の漸近的性質に焦点を当てている。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html#a.4-さらなる読書",
    "href": "appA_LinearAlgebra.html#a.4-さらなる読書",
    "title": "付録 A — 線形代数",
    "section": "A.4 A.4 さらなる読書",
    "text": "A.4 A.4 さらなる読書\n\nこの付録は，行列と線形代数のほんの一部を取り上げているに過ぎない。  多くの計量経済学の教科書には，ここで提供した内容を超える線形代数の入門的なスケッチがある。  Davidson and MacKinnon (2004) の第1章や Wooldridge (2000) の付録 D は，ここで提供されている結果やそれ以上の内容をカバーしている。  Chiang (1984) や Simon and Blume (1994) などの数理経済学の標準的な入門書は，線形代数のイントロダクションを提供している。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "appA_LinearAlgebra.html#さらなる読書",
    "href": "appA_LinearAlgebra.html#さらなる読書",
    "title": "付録 A — 線形代数",
    "section": "A.4 さらなる読書",
    "text": "A.4 さらなる読書\n\nこの付録は，行列と線形代数のほんの一部を取り上げているに過ぎない。  多くの計量経済学の教科書には，ここで提供した内容を超える線形代数の入門的なスケッチがある。  Davidson and MacKinnon (2004) の第1章や Wooldridge (2000) の付録 D は，ここで提供されている結果やそれ以上の内容をカバーしている。  Chiang (1984) や Simon and Blume (1994) などの数理経済学の標準的な入門書は，線形代数のイントロダクションを提供している。",
    "crumbs": [
      "付録",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>線形代数</span>"
    ]
  },
  {
    "objectID": "chap13_Event.html",
    "href": "chap13_Event.html",
    "title": "\n6  イベント・スタディ\n",
    "section": "",
    "text": "6.1 概要\nこれまでの章では，金融市場が情報にどのように反応するかを調査した論文を学習した。  Chapter 10では，株式分割に関する情報への価格調整を研究したFama et al. (1969)を扱った。  Chapter 11では，Ball and Brown (1968)を取り上げ，1年間の市場リターンがその期間の収益ニュースと相関していることを示した。  Chapter 12では，Beaver (1968)を学習し，収益発表の周辺で取引量と価格の変動が増加することを示した。\nこれらの初期の論文以来の数十年間で，研究者が情報に対する市場の反応をどのように研究するかについては，大きな進化が見られ，本章では現在のイベント・スタディの方法について紹介する。\nMacKinlay (1997, p. 13)によると，イベント・スタディ(event study)は，「財務市場データを使用して…特定のイベントが企業の価値に与える影響を測定するものである。  このような研究の有用性は，非常に効率的な市場を仮定すると，イベントの影響はすぐに証券価格に反映されるという事実に由来する。  したがって，比較的短い期間に観察された証券価格を使用して，イベントの経済的影響を測定することができる。」ものとして定義している。\nMacKinlay (1997, p.14)では，「1960年代後半にRay BallとPhilip Brown（1968）およびEugene Fama et al. (1969)による画期的な研究が行われ，その方法論は基本的に現在使用されているものとほぼ同じである。  Ball and Brown (1968)は利益の情報内容を考慮し， Fama et al. (1969)は同時期の増配の効果を除去した後の株式分割の効果を分析している。」と続けている。\nイベント・スタディは，特定のイベントのクラスが経済的に興味のある1つ以上の変数に与える影響を調査する。  資本市場研究では，経済的に興味のある変数は，通常，イベントの周辺での企業の株式リターンである。\nイベント・スタディの基本的な要素は次のとおりである。\nさらに，第12章で見たように，観測をイベント時刻に整列させることは，ほとんどのイベント・スタディにおいて重要な特徴である。",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>イベント・スタディ</span>"
    ]
  },
  {
    "objectID": "chap13_Event.html#概要",
    "href": "chap13_Event.html#概要",
    "title": "\n6  イベント・スタディ\n",
    "section": "",
    "text": "イベントのクラス：収益発表、合併発表、株式分割、収益予測の変更 \n\n多くの研究論文では，これらのイベントは興味の対象となる処置を表している。 \n\n興味のある結果変数：会計方針、市場リターン、取引量 \n\nコントロール観測：イベントが発生しなかった観測 \n\nコントロール変数：イベント（たとえば、配当発表と収益発表、予測の修正と収益発表）とリターンの両方と相関する可能性のある追加変数（コントロール変数については、第4章を参照）。\n\n\n\n\n6.1.1 ディスカッション課題\n\nBall and Brown (1968)はMacKinlay (1997)のイベント・スタディの定義を満たしていますか？  どのような特徴があり，もしあれば何が欠けているか，\n\nBeaver (1968)はMacKinlay (1997)のイベント・スタディの定義を満たしていますか？  どのような特徴があり，もしあれば何が欠けているか，",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>イベント・スタディ</span>"
    ]
  },
  {
    "objectID": "chap13_Event.html#現代のイベントスタディ",
    "href": "chap13_Event.html#現代のイベントスタディ",
    "title": "\n6  イベント・スタディ\n",
    "section": "\n6.2 13.2 現代のイベント・スタディ",
    "text": "6.2 13.2 現代のイベント・スタディ\n\nイベント・スタディは時代とともに進化してきた。  現在のイベント・スタディは，一般的にMacKinlay (1997)の定義を満たしているが，様々な状況に適応されるにつれて，イベント・スタディのアプローチは変化してきた。\n\n変化の1つは，研究者が，企業固有の発表に対する市場の反応ではなく，規制の経済的影響を理解するためにイベント・スタディを使用することに興味を持つようになったことである。  Fama et al. (1969)は株式分割を，Beaver (1968)は収益発表を研究した。  各ケースでは，これらのイベントは，おおむね互いに独立している（たとえば，時間的に過度に集中していない）企業レベルのイベントである。  対照的に，以下で学習する3つの最近のイベント・スタディは，立法の可能性に影響を与えるイベント（Larcker et al., 2011; Zhang, 2007）や会計基準の施行（Khan et al., 2017）などの規制イベントを使用している。\n\n関連する変化は，市場効率性への依存度の増加であり，典型的な現代のイベント・スタディは，MacKinlay（1997）の言葉で言えば，「比較的短い期間に観察された証券価格」を「イベントの経済的影響の尺度」として使用している。  Fama et al. (1969)もBeaver (1968)も，株式分割や収益発表に市場が反応するように見えることを確立する際に，市場効率性に大きく依存しておらず，株式分割や収益発表が価値を創出（または破壊）するかどうかを示そうとしていない。  対照的に，現代のイベント・スタディは，しばしば規制を評価するために市場効率性に依存している。  たとえば，イベント・スタディを使用して「FASBの基準は株主価値を創出するか？」(Khan et al., 2017)と問うことは，市場がより強度の市場効率性によって暗示されるような情報を持っているという前提に大きく依存している。\n\nこの変化した強調の結果，研究者が取り組む独立した観測が少なくなることがしばしばある。  たとえば，Zhang (2007)の主要な分析は4つのイベント・ウィンドウに焦点を当てており，Beaver (1968)の506の収益発表よりもはるかに少ない。  後述するように，研究者はしばしばデータの相対的な不足を解消するために補助的な分析を行う。\n\n\n6.2.1 小規模なイベント・スタディ\n\n現代のイベント・スタディをよりよく理解するために，自分自身の小規模な研究を行う。  Appleの価値創造プロセスをよりよく理解したいとし，特にAppleの製品開発プロセスに焦点を当てることにしたとしよう。  執筆時点（2022年中頃）では，Appleは世界で最も価値のある企業であり，時価総額は2兆ドルを超えている。したがって，株主に価値を創出する方法を理解することは研究者にとって興味深いかもしれない。\n\nAppleは製品パイプラインについて非常に秘密主義的であるため，製品が発売されるメディアイベントは注目されることが多い。  たとえば，2007年1月9日に開催されたMacworld Conference & Expo San Francisco 2007では，AppleはiPhoneを発表し，これがAppleの主要な収益源となり，世界最大の携帯電話製品の1つとなった。  2010年1月27日に開催されたApple Special Eventでは，AppleはiPadを発表し，Appleのタブレットコンピュータとなった。\n\nしたがって，Appleの製品がAppleの株主に価値を創出するかどうかを理解するために，Appleのメディアイベントを興味の対象としてイベント・スタディを実施することができる。\n\nfarrパッケージには，Wikipediaで見つかったデータを元にしたデータフレームapple_eventsが含まれています。  この表の最後の数行を見てみましょう。\n\ntail(apple_events)\n\n\nイベント・スタディを実施するためには，CRSPからリターンデータが必要となる。  まず，CRSPでリターンを検索できるようにAppleのPERMNOを取得する必要がある。  AppleのティッカーがAAPLであることを知っていると便利である。\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres())\n\nstocknames &lt;- tbl(db, Id(schema = \"crsp\", table = \"stocknames\"))\ndsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"dsf\"))\ndsi &lt;- tbl(db, Id(schema = \"crsp\", table = \"dsi\"))\n\nidx_daily &lt;- tbl(db, Id(schema = \"comp\", table = \"idx_daily\"))\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nstocknames &lt;- load_parquet(db, schema = \"crsp\", table = \"stocknames\")\ndsf &lt;- load_parquet(db, schema = \"crsp\", table = \"dsf\")\ndsi &lt;- load_parquet(db, schema = \"crsp\", table = \"dsi\")\n\nidx_daily &lt;- load_parquet(db, schema = \"comp\", table = \"idx_daily\")\n\n\n\n\n\napple_permno &lt;-\n  stocknames |&gt;\n  filter(ticker == \"AAPL\") |&gt; # Apple社\n  select(permno) |&gt; # permno変数を選択\n  distinct() |&gt; # 重複を削除\n  pull() # データを取得\n\n\n次に，AppleのPERMNO（apple_permnoは14593）を使用して，CRSPからリターンデータを取得する。  この場合，Appleの日次リターン（ret）をcrsp.dsfから取得し，時価加重「市場」リターン（vwretd）をcrsp.dsiから取得し，市場調整リターンをret - vwretdとして計算する。  この場合，2005年初めからのすべてのリターンを取得し，apple_eventsのすべてのイベントをカバーする。\n\napple_rets &lt;-\n  dsf |&gt;\n  inner_join(dsi, by = \"date\") |&gt;\n  mutate(ret_mkt = ret - vwretd) |&gt;\n  select(permno, date, ret, ret_mkt, vol) |&gt;\n  filter(permno == apple_permno,\n         date &gt;= \"2005-01-01\") |&gt;\n  collect()\n\n\n第12章で学習した利益マネジメントとは異なり，Appleのメディアイベントは複数日にわたって行われるため，イベント・ウィンドウも複数日にわたって拡張する必要がある。  情報の漏洩を考慮して，メディアイベントの開始の前日からメディアイベントの終了の翌日までの1取引日をイベント・ウィンドウとして設定し，市場にメディアイベントの価値の意味を処理する時間を与える。  このために，farrパッケージのget_event_dates()関数を使用する。get_event_dates()は，裏でget_trading_dates()とget_annc_dates()を使用して，第12章で学習したtrading_datesやannc_datesなどのテーブルを取得する。\n\napple_event_dates &lt;-\n  apple_events |&gt;\n  mutate(permno = apple_permno) |&gt;\n  get_event_dates(db,\n                  end_event_date = \"end_event_date\",\n                  win_start = -1, win_end = +1)\n\ntail(apple_event_dates)\n\n\nAppleのリターンを時系列でグラフィカルに表現するために，メディアイベントに関する情報を含めてデータを整理する。\n\napple_data &lt;-\n  apple_rets |&gt;\n  left_join(apple_event_dates,\n            join_by(permno, date &gt;= start_date, date &lt;= end_date)) |&gt;\n  mutate(is_event = !is.na(start_date)) |&gt;\n  select(permno, date, ret, ret_mkt, vol, is_event)\n\n\n必要なデータが揃ったので，cumprod()関数を使用して累積リターンを計算し，これらのリターンを時系列でプロットすることができる。\n\napple_data |&gt;\n  arrange(date) |&gt;\n  mutate(cumret = cumprod(1 + coalesce(ret, 0)),\n         switch = coalesce(is_event != lead(is_event), FALSE)) |&gt;\n  ggplot(aes(x = date, y = cumret)) +\n  geom_line() +\n  geom_ribbon(aes(ymax = if_else(!is_event | switch, cumret, NA),\n                  ymin = 0, fill = \"Non-event\")) +\n  geom_ribbon(aes(ymax = if_else(is_event | switch, cumret, NA),\n                  ymin = 0, fill = \"Event\")) +\n  theme(legend.position = \"inside\", legend.position.inside = c(0.2, 0.8))\n\n\n図13.1の折れ線は，ウィンドウの開始以来の累積リターンを表している。  この折れ線には，2つの「エリア」プロットが追加されている：1つは非イベントウィンドウ用，もう1つはイベントウィンドウ用である。  ほとんどの日付は非イベント日であり，プロットではイベントウィンドウを識別するのが難しい。  しかし，「ズームイン」することで，イベントウィンドウを識別しやすくなる。図13.2は，2020年第4四半期に焦点を当てている。\n\napple_data |&gt;\n  arrange(date) |&gt;\n  mutate(cumret = cumprod(1 + coalesce(ret, 0)),\n         switch = coalesce(is_event != lead(is_event), FALSE)) |&gt;\n  filter(date &gt;= \"2020-09-01\", date &lt;= \"2020-12-31\") |&gt;\n  ggplot(aes(x = date, y = cumret)) +\n  geom_line() +\n  geom_ribbon(aes(ymax = if_else(!is_event | switch, cumret, NA),\n                  ymin = 0, fill = \"Non-event\")) +\n  geom_ribbon(aes(ymax = if_else(is_event | switch, cumret, NA),\n                  ymin = 0, fill = \"Event\")) +\n  theme(legend.position = \"bottom\")\n\n\n上記のプロットからは，Appleのメディアイベントが異常なリターンと関連していることを示すものはほとんどないが，より形式的にこれをテストするために回帰分析を使用する。  リターンがイベント指標変数がTRUEの場合に異なるかどうかを検討する。  12章で見たように、Beaver (1968)で使用された二乗リターン残差に類似した絶対リターン値と，相対取引量も考慮する。\n\nfms &lt;- list(\"ret_mkt\" = lm(ret_mkt ~ is_event, data = apple_data),\n            \"abs(ret)\" = lm(abs(ret) ~ is_event, data = apple_data),\n            \"Volume\" = lm(vol / mean(vol) ~ is_event, data = apple_data))\n\n\nfms &lt;- list(\"ret_mkt\" = lm(ret_mkt ~ is_event, data = apple_data),\n            \"abs(ret)\" = lm(abs(ret) ~ is_event, data = apple_data),\n            \"Volume\" = lm(vol / mean(vol) ~ is_event, data = apple_data))\n\n\nこれらの回帰の結果は表13.1に報告されている。  これらの結果は，比較的簡単に解釈すると，低いリターンが示されているが，取引量やリターンのボラティリティの高い（または低い）レベルは示されていない。\n\n上記のコードは，Appleのイベント期間中のリターンが非イベント期間中のリタンと異なるかどうかを調べている。  farrの別の関数であるget_event_cum_rets()は，2つのアプローチを使用して，イベントウィンドウ全体で市場調整リターンとサイズ調整リターンを使用して，累積生のリターンと累積異常リターンを計算する。  この関数を使用して，各Appleイベントの周辺での累積リターンを取得する。\n\nrets &lt;-\n  apple_events |&gt;\n  mutate(permno = apple_permno) |&gt;\n  get_event_cum_rets(db,\n                     win_start = -1, win_end = +1,\n                     end_event_date = \"end_event_date\")\n\n\nまず，平均的にほとんどゼロと変わらない市場調整リターンを見てみる。\n\nsummary(rets$ret_mkt)\n\n\n正のリターンイベントは何件あるか？（答え：半分以下！）\n\nsummary(rets$ret_mkt &gt; 0)\n\n\n最後に，図13.3は，イベント日によるAppleメディアイベントの市場調整リターンを描いている。\n\nrets |&gt;\n  ggplot(aes(x = event_date, y = ret_mkt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ 1\")\n\n\n\n6.2.2 演習\n\n上記のプロット作成時に，cumret = cumprod(1 + coalesce(ret, 0))の代わりにcumret = exp(cumsum(log(1 + coalesce(ret, 0))))を使用した場合，プロットがどのように変化すると予想されるか。  時間の経過とともに1つの計算を他の計算よりも好む理由はありますか。\n\nこの場合，cumret = cumprod(1 + ret)（つまり，coalesce()関数を削除）を使用した場合，異なる結果が得られますか？  もしそうならば，なぜか？  そうでない場合，常にこのようなケースであることを期待する理由はあるか（たとえば，Apple以外の株式の場合）？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>イベント・スタディ</span>"
    ]
  },
  {
    "objectID": "chap13_Event.html#イベントスタディと規制",
    "href": "chap13_Event.html#イベントスタディと規制",
    "title": "\n6  イベント・スタディ\n",
    "section": "\n6.3 13.3 イベント・スタディと規制",
    "text": "6.3 13.3 イベント・スタディと規制\n\nZhang (2007, p. 74)は，「関連する立法イベントに対する市場の反応を調査することで，サーバンズ・オクスリー法（SOX）の経済的影響を調査する」と述べている。  Zhang (2007, p. 75)は，「米国市場の累積時価加重（等時価加重）生のリターンは，主要なSOXイベントの周辺でそれぞれ-15.35%（-12.53%）に相当する」と結論付けている。  Zhang (2007)は米国市場のCRSPリターンを使用しているため，関連データのローカルコピーを収集する。\n\ndsi_local &lt;-\n  dsi |&gt;\n  select(date, vwretd, ewretd) |&gt;\n  collect()\n\n\nZhang (2007, p. 76)は，「主要なSOXイベント」（以下で定義）に焦点を当てたいくつかの分析を行い，「代替仕様において推定された米国の累積異常リターンは，すべて統計的に有意であり，-3.76%から-8.21%の範囲に及ぶ」と結論付けている。  ここでの「すべて」とは，時価加重リターンと等時価加重リターンのそれぞれについて，および2つのモデルに対する異常リターンを使用していることを意味する。  便宜上，米国に上場していないカナダの株式からなる市場を対象とした「市場モデル」に対する異常リターンを測定するモデルに焦点を当て，複数の非米国ポートフォリオのリターンを基準としてブレンドする第2のモデルの分析を省略する。  このために，2001年と2002年のCompustatの指数データ（comp.idx_daily）からトロント総合指数（gvkeyx == \"000193\"）のリターンデータを収集し，このデータセットをローカルコピーのcrsp.dsiとマージする。\n\ncan_rets &lt;-\n  idx_daily |&gt;\n  filter(gvkeyx == \"000193\") |&gt;\n  window_order(datadate) |&gt;\n  mutate(ret_can = if_else(lag(prccd) &gt; 0, prccd/lag(prccd) - 1, NA)) |&gt;\n  filter(datadate &gt;= \"2000-01-01\", datadate &lt;= \"2002-12-31\") |&gt;\n  rename(date = datadate) |&gt;\n  select(date, ret_can) |&gt;\n  collect()\n\n\nZhang (2007, p. 88)に従い，予想リターンモデルには「2001年12月28日の100日前の日次リターンデータ」を使用する。\n\nreg_data &lt;-\n  dsi_local |&gt;\n  inner_join(can_rets, by = \"date\") |&gt;\n  filter(date &lt; \"2001-12-28\") |&gt;\n  top_n(100, wt = date)\n\n\n次に，等時価加重ポートフォリオと時価加重ポートフォリオの市場リターンモデルをカナダのリターンに対して適合させる。\n\nfm_vw &lt;- lm(vwretd ~ ret_can, data = reg_data)\nfm_ew &lt;- lm(ewretd ~ ret_can, data = reg_data)\n\n\nつぎに、これらのモデルを使用してすべての観測値の超過リターンを計算する。  以下のコードでは、この本のいくつかの場所で使用するイディオムであるpick(everything())を使用している。  ネイティブパイプ（|&gt;）に続く関数では、しばしばパイププレースホルダ_を使用して、パイプの前に供給されたデータフレームにアクセスする。  ただし、このプレースホルダは、mutate()内の関数からはアクセスできません。  幸いなことに、代わりにpick()を使用できる。  pick()関数は、mutate()のような関数内でデータから列を簡単に選択する方法を提供する。  現在の設定では、everything()を使用して、ソースデータのすべての変数を選択することを示すことができる。  そのため、pick(everything())は、パイププレースホルダ_の制限を回避するための便利な回避策である。\n\ndsi_merged &lt;-\n  dsi_local |&gt;\n  inner_join(can_rets, by = \"date\") |&gt;\n  mutate(abret_vw = vwretd - predict(fm_vw, pick(everything())),\n         abret_ew = ewretd - predict(fm_ew, pick(everything()))) |&gt;\n  select(-ret_can)\n\n\n表2から、Zhang (2007)はリターンの日次標準偏差を約1.2%として計算しているようだ。  この計算の正確な根拠は不明だが，類似した分析は「2001年12月28日の100日前の日次リターンデータを使用して推定される」(Zhang, 2007, p.88).  したがって，以下の計算を使用してこの基準で日次ボラティリティを計算し，1.28%という値を得る。\n\nsd_ret &lt;-\n  dsi_local |&gt;\n  filter(date &lt; \"2001-12-28\") |&gt;\n  top_n(100, wt = date) |&gt;\n  summarize(sd(vwretd)) |&gt;\n  pull()\n\n\nfarrパッケージには，Zhang (2007)の表2にあるイベント・ウィンドウの日付が含まれているデータフレームzhang_2007_windowsが含まれている。  これらのデータをdsi_localからのリターンデータと組み合わせて，各イベント・ウィンドウの累積リターンを計算することができる。  Zhang (2007)に従い，各イベントについてt統計量を計算するために，各ウィンドウの取引日数の平方根で日次リターンのボラティリティをスケーリングして標準誤差を推定することができる。  異常リターンモデルの日次ボラティリティを推定するために，残差の標準偏差を使用する。\n\nzhang_2007_rets &lt;-\n  zhang_2007_windows |&gt;\n  inner_join(dsi_merged, join_by(beg_date &lt;= date, end_date &gt;= date)) |&gt;\n  group_by(event) |&gt;\n  summarize(n_days = n(),\n            vwret = sum(vwretd),\n            ewret = sum(ewretd),\n            abret_vw = sum(abret_vw),\n            abret_ew = sum(abret_ew),\n            vw_t = vwret / (sqrt(n_days) * sd_ret),\n            ew_t = ewret / (sqrt(n_days) * sd_ret),\n            abret_vw_t = abret_vw / (sqrt(n_days) * sd(fm_vw$residuals)),\n            abret_ew_t = abret_ew / (sqrt(n_days) * sd(fm_ew$residuals)))\n\n\nその後の分析では，Zhang (2007)は「主要なSOXイベント」に焦点を当てているようであり，これらのイベントは「統計的に有意な」リターンを持つイベントのようであり，表1のパネルD（2007年，91-92ページ）に結果を報告している。  この手順の主要な要素を再現し，我々の結果はZhang (2007)で「CAR2」として報告された結果とおおよそ一致している。\n\nzhang_2007_res &lt;-\n  zhang_2007_rets |&gt;\n  filter(abs(vw_t) &gt; abs(qnorm(.05))) |&gt;\n  summarize(vwret = sum(vwret),\n            ewret = sum(ewret),\n            abret_vw = sum(abret_vw),\n            abret_ew = sum(abret_ew),\n            n_days = sum(n_days),\n            vw_t = vwret / (sqrt(n_days) * sd_ret),\n            ew_t = ewret / (sqrt(n_days) * sd_ret),\n            abret_vw_t = abret_vw / (sqrt(n_days) * sd(fm_vw$residuals)),\n            abret_ew_t = abret_ew / (sqrt(n_days) * sd(fm_ew$residuals)))\n\n\n「主要なSOXイベント」における累積生の時価加重リターンを推定すると、 -15.2\\% （$t $統計量 -3.18 ）となり、Zhang (2007)で報告された値 -15.35\\% （ t 統計量 -3.49 ）に非常に近い。  しかし、4つの「主要なSOXイベント」における累積異常時価加重リターンの推定値は -3.18\\% （t 統計量 -1.02 ）であり、Zhang (2007)で報告された値 -8.21\\% （t 統計量 -2.99 ）よりもゼロに近い。これは、表1のパネルDで報告された8つの値のうち、従来のレベル（両側検定で5%）で統計的に有意な唯一の値である。\n\n\n6.3.1 13.3.1 ディスカッション課題\n\n\n6.3.1.1 Zhang (2007)\n\n\n米国企業の市場価値に対するSOXの影響を評価する際に，生のリターンと異常リターンの相対的なメリットは何か？  Zhang (2007)の表2のパネルBの焦点となる4つのイベントについて，カナダ，ヨーロッパ，アジアの生のリターンについて何が観察されるか？  これは，Zhang (2007)の結果について懸念を抱かせるか？\n\n\n表2のパネルDに報告されている検定統計量を構築するプロセスを説明せよ。  これらの結果はどれほど説得力があるか？Leuz (2007, p. 150)によると，Zhang (2007)は「イベントリターンの有意性を評価する際に非常に慎重である」と評価しているが，これに同意するか？\n\n\nZhang (2007)の設定でランダム化推論を使用して統計推論をどのように実施するかを詳細に説明せよ（このアプローチについては，19.7節を参照）。  このアプローチを適用する際に直面する課題と設計上の選択肢は何か？このアプローチは，Zhang (2007)で使用されているブートストラップアプローチと異なるか？\n\n\nLeuz (2007)は，SOXが企業にとって有益であるという証拠を見つけたZhang (2007)以外の研究を特定しているか？  これらの結果をどのように調整できるか？  2つの論文の相反する主張を評価するために，どのような手順を踏むか？\n\n\n6.3.1.2 Khan et al. (2017)\n\n\nKhan et al. (2017)で検討されている研究課題は何か？  （ヒント：タイトルを読んでください。）\n\n\nKhan et al. (2017, p. 210)は「会計基準の利点を評価するための理想的な研究デザインは，企業が特定の基準によって要求される情報を開示する任意の開示体制と，企業が同じ情報を開示することが義務付けられる強制的な開示体制を比較することである」と主張している。  この研究デザインがこの問題に対処するために「理想的」であると思うか？  この理想的なデザインで暗示される処置は何か？\n\n\n上記のAppleイベントスタディとKhan et al. (2017)を比較して，2つの研究の相対的な強みと弱みは何か？  「Apple製品は価値を追加するか？」という問題に対処するために，イベントスタディアプローチは適切だと考えるか？  Khan et al. (2017)の研究問題に対処するために，イベントスタディアプローチは適切だと考えるか？  その理由は何か？\n\n\n会計基準制定者は，「推定リスクの低減」を会計基準の目標と見なすと思うか？  改善された基準と推定リスクの低減を結びつける議論の質を評価せよ。  Panel Aの帰無仮説は，影響を受けた企業のCARが影響を受けていない企業のCARと異ならないというものである。  「最も負の」CAR差と「最も正の」CAR差のみを報告することは適切か？  （ヒント：帰無仮説が真である場合，「統計的に有意な」係数を持つ基準は何個あると予想できるか？）\n\n\nKhan et al. (2017)の表5のパネルBの結果を解釈せよ。\n\n\n6.3.1.3 Larcker et al. (2011) “LOT”\n\n\nLOTとFFJRは，研究デザインにおける市場効率の役割についてどのように異なるか？\n\n\nLOTの表1を考えてみよ。LOTのイベントスタディデザインとFFJRのデザインの違いは何か？  これらの違いの意味は何か？\n\n\nTable 1はどのように作成されたと考えるか？  Table 1の基礎となるプロセスに潜在的な問題があると思うか？  Table 1を作成するための代替アプローチを提案できるか？\n\n\nプロキシアクセスを考えてみよ，論文の中心的な結果のいくつかはプロキシアクセスに関連している。  もし会社の株主であるとしたら，プロキシアクセスについてどのような懸念があるか？  なぜこれがあなたの株式の価値を減少させる可能性があるか？  具体的な事例について考えてみよ；価値が低下する状況について具体的に述べよ。  前の質問で特定した問題に企業がどのようにさらされているかを測定するために，変数NLargeBlockとNSmallCoalitionsはどの程度適しているか？  （これに関連して，可能な価値低下の結果のタイミングと変数測定のタイミングを考慮してください。）\n\n\nLOTはいくつかのモンテカルロシミュレーションを使用している。これらは，Zhang (2007)によるブートストラップ解析とどのように比較されるか？  シミュレーションは，Zhang (2007)のブートストラップアプローチと同じ基本的な問題に対処しているか？",
    "crumbs": [
      "資本市場研究",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>イベント・スタディ</span>"
    ]
  },
  {
    "objectID": "chap17_Natural.html",
    "href": "chap17_Natural.html",
    "title": "\n9  自然実験\n",
    "section": "",
    "text": "9.1 ランダム化実験\n本パートでは、データから因果関係を推論する際に関連するいくつかの問題について探求する。  このトピックに関する多くの論文は、操作変数や回帰不連続デザインなどの方法に直接取り組む。\nここでは、ランダム化実験のベンチマーク設定から始め、統計的方法が（設定の特徴に応じて）信頼性のある因果関係を支持する可能性がある設定に段階的に移行することを意識的に選択している。  架空のランダム化実験と推測される因果関係のメカニズムについて慎重に考えることは、統計分析を問題の因果関係に関する問題に適用するために必要な思考を研ぎ澄ますのに役立つと考えている。\nこの章では、ランダム化比較試験の概念とそれに関連する自然実験の概念を紹介する。  多くの議論が因果関係の推論におけるランダム割り当ての価値に焦点を当てている一方で、実験の他の特徴も重要であることを説明する。  認識と開示の問題を探求し、研究問題のフレーミング自体がどんな実験にも重要な影響を与えることを示す。\nランダム化実験は、研究デザインの中で「最も信頼性の高い方法」として広く認識されている。  ランダム化実験のアイデアは非常にシンプルである：観測値はランダムに処置条件に割り当てられる。  二値の処置変数の場合（つまり、単位が処置されるかどうか）、単位が処置されるかどうかは、コイントスなどのランダム化メカニズムによって決定される。  因果関係ダイアグラムにおいて、Xが処置変数である場合、このようなランダム化は、Xに矢印がないことを意味し、因果効果を推定する際に制御する必要がある交絡要因がないことを意味する。\n具体的な例として、Jackson et al. (2009)を考えてみよう。  タイトル（「企業の減価償却方法選択の経済的影響」）から、処置の対象が企業の減価償却方法の選択であることが推測される。  Jackson et al. (2009)が焦点を当てる具体的な方法選択は、加速減価償却と直線減価償却であり、興味のある結果（「経済的結果」）は資本投資である。\n実際には、企業は、関連する資産の経済的減価償却、資産の有用寿命（寿命の短い資産の場合、選択は重要ではない可能性がある）、およびさまざまな財務報告のインセンティブ（例：成長する企業は、直線減価償却を使用すると、近い将来に収益が高く報告される）など、いくつかの要因に基づいて、加速減価償却と直線減価償却の選択を行う可能性が高い。  これらの推測される減価償却方法の選択に影響を与える要因が資本投資にも影響を与える場合、Jackson et al. (2009)で求められるような因果推論を混乱させる可能性がある。\nしかし、何らかの方法で企業を加速減価償却条件または直線減価償却条件にランダムに割り当てる能力があれば、割り当てられた処置条件に基づいて企業の資本投資を比較することで、因果関係を推論することができる。\nこのような実験を行う際には、明らかにいくつかの課題がある。  まず、割り当てられた減価償却方法を受け入れるよう企業に強制するための十分な権限を持つ規制当局である必要があるだろう。  そのような権限を持っていても、減価償却方法の選択の影響を理解する強い欲求がない場合、なぜ規制当局がそのような実験を行うかは明確ではない。そして、通常、規制当局はより重要な問題に取り組んでいる。  一部の実験は、単一の企業の協力だけで済むかもしれないが、この研究問題には当てはまらない（例えば、独立した投資権限を持つ多くの部門を持つ企業が必要となる）。\n実際には、実験に最も近いのは、研究者のコントロール外の要因に依存して、処置条件への割り当てをランダムまたはDunning (2012)の用語で「あたかもランダム」で行う自然実験と呼ばれるものである。\nこの章では、まず、ランダム化実験のベンチマーク形式であるランダム化比較試験を研究する。  次に、ランダム化比較試験を他の実験形式、自然実験を含む、と比較する。  ランダム化実験に関する議論を具体的にするために、実務家と研究者の両方にとって興味深い環境である認識と開示に焦点を当てる。  この環境を念頭に置いて、Michels (2017)で使用されている可能性のある自然実験に移り、この環境がどのような信頼性のある推論を支持するかを考える。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>自然実験</span>"
    ]
  },
  {
    "objectID": "chap17_Natural.html#ランダム化実験",
    "href": "chap17_Natural.html#ランダム化実験",
    "title": "\n9  自然実験\n",
    "section": "",
    "text": "9.1.1 ベンチマークとしてのランダム化比較試行\n\nランダム化実験は、医学においてランダム化比較試行またはRCTとして実施されることが多い。  Akobeng (2005, p.837)は、RCTを「参加者が2つ以上の臨床介入のうちの1つにランダムに割り当てられる研究の一種」と説明している。  RCTは、利用可能な仮説検定の中で最も科学的に厳密な方法であり、介入の効果を評価するためのゴールドスタンダードの試験と見なされている。\n\n理想的な形では、RCTは単に処置へのランダム割り当てを含む以上のものである。  Akobeng (2005)に基づいて、理想的なRCTの特徴として以下を挙げることができる。\n\n\n比較する処置が事前に指定されている。 \n\n興味のある結果が事前に指定されている。 \n\n提案された分析が事前に指定されている。 \n\n必要な参加者数が事前に特定されている（パワー計算を使用）。 \n\n必要な倫理承認が得られた後に参加者が募集される。 \n\n参加者は処置群にランダムに割り当てられる（コントロールを処置の1つと考えることができる）。  参加者はどの処置群に割り当てられたかを知らない（隠蔽）。  処置を行う者（例：医師、看護師、研究者）は、参加者がどの処置群に割り当てられたかを知らない（ブラインディング）。\n\nしたがって、ランダム割当処置は、RCTのいくつかの構成要素の1つに過ぎない。  候補となる自然実験が処置へのあたかもランダムな割り当てを提供すると主張できたとしても、自然実験にはRCTの他の特徴が欠けている。\n\n第1に、処置は研究者によって指定されるのではなく、企業、規制当局、または「自然」によって指定される。ここで「自然」とは、自然だけでなく、研究者によって十分に理解されていないかもしれない複雑な経済的な力を含む広い解釈を意味する。  （自然実験の文脈で用いられる広い解釈を強調するために、「自然」という言葉を引用符で囲むことがよくある。）  これは、自然実験を「発見」する研究者は、自然が割り当てる処置が研究者にとって興味深いものであることを期待するか、他の研究者にこれらの処置が興味深いものであると説得する必要があることを意味する。\n\n第2に、これにより、一般的には、「実験」が実施される前に結果と分析手順を事前に指定することは意味がない。  多くの場合、自然実験は実験が実施された後に特定される。  その結果、p-hackingを防ぐことが難しくなる。p-hackingとは、研究者が意識的であろうと無意識であろうと、さまざまな結果と分析手順を検討し、最終的に「統計的に有意な」結果を得るまでのプロセスを指す。  自然実験のこの制限については、第19章で詳しく説明する。\n\n第3に、一般的に、自然実験は処置割り当ての隠蔽を提供しない。[^1]\n\nただし、隠蔽がビジネス研究の設定で常に意味があるわけではないことに注意する。  RCTは、参加者が意識的な行動なしに作用すると予想される薬剤やその他の医療介入を評価するためによく使用される。  実際、研究者が他の行動によって交絡されていない処置の直接的な効果に最も興味を持つことが多いため、コントロール群にはプラセボ処置が含まれる。  しかし、興味のある処置が会計基準、インセンティブ報酬契約の構成要素、または資金調達制約である場合、直接的で無意識の効果について話すことは意味がない：マネージャーが自分の報酬契約の特徴に気づいていない場合、それに対応して行動を起こすことはない。  一般的に、研究者は総処置効果に興味を持っている可能性が高い。\n\nランダム割り当ての事実の隠蔽は、シグナリングを含む設定で特に重要かもしれない。なぜなら、シグナルの受信者の反応は、シグナルが送信者の状況に対する内生的な反応であることを前提としているからである。  シグナルが受信者にランダムに割り当てられたことが知られている場合、シグナリング価値はない。  Armstrong et al. (2022)で議論されているように、シグナリングは、任意開示、会計選択、配当政策、インサイダー株式購入、企業の社会的責任、監査を受ける決定などのパターンを説明するために使用されてきた。[^2]\n\n\n9.1.2 興味のある処置の特定\n\nランダム化比較試行を使用していても、適用される処置が最も興味を持っているものであることを確実にすることは難しいことがある。  たとえば、新型コロナウイルスの感染拡大を抑制するためのKN95マスクの効果を研究したいとしよう。  実験のために参加者を処置群と対照群に分けるだけで済むという単純な視点があるかもしれないが、実際には複雑さが存在する。\n\n処置群のアプローチの1つは、このグループのすべての参加者に研究期間中に使用するためのマスクを支給し、他の点では参加者が日常生活を送るようにすることである。  これは、「KN95マスクを着用する」がより興味を持って評価したい処置であるかもしれないにもかかわらず、「KN95マスクを着用する」ではなく「KN95マスクを受け取る」ということを意味している。\n\n「KN95マスクを着用する」を処置として興味がある場合、実際にマスクが着用されているかどうかを測定するメカニズムを実装するかもしれない。  これは、マスクが適切に着用されているかどうかを記録するためにモニターを送り出すことで達成できるかもしれないが、これにはかなりの追加費用がかかり、そのようなデータ記録が完全であるか、または実現可能であるかどうかは疑わしい（例：参加者を自宅や職場で観察する）。  別のアプローチは、参加者にマスク着用の行動を記録するように依頼し、参加者が正直で熱心に行うことを期待することである。\n\n行動の記録自体が行動に影響を与える可能性があるため（例：記録を作成することでマスク着用を思い出させることができる）、これは事実上処置を「KN95マスクを受け取り、マスク着用の記録を保持する」と変更し、広範な行動の記録が政策のレパートリーの一部と見なされない限り、興味を持たれないかもしれない。\n\n参加者を「KN95マスクを受け取る」かどうかにランダムに割り当てることしかできず、「KN95マスクを着用する」を処置として興味がある場合、考慮する必要がある問題は、参加者がマスクを着用する熱心さがランダムではないという現実である。  幸いなことに、適切なデータがあれば、「KN95マスクを着用する」効果を評価するために、参加者を「KN95マスクを受け取る」処置に割り当てる能力しかない場合でも、意図治療分析を実施することができる。  この分析は、第20章の焦点である因果推定技術を使用しており、このアプローチについてのさらなる議論は、Dunning (2012)の第4章と第5章に見られる。\n\nコントロールを単に「無処置」グループと考えることが誘惑されるかもしれないが、実際には、推定したい特定の処置効果に応じて処置を指定する際に重要な選択肢がある。  たとえば、マスクを着用することがウイルスの伝播を物理的に防ぐからではなく、マスクを着用することが行動に影響を与える可能性があるため、マスクを着用することが新型コロナウイルスの伝播に影響を与えるかもしれない。  マスクを着用することで、着用者が他者とのやり取りにより慎重になる可能性があるかもしれないし、会話がより不自然になることで、マスクは社会的相互作用を減らすかもしれないし、マスクを着用することで他者が着用者に避ける可能性がある（ただし、この効果はパンデミック前の時代の方がより妥当であったかもしれない）。  また、マスクは着用者（および他者）に安心感を与え、社会的相互作用を増加させる可能性がある。  また、上記で議論したように、「KN95マスクを受け取り、マスク着用の記録を保持する」が処置である場合、コントロールは「KN95マスクを受け取らず、マスク着用の記録を保持する」であると仮定すると、提供されない場合、一部の人々が自分自身のマスクを着用する可能性がある。[^3]\n\n新型コロナウイルスの伝播に対するKN95マスクの直接的な効果を推定することに興味がある場合、二重盲検プロトコル（つまり、隠蔽とブラインディングを備えたプロトコル）を使用し、コントロール処置を「効果の低いマスクを着用する」と指定することが望ましいかもしれない（この場合、推定しやすい効果は、効果の低いマスクよりもKN95マスクの効果である）または「完全に無効なマスクを着用する」と指定することが望ましいかもしれない（この場合、推定しやすい効果は、無効なマスクよりもKN95マスクの効果であり、これはある意味で「マスクなし」を代理するかもしれない）。[^4]\n\n\n9.1.3 興味のある結果の特定\n\nKN95マスクを用いた架空のRCTに関する議論を続けると、興味のある結果は明らかに思えるかもしれない。  おそらく、参加者を追跡し、記録された新型コロナウイルスの感染症の発生率を測定するだけで十分である。  我々はここでいくつかのことを暗黙的に前提としているが、それは、KN95マスクを着用しているかどうかに関係なく、新型コロナウイルスに感染するリスクが適度にあるということである。  研究期間中に新型コロナウイルスが撲滅された環境で参加者を対象とした研究を行うことは有益ではない可能性が高い。なぜなら、どちらの処置条件でも誰も新型コロナウイルスに感染しないからである。  また、症状が現れる期間が研究の期間内であることも重要である。  新型コロナウイルス感染症は数日以内に感染として現れるかもしれないが、症状がリスク要因にさらされてから数年後に現れる疾患では、事態ははるかに困難である。\n\nしかし、マスクが症例の発生率だけでなく、発生した症例の重症度にも影響を与える場合、新型コロナウイルスによる入院、重篤な健康問題、死亡などの追加の指標を追跡する必要があるかもしれない。\n\nさらに複雑なのは、興味のある結果が他者の疾病の発生率や重症度に対するマスク着用の効果である可能性があることである。  これを興味のある結果として採用すると、処置割り当てアプローチと追跡される指標に重大な変更が必要になる可能性がある。なぜなら、参加者に処置を割り当て、彼らが相互作用する人々の健康結果を追跡することは実用的ではないからである。  代わりに、結果は範囲が狭められる可能性が高い（例：一般的な疾病の伝播ではなく、オフィスや学校での疾病の伝播）し、処置割り当ては個人よりも高いレベル（例：学校やオフィス）で行われる可能性が高い。\n\n\n9.1.4 実験室実験\n\n上記で説明した架空の実験は、関連する文脈で実際の意思決定者が実際の意思決定を行うフィールド実験に類似している。  会計研究では、より一般的なのは実験室実験である。  典型的な実験室実験は、便宜的な参加者（例：大学生やオンライン調査参加者）が非常にスタイライズされた環境で「意思決定」を行うことを含む。\n\nRCTとの類似性は、純粋な実験室環境でKN95マスクや新型コロナウイルスワクチンを評価することがどれほど困難であるかを考えることで見ることができる。  確かに、実験室環境でマスクを支給したりワクチンを投与することはできるが、高度に制御された実験室環境は、参加者が新型コロナウイルスに遭遇することが期待されない環境であり、マスクやワクチンなどの予防措置の効果を評価するための環境としては役に立たない。  さらに、新型コロナウイルスに潜在的にさらされる必要がある期間は数週間から数か月であり、おそらく実験室環境をさらに排除する。[^5]  結果として、ほとんどのRCTは、処置割り当てと投与後に参加者が通常の環境に入るフィールド実験要素を含む。\n\n実験室実験は、人間の意思決定の一般的な特徴を理解するために有用であるかもしれないが、大学生の反応から架空の会計政策変数や架空の投資決定に関する結論を導くことは、実世界のビジネスの意思決定についての結論には大きな飛躍である。  これは、会計研究者が検討するほとんどの研究問題に当てはまると言える。  これに一貫して、実験室実験を使用した研究の会計実証研究への影響は、トップの会計ジャーナルのほとんどの研究（Gow et al., 2016を参照）において非常に限られているように思われる。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>自然実験</span>"
    ]
  },
  {
    "objectID": "chap17_Natural.html#自然実験",
    "href": "chap17_Natural.html#自然実験",
    "title": "\n9  自然実験\n",
    "section": "\n9.2 自然実験",
    "text": "9.2 自然実験\n\n自然実験は、観察が自然（または研究者のコントロール外の他の力）によって処置群と対照群にランダムまたは「まるで」ランダムに割り当てられるときに発生する（Dunning, 2012）。  このような割り当てが（まるで）ランダムである場合、自然実験は因果推論の目的においてフィールド実験と同様に機能することができる。\n\nDunning (2012, p.3)は、「自然実験の魅力は、定義的な特徴を信じがたいほど満たす研究デザインに魅力的なラベルが適用される可能性があるため、概念的な拡張を引き起こすかもしれない」と主張している。  以下で議論するように、このような概念的な拡張は、会計研究では一般的であるように思われる。\n\n懸念される概念的な拡張の1つの側面は、まるでランダムな割り当ての主張である。  いくつかの場合、割り当てメカニズムの無知が、そのメカニズムの深い理解を必要とするランダム性の詳細な評価の代わりになるように見える。  コインが表か裏になるプロセスは神秘的であるが、コイントスのランダム性は、コインが公平である（つまり、表か裏が同じくらいの確率で出る）という深い理解に基づいている。\n\n上記のRCTに関する議論は、ランダム化比較実験やフィールド実験の実施方法に関する入門的な説明を目的としたものではない。  その代わりに、処置割り当てを単にランダム化することは、よく設計されたフィールド実験の要素の1つであるに過ぎないことを示すことが目的であった。  自然実験を評価する際には、研究者が選択肢を通じて調整することができない場合でも、フィールド実験で提起された問題を考慮する必要がある。\n\nたとえば、「自然」が（見かけ上）ランダム化する変数は、研究者が研究したいと考える変数とは必ずしも一致しないことがよくある。  上記で見たように、フィールド実験でも、検討すべき正確な処置を慎重に考慮する必要があり、研究者が選択するものと一致する選択を「自然」が行う可能性は低いように思われる。  これに対する1つの対応策は、研究者が自然がランダム化したものが興味の対象であると主張することである。  興味のあるものと同等であると主張するための別の対応策もある。  最後に、一部の場合では、「自然」によってランダム化された変数が、因果推定の要件を満たすことがある。これについては、第20章で詳しく調べる。\n\n\n9.2.1 会計研究における自然実験\n\n2014年の会計研究のサーベイにおいて、Gow et al. (2016)は、「自然実験」または「外生的ショック」を利用して因果効果を特定するために利用された5つの論文を特定した。  しかし、Gow et al. (2016)は、これらの論文をより詳しく調べると、「どれも信憑性のある自然実験を提供していないことが明らかになる」と述べている。\n\n主な困難な点は、ほとんどの「外生的ショック」（例：SECの規制変更や裁判所の判決）は、処置群と対照群に単位をランダムに割り当てるわけではないため、自然実験として適格ではないことである。  たとえば、Dodd-Frank法の初期バージョンには、米国企業に対してスタッガード・ボード構造を撤廃するように強制する規定が含まれていた。[^6]  この出来事を利用して、スタッガード・ボードを有する企業と有しない企業の超過収益を、このDodd-Frank規定の発表前後に見ることで、スタッガード・ボードの評価結果を評価することは誘惑されるかもしれない。  これは潜在的に興味深いかもしれないが、この自称「自然実験」は企業を処置群と対照群にランダムに割り当てるわけではない。\n\nさらに、自然実験に依存する研究で説明変数の選択を慎重に考慮することが重要である。  特に、研究者は、時々、分析において処置に影響を受ける共変量を誤って使用している。  Imbens and Rubin (2015, p.116)によると、処置後の変数を共変量として含めることは、因果推論の妥当性を損なう可能性がある。[^7]\n\nGow et al. (2016)によって指摘された可能性のある自然実験の1つは、Li and Zhang (2015)である。  Li and Zhang (2015, p.80)は、SECが「一連の無作為に選択されたパイロット株式に対してショートセール価格テストの一時的な停止を命じた」という規制実験（Reg SHO）を研究している。  Li and Zhang (2015, p.79)は、「マネージャーがショートセール圧力に対するポジティブな外生的ショックに反応して、悪いニュースの予測の精度を低下させる」と推測している。  しかし、処置がこれらの予測の特性に影響を与え、Li and Zhang (2015)がその特性に依存しようとすると、「自然実験」の側面を損なうリスクがある。  Reg SHOについては、第19章で詳しく調べる。\n\nMichels (2017)は、Gow et al. (2016)によって特定された別の可能性のある自然実験であり、以下でさらに調査する。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>自然実験</span>"
    ]
  },
  {
    "objectID": "chap17_Natural.html#認識と開示",
    "href": "chap17_Natural.html#認識と開示",
    "title": "\n9  自然実験\n",
    "section": "\n9.3 認識と開示",
    "text": "9.3 認識と開示\n\n会計研究における最も長期にわたる問題の1つは、特定の項目が財務諸表に認識されるか、たとえば、それらの財務諸表の注記に開示されるかどうかが重要かどうかということである。  以下の短い例が示すように、認識と開示に関する議論は、財務報告において最も激しいものの1つである。\n\n\n9.3.1 株式報酬\n\n株式報酬の会計処理は、会計基準制定機関が取り組んだ中で最も論争の的となったトピックの1つである。  1973年、米国財務会計基準委員会（FASB）は、株価とオプション行使価格の差（または「内在価値」）をオプションの付与日に測定することを企業に要求するAPB 25を発行した。この内在価値は、ほとんどの従業員オプションに対してゼロに等しい。  1993年、FASBは、企業がオプションの付与日の公正価値に基づいて費用を測定することを提案する公開草案を発行した。  APB 25はBlack and Scholes（1973）より前に発行されたが、1993年までには、オプションの公正価値を測定する方法は広く受け入れられ、理解されていた。  この公開草案は、企業から激しい抵抗を受け、米国議会は、FASBが提案された通りに基準を最終化することを許可すべきかどうかについての公聴会を開催した。\n\n1995年、FASBは、費用を付与日の公正価値を使用して測定することを好むが、企業が費用を付与日の公正価値アプローチを使用して測定した場合の純利益がいくらであったかを開示するだけで、APB 25の測定アプローチを使用して費用を認識することを許可するSFAS 123を発行した。  FASBは、その根拠について、プライベート・セクターの会計基準設定を脅かす可能性があるため、費用認識を要求しなかったと認めた。\n\nほとんどの企業は、2002年の夏までAPB 25の測定アプローチを適用していたが、2002年の夏に一部の企業が付与日価値アプローチを採用した（Aboody et al., 2004; Brown and Lee, 2011）。  2001年の財務報告の失敗を踏まえ、FASBは株式報酬の会計処理を再検討し、2004年にSFAS 123Rを発行した。この基準は、2005年6月15日以降に開始される会計年度に効力を発揮した。  SFAS 123Rの主な効果は、付与日の公正価値を使用して株式報酬費用を認識することを要求することであった。\n\nしかし、SFAS 123Rが採用された後も、論争は続いた。  元米国閣僚や3人のノーベル賞受賞者を含む著名な人物が、従業員の株式オプションの価値は企業の費用を表していないため、費用を認識することは不適切であるという主張を繰り返し（Hagopian, 2006）、2008年にSECに申し立てを行い、SECがFASBにSFAS 123Rを発行することを許可することでその職務を怠ったと主張した。\n\n\n9.3.2 リースの会計処理\n\nリースとは、資産を所有するレッサーと、レッサーに対して資産を一定期間使用する権利を与えるレッシーとの間の契約であり、その代わりに（通常は定期的な）支払いが行われる。  リースは世界中で巨大なビジネスであり、リースは、不動産、航空機、機械などの資産の使用権を取得するための企業の一般的な方法である。\n\n資産をリースすることは、その資産を購入する代替手段であることが多い。  企業が資産を短期間だけ必要とする場合、資産をリースすることで取引コストを削減し、リース終了時の資産の価値に関連するリスクを排除することができる。  レッサーは、資産の取得、リース、処分を管理し、その価値が耐用年数を通じてどのように変化するかを理解する専門知識を持つ企業であることが多い。\n\n一部の状況では、資産所有の経済的リスクと利益はリース受益者が負担する。  たとえば、リース契約に割安購入選択権（バーゲン購入オプション）が含まれており、合理的な借手がほぼ確実にそれを行使する場合、またはリース期間の終了時に資産の所有権が借手に移転する場合、借手はあらゆる経済的な意味においてその資産を所有していると言える。\n\nただし、契約と同様に、リースには2つの側面がある。  一般的に、リース受益者は、時間の経過とともに支払われる収入の流れと引き換えに資産の経済的所有権を取得する。  多くのリース契約では、支払いは定期的で固定されている。  家を抵当に入れたり、車の購入を資金調達したりしたことがある人は、固定で定期的な支払いの義務をローンとして認識することができるだろう。\n\nしたがって、一部の場合、リースは2つの取引の組み合わせに経済的に等しい。 まず、固定で定期的な返済を必要とするローンを借りること、 そして次に、そのローンの収益を使ってリース資産を取得することである。\n\n1976年に米国財務会計基準委員会によって発行されたSFAS 13（後にASC 840という名称）は、リースが資産の購入によってローンで資金調達されたものに似ている特徴を持つ場合、そのリースを、本質的にローンで資金調達された資産の購入と同等の方法で会計処理することを要求した。  SFAS 13（米国会計基準第13号）は、リース期間終了時の資産所有権の移転やバーゲン購入オプションの存在などの基準を定めており、これらの条件を満たす場合にキャピタル・リース（資本リース）の会計処理を適用することを求めていた。\n\nキャピタル・リースの会計処理では、リース資産とリース債務が貸借対照表に認識される。  リース資産は、リース期間中に「リース受益者の通常の減価償却方針に一致する方法で」償却（または「償却」）される。  同時に、「各最低リース支払額は、債務の残高に一定の定期的利息率を生じるように、債務の減少と利息費用の間で配分される」。\n\nSFAS 13は、キャピタル・リースの会計処理の基準を満たさないリースは、オペレーティング・リースとして会計処理されるべきであると規定していた。  このようなリースについては、資産や負債は認識されず、ほとんどのリースについて、標準は「オペレーティング・リースの賃料は、支払われるたびにリース期間全体にわたって費用として計上される」と規定していた。\n\n2004年、Jonathan Weilはウォールストリートジャーナルにおいて、「企業は、銀行ローンやその他の借入から発生する財務的な義務と同じくらい実際のリース債務を貸借対照表から除外することが許可されている」と述べた。  SFAS 13は、明らかに債務で資産購入されたものと経済的に同等であるリースを分類していたが、その基準により、企業はリースを構築することができ、それらは経済的にはそのような購入に非常に近いが、財務報告目的ではオペレーティング・リースとして分類されていた。  Weilは、米国の小売薬局チェーンWalgreenを指摘し、「貸借対照表には債務がないが、今後25年間で主に店舗に対して193億ドルのオペレーティング・リース支払いを負担している」と述べ、S&P500株価指数の企業の財務諸表の注記に明示されている貸借対照表外のオペレーティング・リースのコミットメントは4820億ドルに上ると述べた。\n\n2006年、FASBは、Weilが説明したような「抜け穴」を埋めることを意図した新しいリース会計基準の作業を開始した。  SFAS 13によって提供されたオペレーティング・リースに関する詳細な情報を考慮すると、SFAS 13はオペレーティング・リースの開示を提供していると見なすことができるが、多くの人々は貸借対照表に認識することを求めていた。\n\n2016年、FASBは新しいリース会計基準であるASC 842を公表し、IASBは類似の基準であるIFRS 16を発行した。  これらの会計基準は、すべてのリースを貸借対照表に持ち込むことを要求している（短期リースを除く）。\n\n\n9.3.3 学術研究\n\n上記の2つの例は、2つの目的を果たしている。  第1に，財務報告における認識と開示の問題の重要性を強調している。  第2に，研究者が認識と開示のトピックを研究する際に直面するいくつかの微妙な問題について考えるための具体的な設定を提供している。  以下で見ていくように、これらの微妙な点のいくつかは、私たちが考えられるどんなフィールド実験でも実施できる場合でも存在するだろう。\n\n「認識」と「開示」とは何を意味するのか？  単純化すると、認識と開示の選択は、特定の金額を財務諸表に含めるか、通常は財務諸表の注記に開示するかの選択である。  しかし、Bernard and Schipper (1994)が指摘しているように、この単純な2択の「含むか含まないか」よりも、選択肢ははるかに複雑である。  FASBの財務会計概念第5号（SFAC 5）は、認識を「企業の財務諸表に資産、負債、収益、費用などとして項目を正式に取り込むプロセス」と定義している。  認識された項目は、言葉と数字の両方で描かれ、その金額は財務諸表の合計に含まれる」とされている。\n\nIASBの概念フレームワークの5.1節には、同様の定義が含まれているが、FASBの定義の一部を明確にしている。  第1に、認識は財務状態と財務業績の報告に関わるものであり、キャッシュフロー計算書に項目を記載するだけでは認識とは見なされない。  第2に、「など」とは「資本」を意味し、これにより財務諸表の要素のリストが完成する。  第3に、「数値」とは金額を意味し、これが財務諸表に要素を記載する方法である。  両定義には、金額が何らかの意味のある方法で合計されることが暗黙のうちに含まれており、財務諸表の項目は合計の構成要素として含まれる。\n\nBernard and Schipper (1994)は、「FASBの公式の発表には、認識についての形式的な定義が見当たらないようである」(p.4)と述べており、IASBの概念フレームワークについても同様のことが言える。  代わりに、Bernard and Schipper (1994)は、「認識を特別な特性を持つ開示の形態と見なすことができる」(p.5)と提案している。\n\n認識と開示は選択肢として見ることができるか？ Bernard and Schipper (1994)は、概念フレームワークを基準設定者に対する拘束条件として受け入れると、認識と開示の選択肢として見ることはできないと提案している。  項目が認識基準を満たす場合、それは認識されなければならず、満たさない場合は認識されてはならない。  しかし、概念フレームワークがこの程度まで基準を決定するかどうかは不明である。  SFAC 5には、株式報酬費用の認識に関して対立する2つの基準が出されたにもかかわらず、SFAS 123とSFAS 123Rの間で変更はなかった。\n\n上記のSFAS 123のケースは、認識と開示の最も鮮明な例の1つであると言える。  SFAS 123では、従業員の株式オプションの費用を測定するためにAPB 25アプローチを使用した企業は、財務諸表の注記に費用のプロフォーマ開示を提供することが求められた。  実際には、企業は、公正価値アプローチを使用して費用を認識した場合に提供する情報を提供する必要があった。  プロフォーマ財務諸表には財務業績計算書のみが含まれているが、財務状態計算書にSFAS 123アプローチを適用した場合の総合的な影響はない。なぜなら、費用への借方は、純利益を通じて保有利益に流れるが、追加の資本剰余金に加算されるためである。\n\nリース会計のケースは、認識と開示の設定においてより典型的かもしれない。  第1に、企業は、キャピタル・リースの会計処理の代わりにオペレーティング・リースの会計処理を選択することはできない。  リースがキャピタル・リースでない場合、オペレーティング・リースの会計処理を適用する必要がある。  リースがキャピタル・リースでない場合、オペレーティング・リースの会計処理を適用する必要がある。  （ただし、SFAS 13では、企業はしばしばリースを構築し、リースにわずかな修正を加えることで、リースが1つの会計処理から他の会計処理に切り替わるようにした。）\n\n第2に、SFAS 13には、SFAS 123で提供されたプロフォーマ開示に相当する要件はなかった。  このようなプロフォーマ開示は、SFAS 13におけるリース会計が財務状態計算書、財務業績計算書、キャッシュフロー計算書に影響を与えるため、広範囲にわたる必要がある。  代わりに、企業は、オペレーティング・リースに関連する賃料費用、最低賃料支払額の合計および次の5つの会計年度ごとのそれぞれについて、リース契約の一般的な説明（例：懸念賃料支払い、更新または購入オプション、価格調整条項などの詳細）を開示することが求められていた。\n\nなぜ認識と開示が重要なのか？  一旦、認識と開示の資本市場への影響に焦点を当てると、スペクトラムの両端にある2つの理論がある。\n\nWatts (1992)によると、「機械論的仮説(mechanistic hypothesis)は、報告された利益数値がどのように計算されているかに関係なく、株価が機械的に反応すると仮定している」。  （機械論的仮説の自然な一般化は、財務諸表に記載された数値と、財務諸表に記載された数値に基づく財務比率に適用することである。）  スペクトラムの反寇には、効率的市場仮説（EMH）があり、その準強い形式では、資本市場は公開されているすべての情報を効率的に処理し、その情報を証券価格に反映すると仮定している。\n\n機械論的仮説の下では、数値が注記に開示されているが財務諸表に認識されていない場合、市場はそれに反応しない。  一方、効率的市場仮説の下では、市場は情報に反応するが、それが認識されているか開示されているかにかかわらずである。\n\n最後の文には、認識と開示の決定によって情報内容が影響を受けないという仮定が含まれている。  しかし、これはいくつかの理由で真実でないかもしれない。\n\n第1に、開示された金額が認識プロセスの一部として金額に集約および変換されるため、情報内容が異なる可能性がある。  オペレーティング・リースの場合、将来のリース支払いは割引され、貸借対照表に認識される負債を形成するために加算される。  さらに、資産は減価償却され、純持ち高で報告される。  これらの計算は、外部の者が開示された情報を正確に複製するのが難しい（たとえば、リース支払いのタイミングや割引率に関する不正確な情報があるため）が、その結果は市場参加者に有益な情報を伝えるかもしれない。\n\n第2に、認識と開示の間の概念的な情報内容が異なる場合でも、認識と開示の決定は、2つの条件間での行動の違いによって情報の特性に影響を与える可能性がある。  たとえば、経営者は認識された情報をより重要と見なし、より正確な数値を生産するためにより多くの努力を払うか（または利益マネジメントにより多くの努力を払うか）もしれない。  監査人も、認識された数値に対する保証を提供する義務が、単に開示された数値に対するそれよりも大きいと見なすかもしれない。\n\nたとえ効率的市場仮説が成立していても、経営者や他の人々がそれが成立していないと信じ、代わりに機械論的仮説が現実をよりよく説明していると仮定した場合、彼らはそれに応じて行動する。  SFAS 123Rの導入に先立ち、多くの企業が株式オプションのベストを加速させて費用を報告を回避した。  この行動の経済的結果はかなり明確であった（基本的には企業から従業員への富の移転）が、Choudhary et al. (2009)は、加速決定が発表されたときに市場がそれに応じて反応したことを発見している。  しかし、Choudhary et al. (2009)は、機械論的な視点にしっかりと根ざした1つの企業（Central Valley Community Bancorp）の視点を引用している。\n\n\n従来の約3年間の残存権利確定期間にわたって企業の利益に対してポジティブな影響を与えると考えられるため，FASBはこれらのオプションを早期行使することが株主の最善の利益になると判断した。\n\n\n認識と開示を研究するための実験の設計  認識と開示が重要であるさまざまな方法を考慮すると、さまざまな政策のメリットを理解するためにフィールド実験を実施することを求める規制当局は、実施可能な実験のさまざまな選択肢に直面する。  認識が開示に対して重要であることを知っても、それがなぜ重要なのかを理解していないと、つまり因果関係のメカニズムを理解していないと、それはあまり役に立たない。  検証されるべき因果関係のメカニズムに応じて、適切な実験設計が異なるため，これは重要である。\n\n実験設計が仮説されるメカニズムによってどのように変化するかを理解する最良の方法は、いくつかの具体的なケースを考慮することである。  投資家が開示されたが認識されていない注記情報をプロフォーマ財務諸表情報に変換する際に困難を抱えているために認識が重要であると仮定すると、企業を3つのグループ：財務諸表に認識する企業、財務諸表に認識しないが注記にプロフォーマ情報を開示する企業（SFAS 123のような）、注記に追加の処理や詳細が必要な情報を開示する企業（SFAS 13のオペレーティング・リースの開示に類似）に分けることができる。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>自然実験</span>"
    ]
  },
  {
    "objectID": "chap17_Natural.html#michels-2017",
    "href": "chap17_Natural.html#michels-2017",
    "title": "\n9  自然実験\n",
    "section": "\n9.4 Michels (2017)",
    "text": "9.4 Michels (2017)\n\nこれまでの章では、自然実験を含む実験と、認識と開示のより広い問題について取り上げてきた。  ここで、Michels (2017)に移り、信頼性のある自然実験を用いて認識と開示の1つの側面を研究している。\n\nMichels (2017)は、財務諸表が発行される前に発生した重要なイベントに対する開示要件の違いを利用している。  これらのイベント（例：火災や自然災害）の発生時期は、貸借対照表の日付に対してランダムである可能性があるため、開示と認識の条件への割り当てはランダムである可能性がある。\n\nfarrパッケージのmichels_2017データセットは、Michels (2017)の423の観測値に関する情報を提供している。  これらの観測値のうち、343の場合、自然災害は前の財務期間の関連する提出後に発生し、そのため、財務効果は現在の期間に認識される。  残りの80件の場合、自然災害は前の財務期間の関連する提出前に発生し、そのため、財務効果はその提出で開示され（および現在の期間の財務を含む後の提出で認識される）。\n\n図17.1は、該当する自然災害と次の提出日の間の日数の分布を示している。\n\nmichels_2017 |&gt;\n  mutate(days_to_filing = as.integer(date_filed - eventdate)) |&gt;\n  ggplot(aes(x = days_to_filing, fill = recognize)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n図17.2は、該当する自然災害とその発生した財務期間の終了日との日数の分布を示している。\n\nmichels_2017 |&gt;\n  mutate(days_to_period_end = as.integer(next_period_end - eventdate)) |&gt;\n  ggplot(aes(x = days_to_period_end, fill = recognize)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n明らかに、大規模な自然災害は1つ以上の企業に影響を与える可能性があり、表17.1はMichels (2017)のサンプルにおける最も一般的な災害日に関するデータを提供している。\n\nmichels_2017 |&gt;\n  count(eventdate) |&gt;\n  arrange(desc(n)) |&gt;\n  top_n(5)\n\nSelecting by n",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>自然実験</span>"
    ]
  },
  {
    "objectID": "chap18_Causal_mechanisms.html",
    "href": "chap18_Causal_mechanisms.html",
    "title": "10  因果メカニズム",
    "section": "",
    "text": "10.1 ジョン・スノウとコレラ\nGow et al. (2016) は、会計研究の多くは因果推論を目的としているものの、統計的手法を単純に適用するだけで因果効果の信頼できる推定を得られるような状況を見つけることは非常に困難であると主張している（そして、本書のこの部分の残りの章もこの主張を支持していると考えられる）。  これは、会計研究者が因果関係に関する主張を諦めなければならないことを意味するのか。\nGow et al. (2016) は、この問いに対する答えは「否である」と主張し、因果メカニズムに焦点を当てることが、因果推論を進めるための一つの方法であると示唆している。\n本章では、因果メカニズムの概念を読者に理解してもらうことを目的としている。因果メカニズムを明確にすることで、例えば無作為割り当て（random assignment to treatment）に依存しない実証分析が可能となるが、会計研究では無作為割り当ての機会がほとんどないため、これは特に重要である。  本章は、本書のこの部分の他の章とは異なり、主に定性的な議論を中心とし、実証分析を含んでいない。\n会計研究は、観察データ（observational data）に依存しつつ因果推論を目指している点で、他の分野と共通している。  このため、観察データを用いて因果メカニズムを特定し、最終的に因果推論を行う他の分野を参考にするのは自然な発想である。  特に、疫学（epidemiology） や 医学（medicine） は、この点でよく引き合いに出される分野である。  次の2つのセクションでは、具体的な例を簡単に紹介し、それらの例がどのように因果推論の信頼性を高める要素を含んでいるかを示す。[^1]  この議論の重要な含意は、会計研究者は、自身の研究結果を生み出す因果メカニズムを明確かつ厳密に特定する必要があるという点である。\nジョン・スノウのコレラ研究は、因果推論の成功例として広く引用されている。  スノウの研究には多くの優れた解説が存在するため、ここでは最も基本的な要点に焦点を当てる。  Freedman (2009, p.339) によれば、「ジョン・スノウはヴィクトリア朝時代のロンドンの医師であった。  1854年、彼はコレラが感染症であり、水の供給を浄化することで予防可能であることを実証した。  この実証は、自然実験（natural experiment） を利用したものであった。  ロンドンの広範な地域では、2つの水道会社が給水を行っていた。  Southwark and Vauxhall 社は汚染された水を供給しており、この会社の水を使用する家庭の死亡率は、『Lambeth 社（比較的純粋な水を供給していた）の給水地域の死亡率の8～9倍』であった。」  しかし、スノウの研究の核心は単なる自然実験の利用にとどまらなかった。  まず、スノウの推論はコレラの感染メカニズムに関するものであり、これは「困難なデータ収集作業」が始まる前に多くが行われたと考えられる。  既存の理論では、「腐敗した有機物によって生成される臭気」とされていた。 当時の既存の理論では、「腐敗した有機物が発する悪臭（miasma）」がコレラの原因とされていた。  しかし、スノウは定性的な推論を行い、このメカニズムが非現実的であると考えた。  代わりに、自身の医学的知識と観察された事実をもとに、「生物（病原体）が水や食物に含まれ、それが体内に侵入し、増殖して症状を引き起こす。そして、多数の病原体が排泄物として体外に排出され、水や食物を汚染し、新たな感染者を生み出す」という仮説を立てた（Freedman, 2009, p. 342）。\n仮説を手に入れたスノウは、それを証明するためのデータを収集する必要があった。  彼のデータ収集には、サザーク・ヴォークホール社が運営するブロードストリートポンプ周辺の地域での家庭訪問調査が含まれていた。  データ収集の一環として、スノウは異常なケース（たとえば、ビールを飲む醸造所の労働者）を考慮に入れる必要があった。  こうした質的推論と綿密なデータ収集が、ブロードストリートポンプによって提供された処置割り当てメカニズムの「まるで」ランダムな性質を（現代の読者に）確立する上で重要な要素であることに注意することが重要である。  スノウの慎重な方法は、彼のデータに自然実験があると主張することができたショートカットアプローチとは対照的である。\nこの例のもう1つの重要な特徴は、スノウの仮説が広く受け入れられるようになったのは、正確な因果メカニズムに関する説得力のある証拠が提供された後だったという点である。 「しかし、広く受け入れられるようになったのは、1883年のインドでの流行時にロベルト・コッホが原因菌（コンマ状の桿菌であるコレラ菌 Vibrio cholerae）を分離したときだった」（Freedman, 2009, p. 342）。  スノウの考えが広く受け入れられるようになったのは、説得力のある因果メカニズム（すなわち、現在その病気の原因であると知られている微生物の直接観察）が提供された後のことだった。\n我々は、会計学の分野でも、研究者が注意深く自分たちの観察結果に対する仮定された因果メカニズムを明確に表現すれば、同様のことが起こると予想している。  もちろん、研究者は、検討されている制度的環境で観察された行動と一致することを示す必要がある。  以下で議論するように、制度現象の詳細な記述的研究は、提案されたメカニズムを評価するために使用される情報の重要な部分を提供する。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>因果メカニズム</span>"
    ]
  },
  {
    "objectID": "chap17_Natural.html#ディスカッション課題",
    "href": "chap17_Natural.html#ディスカッション課題",
    "title": "\n9  自然実験\n",
    "section": "\n9.5 ディスカッション課題",
    "text": "9.5 ディスカッション課題\n\n\nMichels (2017)の仮定の1つは、次の提出の貸借対照表日の前後に自然災害が発生するかどうかがランダムであるということである。  自然災害の固有の特性は、それらがランダムであることを保証するか？なぜか？  そうでない場合、Michels (2017)のサンプルにおける自然災害のランダム性をどのように評価するか？  上記の分析は、この評価に役立つのか？\n\n\n自然災害の発生からその出来事についての報告までのプロセスを想像して説明しなさい。  認識されたイベントと開示されたイベントでは、このプロセスが異なると考えるか？\n\n\n上記の分析から、5つの自然災害が228の観測値を占めているように見える。  各日付と「災害」という単語のシンプルなGoogle検索により、これらのイベントがハリケーン・カトリーナ（2005年8月29日）、ハリケーン・アイク（2008年09月13日）、ハリケーン・アイバン（2004年09月16日）、ハリケーン・チャーリー（2004年08月13日）、ハリケーン・ウィルマ（2005年10月24日）であることがわかる。  サンプルの大部分を占める少数の災害が問題であるか？\n\n\nMichels (2017)は自然災害に関するデータをどこから入手しているのか？このデータソースに問題があるか？データ収集に別のアプローチを使用することは可能か？そのアプローチはどのような課題に直面するか？\n\n\n会計において繰り返し問われるのは、情報が開示されるか認識されるかが重要かどうかということである。  もし市場が効率的であれば、情報が開示される場所は問題にならないはずであり、したがって、認識は開示に対して重要ではないはずである。  この見解の背後にある仮定は何か？  Michels (2017)の設定では、それらが成立するかどうかを信じる理由はあるか？  Michels (2017)がクリーンな因果推論を提供する能力にはどのような影響があるか？  開示されたイベントと認識されたイベントに対する重要性基準はどのように異なるか？  これらの基準の違いは、Michels (2017)の実証分析に影響するか？\n\n\nMichels (2017)はどのような因果推論を導いているか？これらに関してどのような問題があるか（あれば）？\n\n\n最近見た、非実験データの実証分析を使用している論文を選択しなさい。（そのような論文を選択できない場合、Hopkins et al. (2022)が1つの選択肢を提供している）論文の要約を見ると、この論文が因果推論を導こうとしているかどうかを判断できますか？\n\n\n選択した論文で、著者が最も重要だと考えている（または描きたい）と思われる因果推論を選択しなさい。  どの表が関連する実証分析を提供していますか？  この因果推論について、論文の議論または自分のバックグラウンド知識を使用して重要な変数を特定し、おおまかな因果図をスケッチしなさい。  あなたの因果図を考慮して、報告された因果推論をどの程度信頼性があると考えるか？",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>自然実験</span>"
    ]
  },
  {
    "objectID": "chap18_Causal_mechanisms.html#喫煙と心臓病",
    "href": "chap18_Causal_mechanisms.html#喫煙と心臓病",
    "title": "10  因果メカニズム",
    "section": "10.2 喫煙と心臓病",
    "text": "10.2 喫煙と心臓病\n\nより最近の因果推論の妥当性の例が、Gillies (2011)によって議論されている。  Gillies (2011)は、Doll and Peto (1976)による、1951年から1971年までの間に男性医師の死亡率を調査した論文に焦点を当てている。  Doll and Peto (1976)のデータは、「喫煙と肺がんとの間に顕著な相関がある」と示していた（Gillies, 2011, p.111）。  Gillies (2011)は、「この相関は、当時、ほとんどの研究者（すべてではないが）によって、喫煙と肺がんとの間の因果関係を確立するものとして受け入れられていた」と主張している。  実際、Doll and Peto自身も「喫煙者の肺がんによる余剰死亡は、喫煙によるものである」(p.1535)と述べている。  一方、Doll and Peto (1976)は、喫煙と心臓病との間に関連があるという非常に統計的に有意な証拠を持っていたが、その関連に対する直接的な因果関係の説明を推論することには慎重だった。  Doll and Peto (1976, p.1528)は、「これらの状態が喫煙と関連していると言うことは、必ずしも喫煙が…それらを引き起こしたとは限らない。  関係は、喫煙がアルコール摂取や人格の特徴など、疾病を引き起こす他の要因と関連していた可能性がある」と指摘している。\n\nGillies (2011)は、1979年から1989年にかけての動脈硬化に関する広範な研究について議論し、その結果、「1980年代末までに、LDLの酸化が動脈硬化斑の形成につながる過程で重要なステップであることが確立された」と結論付けている。  後の研究（Morrow et al., 1995, p.1201）は、「喫煙が人間の生物学的成分の酸化修飾を引き起こすことを示す説得力のある証拠を提供した」と述べている。[^2]  Gillies (2011, p.120)は、この証拠だけでは、喫煙と心臓病を結びつける確認されたメカニズムを確立することができなかったことを指摘している。なぜなら、必要な酸化は血流ではなく動脈壁で起こる必要があり、この欠けている部分を確立するために後の研究が行われた。[^3]  したがって、2つの数十年にわたる多くの研究を通じて、喫煙と動脈硬化の間の因果メカニズムの妥当なセットが確立された。\n\nGillies (2011)は、喫煙と動脈硬化の間の因果関係が確立されたプロセスが、Russo-Williamsonの論文を示していると主張している。  Russo and Williamson (2007, p.159)は、「メカニズムは因果関係を一般化することを可能にする」と述べている。つまり、サンプルデータにおいて適切な依存関係が確認されれば、「C がサンプル集団において E を引き起こす」という因果的主張を正当化することはできるが、より一般的な「C が E を引き起こす」という主張を正当化するためには、妥当なメカニズムや理論的なつながりが必要である。  逆に、メカニズムは否定的な制約も課す。つまり、C から E への妥当なメカニズムが存在しない場合、その相関関係は疑似的なものである可能性が高い。  したがって、メカニズムは、確率的証拠だけでは十分に決定できない因果モデルを区別するために用いることができる。  Russo-Williamsonの論文は、スノウとコレラの場合にも適用されたと言える。つまり、スノウによって提供された因果説明が広く受け入れられる前に、メカニズム（すなわち、コレラ菌 Vibrio cholerae）の確立が不可欠であった。  これは、最初に相関に基づいて推測された喫煙と肺がんの場合にも見られる。直接的な生物学的説明が提供される前に推測された。[^4]",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>因果メカニズム</span>"
    ]
  },
  {
    "objectID": "chap18_Causal_mechanisms.html#会計研究における因果メカニズム",
    "href": "chap18_Causal_mechanisms.html#会計研究における因果メカニズム",
    "title": "10  因果メカニズム",
    "section": "10.3 会計研究における因果メカニズム",
    "text": "10.3 会計研究における因果メカニズム\n\nGow et al. (2016)は、会計研究者は、観察データに取り組むが最終的に因果関係を示す分野である、疫学、医学、政治学などの分野から学ぶことができると提案している。  ランダム化比較試験（RCT）は、ある意味で疫学におけるゴールドスタンダードとされているが、そのような試験を実施することが現実的でない場合や倫理的に許容されない場合も多い。  例えば、政治学においては、国をランダムに割り当てて民主主義や社会主義といった処置条件を適用することは不可能である。  それにもかかわらず、これらの分野では、想定される原因から想定される結果への明確なメカニズムや因果経路を確立することによって、妥当な因果推論を導き出すことができる場合が多い。\n\n因果識別戦略が比較的説得力のある論文の一つに、Brown et al. (2015) がある。この論文では、「州全体で施行される運転中の携帯電話使用規制を利用して、モバイル通信が地域の情報伝達や地域の投資家活動に与える影響」を分析している。  著者らは、「これらの規制が…地域の情報流通を妨げ、…規制が施行された州に本社を置く企業の株式市場活動に影響を与える」ことを発見した。  Miller and Skinner (2015, p. 229) は、「著者らの研究環境と研究デザインを考慮すると、通常の開示研究で問題となる逆の因果関係や相関する欠落変数による説明が成り立つとは考えにくい」と指摘している。  しかし、研究デザインの明白な頑健性にもかかわらず、推定された効果が発生する正確な因果メカニズムに関するより詳細な証拠が示されれば、結果はさらに説得力を増すであろう。著者らは、そのような説明を提供することに相当な努力を払っているように見受けられる。[^5]  例えば、運転中の地域投資家の取引活動が、規制導入前には見られたが、導入後には見られなくなったという証拠が示されれば、Brown et al. (2015) の結論を大いに裏付けるものとなるだろう。[^6]\n\n別の例として、多くの公表された論文は、経営者が負債コストの削減などの利益を得るために、条件付き保守主義を報告戦略として採用することを示唆している。  しかし、Beyer et al. (2010, p.317) が指摘するように、そのような報告戦略に事前にコミットするためには、「経営者が良いニュースを意図的に開示しないことを信頼性をもって約束する、または利益に対して損失よりも厳格な検証を課す会計情報システムにコミットすることを可能にするメカニズムが必要である」。しかしながら、そのようなコミットメントがどのように行われるのかについての研究は、最近になってようやく焦点を当てられるようになってきた（例：Erkens et al., 2014）。[^7]  重要な会計研究の課題について、正確な因果メカニズムをより深く理解することが極めて重要であることは明白である。  これらのメカニズムについて明確に議論することで、査読者や読者は何が前提とされているのかを理解し、提示された因果メカニズムの妥当性を評価することができるようになるだろう。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>因果メカニズム</span>"
    ]
  },
  {
    "objectID": "chap18_Causal_mechanisms.html#ディスカッション課題",
    "href": "chap18_Causal_mechanisms.html#ディスカッション課題",
    "title": "10  因果メカニズム",
    "section": "10.4 ディスカッション課題",
    "text": "10.4 ディスカッション課題\n\n\n10.4.1 Li et al. (2018)\n\n\nLi et al. (2018) の因果メカニズムについて考えてみよう。  あなたが、会社の10-Kのために重要な顧客に関する情報を準備するプロセスに関与していると仮定しよう。  あなたの主な専門分野は何であると考えるか。  この役職にある間に、裁判所がIDDを支持または却下した場合、そのことを知ることになると考えるか。  もしそうであれば、どのようにしてその情報を得るか。  もしそうでなければ、裁判所の判決が提出書類の内容に関する決定にどのような影響を与える可能性があるか。\n\n\n関連する管理職がIDDに関する裁判所の判決を把握していると仮定した場合、これが開示の決定にどのような影響を与えると考えられるか。 この影響はいつ発生するか。  どの程度の期間継続すると考えられるか。\n\n\nPepsiCoはニューヨーク州に本社を置いている。  この事実は、Li et al. (2018, pp. 272–3) における PepsiCo Inc. v. Redmond 事件の議論において、リサーチ・デザインに影響を与えるか。  –&gt; この事実を考慮すると、Li et al. (2018) はどのように研究を進めるべきであったか。\n\n\n\nLi et al. (2018, p. 284) のプロットを確認せよ。ここでは何が示されることを期待すべきか。 フィットした曲線と赤い縦破線を除いた状態を想像した場合、この証拠はどの程度説得力があるか。なぜ著者らはこれらを含めたと考えられるか。 また、Li et al. (2018, p. 284) の図にTable 2の結果を反映させる場合、どのように描写すべきか（特に数値の大きさに着目せよ）。\n\n\n\nSection 3.2では、二方向固定効果（学年と学生ID）を用いた回帰を実施した。Li et al. (2018) のTable 2の回帰 (3) および (4) において、学年と学生IDに相当する変数は何か。 Table 2 に POST \\times TREAT の係数が報告されていないのはなぜか。 IDDが一度も導入されなかった州に所在する企業に対して、POST をどのようにコーディングすべきか。\n\nLi et al. (2018) のTable 2とTable 3を比較せよ。両者の中で最も比較可能な回帰はどれか。 また、Table 3 はTable 2に比べてどのような洞察を提供しているか。\n\n\n\n10.4.2 Burks et al. (2018)\n\n\n自身の州以外の州がIBBEAを実施した場合、銀行の上級幹部としてどのような機会が生じる可能性があるか。これらの機会をどのように評価するか。評価にはどの程度の時間がかかるか。1\n\n\n\n自身の州がIBBEAを実施しようとしている銀行の上級幹部であるとする。他州の銀行の関心が高まることを予想する場合、どのような行動を取るか。他州の銀行の参入方法によって、取るべき行動は変わるか。 \nBurks et al. (2018) は「預金上限（deposit cap）」について説明していない。Rice and Strahan (2010) によると、「IBBEAは、州外銀行が州内に初めて進出する際の州全体の預金集中率の上限を30%に設定している」とある。異なる州における預金上限の違いは何によって生じるか（Table 1を参照）。この制限は、IBBEAを実施する州内のすべての銀行の行動に同じように影響を与えるか。\n\n\n\n「結果は、IBBEAの導入により情報開示が有意に増加すること（主効果）と一致する。また、30%未満の預金上限を課す制限が情報開示に対して有意な抑制効果を持つことが判明した。」 30%未満の上限を実施した州はいくつあるか（Table 1を参照）。このうちコロラド州は25%の上限を設定していた。1997年時点で市場シェア最大の銀行は17.87%であった。 この事実を考慮すると、25%と30%の違いには実際にどのような意味があるか。\n\n\n\n\n10.4.3 Christensen et al. (2017); Glaeser and Guay (2017)\n\n\nChristensen et al. (2017) は、SEC提出書類での情報開示義務が鉱山安全に因果的影響を及ぼすことを発見したと主張している。 しかし、この情報はすでにMSHA（鉱山安全衛生局）のウェブサイトで公開されていた。この事実が重要であるのはなぜか。 因果的影響の証拠をどの程度説得力があると考えるか。特に説得力がある（または問題がある）要素は何か。\n\n\n\nChristensen et al. (2017) は、この影響が生じる因果メカニズムをどのように説明（または推測）しているか。\n\n\n\nGlaeser and Guay (2017) は、証券法を安全規制の執行手段として活用することが、因果効果をもたらす別のメカニズムであると示唆している。この指摘は、Christensen et al. (2017) における因果効果の推定の妥当性を損なうか。",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>因果メカニズム</span>"
    ]
  },
  {
    "objectID": "chap18_Causal_mechanisms.html#footnotes",
    "href": "chap18_Causal_mechanisms.html#footnotes",
    "title": "10  因果メカニズム",
    "section": "",
    "text": "[訳註]IBBEA(Interstate Banking and Branching Efficiency Act of 1994)とは、1994年の州際銀行・支店効率法のことである。この法律は、アメリカにおける州を越えた銀行業務の制限を緩和し、銀行の州際合併や支店展開を促進するために制定された。↩︎",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>因果メカニズム</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html",
    "href": "chap19_Natural_experiments_revisited.html",
    "title": "\n11  自然実験:再訪\n",
    "section": "",
    "text": "11.1 再現危機？\nこの章では、自然実験のトピックを再訪する。  まず、登録報告（Registered Reports）の概念、その目的、およびその限界について論じる。  次に、米国証券取引委員会(SEC)によって実施された実験「Reg SHO」と、それに関する研究を検討する。特に、Fang et al.（2016）の研究に焦点を当て、この規制を利用して利益マネジメント(Earnings Management)への影響を分析した点を詳しく論じる。\nこの章では、以下のような領域でスキルと知識を磨く機会を提供する。  第1に、利益マネジメントのテーマを再訪し、Dechow et al. (1995)以来の測定手法の発展について学ぶ。このテーマは第16章ですでに扱っている。  第2に、Reg SHOおよび頻繁に研究されている証券会社の閉鎖(broker-closure shocks)の事例を用いて、自然実験とされる評価スキルをさらに発展させる。  第3に、差分の差(diffrence-in-differences)手法を探求し、ランダムな割当がある場合と、ランダムな割当がない場合の「平行トレンド仮定」(parallel trends assumption)に基づく場合の両方について考察する。  第4に、第4章および第18章で扱った因果ダイアグラムと因果メカニズムに関連する概念を応用する機会をもつ。  第5に、統計的推論のテーマを再訪し、この章をランダム化推論(randomization inference)を考慮する機会として利用する。  第6に、Frisch-Waugh-Lovellの定理を基に、会計研究の多くの分野で一般的な二段階回帰の使用に関連する問題を考察する。\nロバート・ウィグルズワースによるFinancial Timesの記事は、ファイナンス研究における「再現危機」を取り上げた。  Wigglesworthは、デューク大学のファイナンス教授であるCanmpbell Harveyの発言を引用しており、Harveyは「これまでのトップのファイナンス・ジャーナルで特定された400以上の市場を打ち負かすとされる戦略のうち、少なくとも半分は偽物である」と述べている。\nWigglesworthは、研究者が「有意な」および「ポジティブな」結果を探すという実践であるp-hackingを研究者が行っていると指摘し、「問題の核心」を特定した。  ここで「有意性」とは統計的有意性を指し、「ポジティブ」とはいわゆる「帰無仮説」を棄却し、それによって（推定される）人間の知識を前進させる結果を指す。  Harvey（2017）は、90%の発表された研究がこのような「有意な」および「ポジティブな」結果を報告していると示唆する研究を引用している。  「ポジティブ」な結果を報告することは、出版されるだけでなく、引用を集めるためにも重要であり、これは研究者やジャーナルの行動を促すことができる。\nSimmons et al.（2011, p.1359）は、彼らが研究者の自由度と呼ぶものを説明している。   「データを収集し分析する過程で、研究者は多くの決定を下さなければならない。より多くのデータを収集すべきか？ 一部の観測を除外すべきか？ どの条件を統合し、どの条件を比較すべきか？ どの統制変数を考慮すべきか？ 特定の測定値を統合すべきか、それとも変換すべきか、あるいはその両方を行うべきか？」  Simmons et al. (2011, p.1364) は、もう一つのよく知られた研究者の自由度として、「うまくいった実験のみを報告する」ことを挙げている。これは「ファイルドロワー問題」として知られており、うまくいかなかった実験が引き出しにしまわれてしまうことを指す。\n研究者の自由度の影響力を示すために、Simmons et al. (2011) は、ライブの被験者を用いた実験に基づく2つの仮想的な研究を実施した。  彼らは、これらの研究が「統計的に有意な証拠を、誤った仮説のために蓄積し（そして報告し）てしまうことが、いかに容認できないほど容易であるかを示している」と主張している [p.1359]。  また、Simmons et al. (2011, p.1359) は、「データ収集、分析、および報告における柔軟性が、実際の偽陽性率を劇的に上昇させる」と結論付けている。\nおそらく、Simmons at al. (2011) が提起したものと同様の懸念に対応する形で、Journal of Accounting Research (JAR) は、2017年5月に開催された年次カンファレンスにおいて試験的な取り組みを実施した。  JARのウェブサイトによると、このカンファレンスでは「著者が登録ベースの編集プロセス（Registration-based Editorial Process, REP）を通じて作成した論文を発表した」。  このカンファレンスの目的は、会計研究においてREPを導入できるかを検証し、またその最適な実施方法を探ることであった。  カンファレンスで発表された論文は、2018年5月にその後出版された。\nBloomfield et al. (2018, p.317) によると、「REPでは、著者が自身の予測を検証するためにデータを収集・分析する計画を提案する。  ジャーナルは、有望な提案を一人または複数の査読者に送付し、修正を推奨する。  著者はこれに応じて提案を見直す機会を与えられ、しばしば複数回の修正を経た後、提案はリジェクトされるか、または‘原則的アクセプト’が与えられる。… これは、[その後の] 結果が彼らの予測を支持するかどうかに関わらず行われる。」\nBloomfield et al. (2018, p.317) は、REPと従来の編集プロセス（Traditional Editorial Process, TEP）を対比している。  TEPのもとでは、「著者はデータを収集し、分析し、原稿を執筆・修正したうえで、編集者に送る前に何度も見直しを行う」。  また、Bloomfield et al. (2018, p.317) は、「社会科学の査読付き論文のほぼすべてが … TEPのもとで出版されている」と指摘している。\nREPは、Simmons et al. (2011)によって特定されたものを含む疑問の余地のある研究手法を排除するために設計されている。  たとえば、p-hackingの一形態はHARKing（「結果がわかってから仮説を立てる」）である。  その極端な形態では、HARKingは「有意な」相関を探し、それを「予測」するための仮説を立てることを含む。  たとえば、Tyler Vigenが提供する見せかけの相関のウェブサイトを考えてみよう。  このサイトは、メイン州の離婚率とマーガリンの消費量の間の99.26%の相関や、アメリカの科学、宇宙、技術への支出と絞首、絞殺、窒息による自殺との間の99.79%の相関など、明らかにスパリアスな相関をいくつかリストしている。  これらの相関は、通常の人間は、これらの相関を説明する根底にある因果関係がないという強い事前の信念を持っているため、見せかけのものと見なされる。  これらは単なる偶然と見なされる。\nしかし、創造的な学者はおそらく、どんな相関も「予測」するためのストーリーを作り出すことができる。  たとえば、科学への支出を増やすことで、それが社会にとって重要であるという認識が高まるかもしれない。  しかし、科学に注目することは、アメリカが科学を含む多くの分野で相対的に衰退していることを浮き彫りにするだけである。  この衰退にもかかわらず、多くのアメリカ人はこれを乗り越えることができるが、他の人々はそれについて楽観的ではなく、その結果として極端な手段に出るかもしれない…。\nこれは明らかにばかげた推論であるが、もしいくつかの参考文献や洒落た用語を加えれば、おそらくいくつかの学術論文の仮説開発セクションとよく似たものになるだろう。\nBloomfield et al. (2018, p.326) は、2017年のJARカンファレンスからの「論文の結果の強さ」を彼らのセクション4.2で検討し、結論付けている。「… 7つの提案で行われた30の予測のうち、著者が報告した134の統計的検定のうち、少なくとも1つでp \\leq 0.05で支持されると数えられるものは10つある。  残りの20の予測は、報告された84の検定のいずれにもp \\leq 0.05で支持されていない。  全体的に、我々の分析は、論文がJARおよびその同僚誌に掲載された論文の中で一般的なよりもはるかに弱く、著者の予測を支持している」と述べている。[^2]",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#再現危機",
    "href": "chap19_Natural_experiments_revisited.html#再現危機",
    "title": "\n11  自然実験:再訪\n",
    "section": "",
    "text": "11.1.1 ディスカッション課題\n\n\nSimmons et al. (2011) は、Bloomfield et al. (2018, pp.318–319)で議論されたTEPの問題について、より詳細な検討を行っている。  Simmons et al.（2011）が研究した2つの実験は、実際の会計研究の進め方を代表しているといえるか。  アーカイバル・データを使用した実証会計研究には、どのような違いがあるのか。\n\n\nBloomfield et al.（2018, p.326）は、「Hail et al.（2018）は正式な仮説を提示していないため、[結果の] 集計から除外した」と述べている。  正式な仮説がないにもかかわらず、Hail et al. (2018) の提案を2017年の Journal of Accounting Research（JAR）カンファレンスに含めるのは妥当だったのだろうか？  REPは、正式な仮説を持たない論文にも適用可能なのか？  正式な仮説がないことは、Hail et al. (2018) が仮説を検証していないことを意味するのか？  最後の質問に対するあなたの答えは、Hail et al. (2018, p.650) が論文の Table 5 に関する結果をどのように議論しているかと整合しているだろうか？\n\n\nBloomfield et al. (2018)の分析によると、30の仮説に対して218のテストが行われ、異なる仮説には異なる数のテストが行われた。  以下の分析では、30の仮説があり、それぞれが7つのテストを行うと仮定する（合計210のテスト）。  この分析は、Bloomfield et al. (2018)が提供した「通常よりもはるかに弱い」という結論以外の結果の可能性の代替的な解釈を示唆しているだろうか。  set.seed()の値を変更すると、この分析の結果の傾向が変わるだろうか？  以下の分析をより確定的にするためには、どのようにすればよいだろうか？[^3]\n\n\nset.seed(2021)\nresults &lt;-\n  expand_grid(hypothesis = 1:30, test = 1:7) |&gt;\n  mutate(p = runif(nrow(pick(everything()))),\n         reject = p &lt; 0.05)\n\nresults |&gt; \n  group_by(hypothesis) |&gt;\n  summarize(reject_one = any(reject), .groups = \"drop\") |&gt;\n  count(reject_one)\n\n\n  \n\n\n\n\n\nBloomfield et al. (2018, p. 326) は、「いくつかの会議論文の改訂により、TEP（Traditional Empirical Paradigm）で発表された論文の大半と同程度の強さの結果を報告できるようになることは容易に想像できる」と主張している。  例えば、「Li and Sandino (2018) は、主要な仮説に対する統計的に有意な支持を得られなかった。   しかし、受理された提案書に含まれていた非公式な予測と整合的な計画的追加分析において、有意な結果を見出した。… [この証拠を踏まえると] 本号の研究が、TEP の下で発表された研究よりも予測に対する支持が弱いと結論づけるには至らない」（2018, p. 326）と述べている。  これらの結果は、TEP のもとで発表された研究の結果の強さについて何かを示唆していると解釈することはできるだろうか？\n\n\nREP（Registered Reports and Enhanced Transparency Paradigm）が会計研究の主要な研究パラダイムとなることは現実的だろうか。その発展にはどのような課題が伴うのか。\n\n\nBloomfield et al.（2018, p. 337）が実施した調査の回答者のコメントを以下に引用する。このコメントについて論じよ。回答者が「学習チャネル（learning channel）」についてどのような考えを持っていると考えられるか。また、REPがこのチャネルを閉ざしてしまうという指摘に賛同するか。\n\n\n「『帰無結果（null results）』が多く報告されることに驚きは感じない。それは自らの経験からも容易に予測できることだ。研究は反復的なプロセスであり、学習を伴うものだ。特に、非常に新規性の高い研究課題で、我々がまだほとんど知らない領域において、学習チャネルを閉ざすことで研究プロセスにおいて何か有益な発見が得られるとは思えない。」",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#the-reg-sho-experiment",
    "href": "chap19_Natural_experiments_revisited.html#the-reg-sho-experiment",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.2 The Reg SHO experiment",
    "text": "11.2 The Reg SHO experiment\nTo better understand the issues raised by the discussion above in a real research setting, we focus on the Reg SHO experiment, which has been the subject of many studies. In July 2004, the SEC adopted Reg SHO, a regulation governing short-selling activities in equity markets. Reg SHO contained a pilot program in which stocks in the Russell 3000 index were ranked by trading volume within each exchange and every third one was designated as a pilot stock. From 2 May 2005 to 6 August 2007, short sales on pilot stocks were exempted from price tests, including the tick test for exchange-listed stocks and the bid test for NASDAQ National Market stocks.\nIn its initial order, the SEC stated that “the Pilot will allow [the SEC] to study trading behavior in the absence of a short sale price test.” The SEC’s plan was to “examine, among other things, the impact of price tests on market quality (including volatility and liquidity), whether any price changes are caused by short selling, costs imposed by a price test, and the use of alternative means to establish short positions.”\n19.2.1 The SHO pilot sample The assignment mechanism in the Reg SHO experiment is unusually transparent, even by the standards of natural experiments. Nonetheless care is needed to identify the treatment and control firms and we believe it is instructive to walk through the steps needed to do so, as we do in this section. (Readers who find the code details tedious could easily skip ahead to Section 19.2.3 on a first reading. We say “first reading” because there are subtle issues with natural experiments that this section helps to highlight, so it may be worth revisiting this section once you have read the later material.)\nThe SEC’s website provides data on the names and tickers of the Reg SHO pilot firms. These data have been parsed and included as the sho_tickers data set in the farr package.\n\nsho_tickers\n\n\n  \n\n\n\nHowever, these are just the pilot firms and we need to use other sources to obtain the identities of the control firms. It might seem perverse for the SEC to have published lists of treatment stocks, but no information on control stocks.4 One explanation for this choice might be that, because special action (i.e., elimination of price tests) was only required for the treatment stocks (for the control stocks, it was business as usual), no lists of controls were needed for the markets to implement the pilot. Additionally, because the SEC had a list of the control stocks that it would use in its own statistical analysis, it had no reason to publish lists for this purpose. Fortunately, while the SEC did not identify the control stocks, it provides enough information for us to do so, as we do below.\nFirst, we know that the pilot stocks were selected from the Russell 3000, the component stocks of which are found in the sho_r3000 data set from the farr package.\n\nsho_r3000\n\n\n  \n\n\n\nWhile the Russell 3000 contains 3,000 securities, the SEC and Black et al. (2019) tell us that, in constructing the pilot sample, the SEC excluded 32 stocks in the Russell 3000 index that, as of 25 June 2004, were not listed on the Nasdaq National Market, NYSE, or AMEX “because short sales in these securities are currently not subject to a price test.” The SEC also excluded 12 stocks that started trading after 30 April 2004 due to IPOs or spin-offs. And, from Black et al. (2019), we know there were two additional stocks that stopped trading after 25 June 2004 but before the SEC constructed its sample on 28 June 2004. We can get the data for each of these criteria from CRSP, but we need to first merge the Russell 3000 data with CRSP to identify the right PERMNO for each security. For this purpose, we will use data from the five CRSP tables below:\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nmse &lt;- tbl(db, Id(schema = \"crsp\", table = \"mse\"))\nmsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"msf\"))\nstocknames &lt;- tbl(db, Id(schema = \"crsp\", table = \"stocknames\"))\ndseexchdates &lt;- tbl(db, Id(schema = \"crsp\", table = \"dseexchdates\"))\nccmxpf_lnkhist &lt;- tbl(db, Id(schema = \"crsp\", table = \"ccmxpf_lnkhist\"))\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nmse &lt;- load_parquet(db, schema = \"crsp\", table = \"mse\")\nmsf &lt;- load_parquet(db, schema = \"crsp\", table = \"msf\")\nstocknames &lt;- load_parquet(db, schema = \"crsp\", table = \"stocknames\")\ndseexchdates &lt;- load_parquet(db, schema = \"crsp\", table = \"dseexchdates\")\nccmxpf_lnkhist &lt;- load_parquet(db, schema = \"crsp\", table = \"ccmxpf_lnkhist\")\n\n\n\n\nOne thing we note is that some of the tickers from the Russell 3000 sample append the class of stock to the ticker. We can detect these cases by looking for a dot (.) using regular expressions. Because a dot has special meaning in regular expressions (regex), we need to escape it using a backslash (). (For more on regular expressions, see Chapter 9 and references cited there.) Because a backslash has a special meaning in strings in R, we need to escape the backslash itself to tell R that we mean a literal backslash. In short, we use the regex \\. to detect dots in strings.\n\nsho_r3000 |&gt; \n  filter(str_detect(russell_ticker, \"\\\\.\"))\n\n\n  \n\n\n\nIn these cases, CRSP takes a different approach. For example, where the Russell 3000 sample has AGR.B, CRSP has ticker equal to AGR and shrcls equal to B.\nThe other issue is that some tickers from the Russell 3000 data have the letter E appended to what CRSP shows as just a four-letter ticker.\n\nsho_r3000 |&gt; \n  filter(str_length(russell_ticker) == 5,\n         str_sub(russell_ticker, 5, 5) == \"E\")\n\n\n  \n\n\n\nA curious reader might wonder how we identified these two issues with tickers, and how we know that they are exhaustive of the issues in the data. We explore these questions in the exercises at the end of this section.\nTo address these issues, we create two functions: one (clean_ticker()) to “clean” each ticker so that it can be matched with CRSP, and one (get_shrcls()) to extract the share class (if any) specified in the Russell 3000 data.\nBoth functions use a regular expression to match cases where the text ends with either “A” or “B” ([AB]$ in regex) preceded by the a dot (\\\\. in regex, as discussed above). The expression uses capturing parentheses (i.e., ( and )) to capture the text before the dot from the beginning of the string (^(.*)) to the dot and to capture the letter “A” or “B” at the end (([AB])$).\n\nregex &lt;- \"^(.*)\\\\.([AB])$\"\n\nThe clean_ticker() function uses case_when(), which first handles cases with an E at the end of five-letter tickers, then applies the regex to extract the “clean” ticker (the first captured text) from cases matching regex. Finally, the original ticker is returned for all other cases.\n\nclean_ticker &lt;- function(x) {\n  case_when(str_length(x) == 5 & str_sub(x, 5, 5) == \"E\" ~ str_sub(x, 1, 4),\n            str_detect(x, regex) ~ str_replace(x, regex, \"\\\\1\"),\n            .default = x)\n}\n\nThe get_shrcls() function extracts the second capture group from the regex (the first value returned by str_match() is the complete match, the second value is the first capture group, so we use [, 3] to get the second capture group).\n\nget_shrcls &lt;- function(x) {\n  str_match(x, regex)[, 3]\n}\n\nWe can use clean_ticker() and get_shrcls() to construct sho_r3000_tickers.\n\nsho_r3000_tickers &lt;-\n  sho_r3000 |&gt;\n  select(russell_ticker, russell_name) |&gt;\n  mutate(ticker = clean_ticker(russell_ticker),\n         shrcls = get_shrcls(russell_ticker))\n\nsho_r3000_tickers |&gt;\n  filter(russell_ticker != ticker)\n\n\n  \n\n\n\nNow that we have “clean” tickers, we can merge with CRSP. The following code proceeds in two steps. First, we create crsp_sample, which contains the permno, ticker, and shrcls values applicable on 2004-06-25, the date on which the Russell 3000 that the SEC used was created.\n\ncrsp_sample &lt;-\n  stocknames |&gt;\n  mutate(test_date = as.Date(\"2004-06-25\")) |&gt;\n  filter(test_date &gt;= namedt, test_date &lt;= nameenddt) |&gt;\n  select(permno, permco, ticker, shrcls) |&gt;\n  distinct() |&gt;\n  collect()\n\nSecond, we merge sho_r3000_tickers with crsp_sample using ticker and then use filter() to retain cases where, if a share class is specified in the SEC-provided ticker, it matches the one row in CRSP with that share class, while retaining all rows where no share class is specified in the SEC-provided ticker.\n\nsho_r3000_merged &lt;-\n  sho_r3000_tickers |&gt;\n  inner_join(crsp_sample, by = \"ticker\", suffix = c(\"\", \"_crsp\")) |&gt;\n  filter(shrcls == shrcls_crsp | is.na(shrcls)) |&gt;\n  select(russell_ticker, permco, permno)\n\nUnfortunately, this approach results in some tickers being matched to multiple PERMNO values.\n\nsho_r3000_merged |&gt;\n  group_by(russell_ticker) |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\nIn each case, these appear to be cases where there are multiple securities (permno values) for the same company (permco value). To choose the security that is the one most likely included in the Russell 3000 index used by the SEC, we will keep the one with the greatest dollar trading volume for the month of June 2004. We collect the data on dollar trading volumes in the data frame trading_vol.\n\ntrading_vol &lt;-\n  msf |&gt;\n  filter(date == \"2004-06-30\") |&gt; \n  mutate(dollar_vol = coalesce(abs(prc) * vol, 0)) |&gt; \n  select(permno, dollar_vol) |&gt;\n  collect()\n\nWe can now make a new version of the table sho_r3000_merged that includes just the permno value with the greatest trading volume for each ticker.\n\nsho_r3000_merged &lt;-\n  sho_r3000_tickers |&gt;\n  inner_join(crsp_sample, by = \"ticker\", suffix = c(\"\", \"_crsp\")) |&gt;\n  filter(is.na(shrcls) | shrcls == shrcls_crsp) |&gt;\n  inner_join(trading_vol, by = \"permno\") |&gt;\n  group_by(russell_ticker) |&gt;\n  filter(dollar_vol == max(dollar_vol, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(russell_ticker, permno)\n\nBlack et al. (2019) identify 32 stocks that are not listed on the Nasdaq National Market, NYSE, or AMEX firms “using historical exchange code (exchcd) and Nasdaq National Market Indicator (nmsind) from the CRSP monthly stock file” (in practice, these 32 stocks are smaller Nasdaq-listed stocks). However, exchcd and nmsind are not included in the crsp.msf file we use. Black et al. (2019) likely used the CRSP monthly stock file obtained from the web interface provided by WRDS, which often merges in data from other tables.\nFortunately, we can obtain nmsind from the CRSP monthly events file (crsp.mse). This file includes information about delisting events, distributions (such as dividends), changes in NASDAQ information (such as nmsind), and name changes. We get data on nmsind by pulling the latest observation on crsp.mse on or before 2004-06-28 where the event related to NASDAQ status (event == \"NASDIN\").\n\nnmsind_data &lt;-\n  mse |&gt; \n  filter(date &lt;= \"2004-06-28\", event == \"NASDIN\") |&gt;\n  group_by(permno) |&gt;\n  filter(date == max(date, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(permno, date, nmsind) |&gt;\n  collect()\n\nWe can obtain exchcd from the CRSP stock names file (crsp.stocknames), again pulling the value applicable on 2004-06-28.[^5]\n\nexchcd_data &lt;-\n  stocknames |&gt;\n  filter(exchcd &gt; 0) |&gt;\n  mutate(test_date = as.Date(\"2004-06-28\")) |&gt;\n  filter(between(test_date, namedt, nameenddt)) |&gt;\n  select(permno, exchcd) |&gt;\n  distinct() |&gt;\n  collect()\n\nAccording to its website, the SEC “also excluded issuers whose initial public offerings commenced after April 30, 2004.” Following Black et al. (2019), we use CRSP data to identify these firms. Specifically, the table crsp.dseexchdates includes the variable begexchdate.\n\nipo_dates &lt;-\n  dseexchdates |&gt; \n  distinct(permno, begexchdate) |&gt; \n  collect()\n\nFinally, it appears that there were stocks listed in the Russell 3000 file likely used by the SEC (created on 2004-06-25) that were delisted prior to 2004-06-28, the date on which the SEC appears to have finalized the sample for its pilot program. We again use crsp.mse to identify these firms.\n\nrecent_delistings &lt;-\n  mse |&gt;\n  filter(event == \"DELIST\", \n         between(date, \"2004-06-25\", \"2004-06-28\")) |&gt;\n  rename(delist_date = date) |&gt;\n  select(permno, delist_date) |&gt;\n  collect()\n\nNow, we put all these pieces together and create variables nasdaq_small, recent_listing, and delisted corresponding to the three exclusion criteria discussed above.\n\nsho_r3000_permno &lt;-\n  sho_r3000_merged |&gt;\n  left_join(nmsind_data, by = \"permno\") |&gt;\n  left_join(exchcd_data, by = \"permno\") |&gt;\n  left_join(ipo_dates, by = \"permno\") |&gt;\n  left_join(recent_delistings, by = \"permno\") |&gt;\n  mutate(nasdaq_small = coalesce(nmsind == 3 & exchcd == 3, FALSE), \n         recent_listing = begexchdate &gt; \"2004-04-30\",\n         delisted = !is.na(delist_date),\n         keep = !nasdaq_small & !recent_listing & !delisted)\n\nAs can be seen in Table 19.1, we have a final sample of 2954 stocks that we can merge with sho_tickers to create the pilot indicator.\n\nsho_r3000_permno |&gt; \n  count(nasdaq_small, recent_listing, delisted, keep)\n\n\nsho_r3000_sample &lt;-\n  sho_r3000_permno |&gt;\n  filter(keep) |&gt;\n  rename(ticker = russell_ticker) |&gt;\n  left_join(sho_tickers, by = \"ticker\") |&gt;\n  mutate(pilot = !is.na(co_name)) |&gt;\n  select(ticker, permno, pilot)\n\nAs can be seen the number of treatment and control firms in sho_r3000_sample corresponds exactly with the numbers provided in Black et al. (2019, p. 42).\n\nsho_r3000_sample |&gt;\n  count(pilot)\n\nFinally, we will want to link these data with data from Compustat, which means linking these observations with GVKEYs. For this, we use ccm_link (as used and discussed in Chapter 7) to produce sho_r3000_gvkeys, the sample we can use in later analysis.\nFinally, we will want to link these data with data from Compustat, which means linking these observations with GVKEYs. For this, we use ccm_link (as used and discussed in Chapter 7) to produce sho_r3000_gvkeys, the sample we can use in later analysis.\n\nccm_link &lt;-\n  ccmxpf_lnkhist |&gt;\n  filter(linktype %in% c(\"LC\", \"LU\", \"LS\"),\n         linkprim %in% c(\"C\", \"P\")) |&gt;\n  rename(permno = lpermno) |&gt;\n  mutate(linkenddt = coalesce(linkenddt, max(linkenddt, na.rm = TRUE))) |&gt;\n  select(gvkey, permno, linkdt, linkenddt)\n\nBecause we focus on a single “test date”, our final link table includes just two variables: gvkey and permno.[^6]\n\ngvkeys &lt;-\n  ccm_link |&gt;\n  mutate(test_date = as.Date(\"2004-06-28\")) |&gt;\n  filter(between(test_date, linkdt, linkenddt)) |&gt;\n  select(gvkey, permno) |&gt;\n  collect()\n\nFinally, we can add gvkey to sho_r3000_sample to create sho_r3000_gvkeys.\n\nsho_r3000_gvkeys &lt;-\n  sho_r3000_sample |&gt;\n  inner_join(gvkeys, by = \"permno\")\n\nsho_r3000_gvkeys\n\nTo better understand the potential issues with constructing the pilot indicator variable, it is useful to compare the approach above with that taken in Fang et al. (2016). To construct sho_data as Fang et al. (2016) do, we use fhk_pilot from the farr package.7 We compare sho_r3000_sample and sho_r3000_gvkeys with sho_data in the exercises below.\n\nsho_data &lt;- \n  fhk_pilot |&gt;\n  select(gvkey, pilot) |&gt;\n  distinct() |&gt;\n  group_by(gvkey) |&gt;\n  filter(n() == 1) |&gt;\n  ungroup() |&gt;\n  inner_join(fhk_pilot, by = c(\"gvkey\", \"pilot\")) \n\n\n11.2.1 Exercises\n\nBefore running the following code, can you tell from output above how many rows this query will return? What is this code doing? At what stage would code like this have been used in process of creating the sample above? Why is code like this not included above?\n\n\nsho_r3000 |&gt;\n  anti_join(crsp_sample, join_by(russell_ticker == ticker)) |&gt;\n  collect()\n\n\nFocusing on the values of ticker and pilot in fhk_pilot, what differences do you observe between fhk_pilot and sho_r3000_sample? What do you believe is the underlying cause for these discrepancies?\nWhat do the following observations represent? Choose a few observations from this output and examine whether these reveal issues in the sho_r3000_sample or in fhk_pilot.\n\n\nsho_r3000_sample |&gt;\n  inner_join(fhk_pilot, by = \"ticker\", suffix = c(\"_ours\", \"_fhk\")) |&gt;\n  filter(permno_ours != permno_fhk)\n\n\nIn constructing the pilot indicator, FHK omit cases (gvkey values) where there is more than one distinct value for the indicator. A question is: Who are these firms? Why is there more than one value for pilot for these firms? And does omission of these make sense? (Hint: Identify duplicates in fhk_pilot and compare sho_r3000_gvkeys for these firms.)\nWhat issue is implicit in the output from the code below? How could you fix this issue? Would you expect a fix for this issue to significantly affect the regression results? Why or why not?\n\n\nsho_data |&gt; \n  count(gvkey, ticker) |&gt; \n  arrange(desc(n))\n\n\n11.2.2 Early studies of Reg SHO\nThe first study of the effects of Reg SHO was conducted by the SEC’s own Office of Economic Analysis. The SEC study examines the “effect of pilot on short selling, liquidity, volatility, market efficiency, and extreme price changes” [p. 86].\nThe authors of the 2007 SEC study “find that price restrictions reduce the volume of executed short sales relative to total volume, indicating that price restrictions indeed act as a constraint to short selling. However, in neither market do we find significant differences in short interest across pilot and control stocks. … We find no evidence that short sale price restrictions in equities have an impact on option trading or open interest. … We find that quoted depths are augmented by price restrictions but realized liquidity is unaffected. Further, we find some evidence that price restrictions dampen short term within-day return volatility, but when measured on average, they seem to have no effect on daily return volatility.”\nThe SEC researchers conclude “based on the price reaction to the initiation of the pilot, we find limited evidence that the tick test distorts stock prices—on the day the pilot went into effect, Listed Stocks in the pilot sample underperformed Listed Stocks in the control sample by approximately 24 basis points. However, the pilot and control stocks had similar returns over the first six months of the pilot.”\nIn summary, it seems fair to say that the SEC found that exemption from price tests had relatively limited effect on the market outcomes of interest, with no apparent impact on several outcomes.\nAlexander and Peterson (2008, p. 84) “examine how price tests affect trader behavior and market quality, which are areas of interest given by the [SEC] in evaluating these tests.” Alexander and Peterson (2008, p. 86) find that NYSE pilot stocks have similar spreads, but smaller trade sizes, more short trades, more short volume, and smaller ask depths. With regard to Nasdaq, Alexander and Peterson (2008, p. 86) find that the removed “bid test is relatively inconsequential.”\nDiether et al. (2009, p. 37) find that “while short-selling activity increases both for NYSE- and Nasdaq-listed Pilot stocks, returns and volatility at the daily level are unaffected.”\n\n11.2.3 Discussion questions and exercises\n\nEarlier we identified one feature of a randomized controlled trial (RCT) as that “proposed analyses are specified in advance”, as in a registered reports process. Why do you think the SEC did not use a registered report for its 2007 paper? Do you think the analyses of the SEC would be more credible if conducted as part of a registered reports process? Why or why not?\nDo you have concerns that the results Alexander and Peterson (2008) have been p-hacked? What factors increase or reduce your concerns in this regard?\nEvaluate the hypotheses found in the section of Diether et al. (2009, pp. 41–45) entitled Testable Hypotheses with particular sensitivity to concerns about HARKing. What kind of expertise is necessary in evaluating hypotheses in this way?\nHow might the SEC have conducted Reg SHO as part of a registered reports process open to outside research teams, such as Alexander and Peterson (2008) and Diether et al. (2009)? How might such a process have been run? What challenges would such a process face?",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#analysing-natural-experiments",
    "href": "chap19_Natural_experiments_revisited.html#analysing-natural-experiments",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.3 Analysing natural experiments",
    "text": "11.3 Analysing natural experiments\nBoth Alexander and Peterson (2008) and Diether et al. (2009) use the difference-in-differences estimator (“DiD”) of the causal effect that we saw in Chapter 3. The typical approach to DiD involves estimating a regression of the following form:\n\n\\begin{aligned}\nY_{it} = &\\beta_0 + \\beta_1 \\times POST_t + \\beta_2 \\times TREAT _i + \\\\\n&\\beta _3 \\times POST_t \\times TREAT_i + \\varepsilon_{it}\n\\end{aligned}\n\nIn this specification, the estimated treatment effect is given by the fitted coefficient \\hat{\\beta} _3 .\nWhile DiD is clearly popular among researchers in economics and adjacent fields, it is important to note that it is not obvious that it is the best choice in every experimental setting and that credible alternatives exist.\nAnother approach would be to limit the sample to the post-treatment period and estimate the following regression:\n\nY_{it} = \\beta_0 + \\beta_1 \\times TREAT_i + \\varepsilon_{it}\n\nIn this specification, the estimated treatment effect is given by the fitted coefficient \\hat{\\beta}_1 . This approach is common in drug trials, which are typically conducted as RCTs. For example, in the paxlovid trial “participants were randomised 1:1, with half receiving paxlovid and the other half receiving a placebo orally every 12 hours for five days. Of those who were treated within three days of symptom onset, 0.8% (3/389) of patients who received paxlovid were admitted to hospital up to day 28 after randomization, with no deaths. In comparison, 7% (27/385) of patients who received placebo were admitted, with seven deaths.” (Mahase, 2021, p. 1). For the hospital admission outcome, it would have been possible to incorporate prior hospitalization rates in a difference-in-differences analysis, but this would only make sense if hospitalization rates in one period had a high predictive power for subsequent hospitalization rates.[^8]\nYet another approach would include pre-treatment values of the outcome variable as a control:\n\nY_{it} = \\beta_0 + \\beta_1 \\times TREAT _i + \\beta_2 \\times Y_{i,t-1} + \\varepsilon_{it}\n\nTo evaluate each of these approaches, we can use simulation analysis. The following analysis is somewhat inspired by Frison and Pocock (1992), who use different assumptions about their data more appropriate to their (medical) setting and who focus on mathematical analysis instead of simulations.\nFrison and Pocock (1992) assume a degree of correlation in measurements of outcome variables for a given unit (e.g., patient) that is independent of the time between observations. A more plausible model in many business settings would be correlation in outcome measures for a given unit (e.g., firm) that fades as observations become further apart in time. Along these lines, we create get_outcomes() below to generate data for outcomes in the absence of treatment. Specifically, we assume that, if there are no treatment or period effects, the outcome in question follows the autoregressive process embedded in get_outcomes(), which has the key parameter \\rho (rho).[^9]\n\nget_outcomes &lt;- function(rho = 0, periods = 7) {\n  e &lt;- rnorm(periods)\n  y &lt;- rep(NA, periods)\n  y[1] &lt;- e[1]\n  for (i in 2:periods) {\n    y[i] &lt;- rho * y[i - 1] + e[i]\n  }\n  tibble(t = 1:periods, y = y)\n}\n\nThe get_sample() function below uses get_outcomes() for n firms for given values of rho, periods (the number of periods observed for each firm), and effect (the underlying size of the effect of treatment on y). Here treatment is randomly assigned to half the firms in the sample and the effect is added to y when both treat and post are true. We also add a time-specific effect (t_effect) for each period, which is common to all observations (a common justification for the use of DiD is the existence of such period effects).\n\nget_sample &lt;- function(n = 100, rho = 0, periods = 7, effect = 0) {\n  treat &lt;- sample(1:n, size = floor(n / 2), replace = FALSE)\n  \n  t_effects &lt;- tibble(t = 1:periods, t_effect = rnorm(periods))\n  \n  f &lt;- function(x) tibble(id = x, get_outcomes(rho = rho, \n                                               periods = periods))\n  df &lt;- \n    map(1:n, f) |&gt;\n    list_rbind() |&gt; \n    inner_join(t_effects, by = \"t\") |&gt;\n    mutate(treat = id %in% treat,\n           post = t &gt; periods / 2,\n           y = y + if_else(treat & post, effect, 0) + t_effect) |&gt;\n    select(-t_effect)\n}\n\nThe est_effect() function below applies a number of estimators to a given data set and returns the estimated treatment effect for each estimator. The estimators we consider are the following (the labels POST, CHANGE, and ANCOVA come from Frison and Pocock, 1992):\n\n\nDiD, the difference-in-differences estimator estimated by regressing y on the treatment indicator, treat interacted with the post-treatment indicator, post (with the [lm()](https://rdrr.io/r/stats/lm.html) function automatically including the main effects of treat and post), as in Equation 19.1.\n\nPOST, which is based on OLS regression of y on treat, but with the sample restricted to the post-treatment observations, as in Equation 19.2.\n\nCHANGE, which is based on OLS regression of the change in the outcome on treat. The change in outcome (y_change) is calculated as the mean of post-treatment outcome value (y_post) minus the mean of the pre-treatment outcome value (y_pre) for each unit.\n\nANCOVA, which is a regression of y_post on treat and y_pre, as in Equation 19.3.\n\n\nest_effect &lt;- function(df) {\n  \n  fm_DiD &lt;- lm(y ~ treat * post, data = df)\n  \n  df_POST &lt;- \n    df |&gt; \n    filter(post) |&gt;\n    group_by(id, treat) |&gt;\n    summarize(y = mean(y), .groups = \"drop\")\n    \n  fm_POST &lt;- lm(y ~ treat, data = df_POST)\n  \n  df_CHANGE &lt;- \n    df |&gt; \n    group_by(id, treat, post) |&gt;\n    summarize(y = mean(y), .groups = \"drop\") |&gt;\n    pivot_wider(names_from = \"post\", values_from = \"y\") |&gt;\n    rename(y_pre = `FALSE`,\n           y_post = `TRUE`) |&gt;\n    mutate(y_change = y_post - y_pre) \n  \n  fm_CHANGE &lt;- lm(I(y_post - y_pre) ~ treat, data = df_CHANGE)\n  fm_ANCOVA &lt;- lm(y_post ~ y_pre + treat, data = df_CHANGE)\n  \n  tibble(est_DiD = fm_DiD$coefficients[[\"treatTRUE:postTRUE\"]],\n         est_POST = fm_POST$coefficients[[\"treatTRUE\"]], \n         est_CHANGE = fm_CHANGE$coefficients[[\"treatTRUE\"]], \n         est_ANCOVA = fm_ANCOVA$coefficients[[\"treatTRUE\"]])\n}\n\nThe run_sim() function below calls get_sample() for supplied parameter values to create a data set, and returns a data frame containing the results of applying est_effect() to that data set.\n\nrun_sim &lt;- function(i, n = 100, rho = 0, periods = 7, effect = 0) {\n  df &lt;- get_sample(n = n, rho = rho, periods = periods, effect = effect)\n  tibble(i = i, est_effect(df))\n}\n\nTo facilitate running of the simulation for various values of effect and rho, we create a data frame (params) with effect sizes running from 0 to 1 and \\rho \\in \\{ 0, 0.18, 0.36, 0.54, 0.72, 0.9 \\} .\n\nrhos &lt;- seq(from = 0, to = 0.9, length.out = 6) \neffects &lt;- seq(from = 0, to = 1, length.out = 5)\nparams &lt;- expand_grid(effect = effects, rho = rhos)\n\nThe run_sim_n() function below runs 1000 simulations for the supplied values of effect and rho and returns a data frame with the results.\n\nrun_sim_n &lt;- function(effect, rho, ...) {\n  n_sims &lt;- 1000\n  set.seed(2021)\n  \n  res &lt;- \n    1:n_sims |&gt;\n    map(\\(x) run_sim(x, rho = rho, effect = effect)) |&gt;\n    list_rbind()\n  \n  tibble(effect, rho, res)\n                                    \n}\n\n\n\n\n\n\n\nThe following code takes several minutes to run. Using future_pmap from the furrr package in place of pmap() reduces the time needed to run the simulation significantly. Fortunately, nothing in the subsequent exercises requires that you run either variant of this code, so only do so if you have time and want to examine results directly.\n\n\n\n\nplan(multisession)\n\nresults &lt;-\n  params |&gt; \n  future_pmap(run_sim_n, \n              .options = furrr_options(seed = 2021)) |&gt; \n  list_rbind() |&gt; \n  system_time()\n\nWith results in hand, we can do some analysis. The first thing to note is that est_CHANGE is equivalent to est_DiD, as all estimates are within rounding error of each other for these two methods.\n\nresults |&gt; filter(abs(est_DiD - est_CHANGE) &gt; 0.00001) |&gt; nrow()\n\nThus we just use the label DiD in subsequent analysis.\nThe second thing we check is that the methods provide unbiased estimates of the causal effect. Figure 19.1 suggests that the estimates are very close to the true values of causal effects for all three methods.\n\nresults |&gt;\n  pivot_longer(starts_with(\"est\"), \n               names_to = \"method\", values_to = \"est\") |&gt;\n  mutate(method = str_replace(method, \"^est.(.*)$\", \"\\\\1\")) |&gt;\n  group_by(rho, method) |&gt;\n  summarize(bias = mean(est - effect), .groups = \"drop\") |&gt;\n  filter(method != \"CHANGE\") |&gt;\n  ggplot(aes(x = rho, y = bias, \n             colour = method, linetype = method)) +\n  geom_line() +\n  ylim(-0.1, 0.1)\n\nHaving confirmed that there is no apparent bias in any of the estimators in this setting, we next consider the empirical standard errors for each method. Because we get essentially identical plots with each value of the true effect, we focus on effect == 0.5 in the following analysis. Here we rearrange the data so that we have a method column and an est column for the estimated causal effect. We then calculate, for each method and value of rho, the standard deviation of est, which is the empirical standard error we seek. Finally, we plot the values for each value of rho in Figure 19.2.\n\nresults |&gt;\n  filter(effect == 0.5) |&gt;\n  pivot_longer(starts_with(\"est\"), \n               names_to = \"method\", values_to = \"est\") |&gt;\n  mutate(method = str_replace(method, \"^est.(.*)$\", \"\\\\1\")) |&gt;\n  filter(method != \"CHANGE\") |&gt;\n  group_by(method, rho) |&gt;\n  summarize(se = sd(est), .groups = \"drop\") |&gt;\n  ggplot(aes(x = rho, y = se, \n             colour = method, linetype = method)) +\n  geom_line()\n\nFrom the above, we can see that for low values of \\rho , subtracting pre-treatment outcome values adds noise to our estimation of treatment effects. We actually have lower standard errors when we throw away the pre-treatment data and just compare post-treatment outcomes. But for higher levels of \\rho , we see that DiD outperforms POST; by subtracting pre-treatment outcome values, we get a more precise estimate of the treatment effect. However, we see that both DiD and POST are generally outperformed by ANCOVA, which in effect allows for a flexible, data-driven relation between pre- and post-treatment outcome values.\nIn short, notwithstanding its popularity, it is far from clear that DiD is the best approach to use for all analyses of causal effects based on experimental data. Even in the context of the Reg SHO experiment, the appropriate method may depend on the outcome of interest. For a persistent outcome, DiD may be better than POST, but for a less persistent outcome, POST may be better than DiD. And ANCOVA may be a better choice than either POST or DiD unless there are strong a priori reasons to believe that DiD or POST is more appropriate (and such reasons seem more likely to hold for POST than for DiD).",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#evaluating-natural-experiments",
    "href": "chap19_Natural_experiments_revisited.html#evaluating-natural-experiments",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.4 19.4 Evaluating natural experiments",
    "text": "11.4 19.4 Evaluating natural experiments\nBecause of the plausibly random assignment mechanism used by the SEC, Reg SHO provides a very credible natural experiment. However, in many claimed natural experiments, it will be “nature” who is assigning treatment. In Michels (2017), it was literally nature doing the assignment through the timing of natural disasters. While natural disasters are clearly not completely random, as hurricanes are more likely to strike certain locations at certain times of year, this is not essential for the natural experiment to provide a setting from which causal inferences can be credibly drawn. What is necessary in Michels (2017) is that treatment assignment is as-if random with regard to the timing of the natural disaster before or after the end of the fiscal period and discussion questions in Chapter 17 explored these issues.\nMore often in claimed natural experiments, it will be economic actors or forces, rather than nature, assigning treatment. While such economic actors and forces are unlikely to act at random, again the critical question is whether treatment assignment is as-if random. To better understand the issues, we consider a well-studied setting, that of brokerage closures as studied in Kelly and Ljungqvist (2012).\nKelly and Ljungqvist (2012, p. 1368) argue that “brokerage closures are a plausibly exogenous source of variation in the extent of analyst coverage, as long as two conditions are satisfied. First, the resulting coverage terminations must correlate with an increase in information asymmetry. … Second, the terminations must only affect price and demand through their effect on information asymmetry.” Interestingly, these are essentially questions 2 and 3 that we ask in evaluating instrumental variables in Section 20.3. The analogue with instrumental variables applies because Kelly and Ljungqvist (2012) are primarily interested in the effects of changes in information asymmetry, not the effects of brokerage closures per se. In principle, brokerage closures could function much like an instrumental variable, except that Kelly and Ljungqvist (2012) estimate reduced-form regressions whereby outcomes are related directly to brokerage closures, such as in Table 3 (Kelly and Ljungqvist, 2012, p. 1391). But the first of the three questions from Section 20.3 remains relevant, and this is the critical question for evaluating any natural experiment: Is treatment assignment (as-if) random?\nLike many researchers, Kelly and Ljungqvist (2012) do not address the (as-if) randomness of treatment assignment directly. Instead, Kelly and Ljungqvist (2012) focus on whether brokerage closure-related terminations of analyst coverage “constitute a suitably exogenous shock to the investment environment”. Kelly and Ljungqvist (2012) argue that they do “unless brokerage firms quit research because their analysts possessed negative private information about the stocks they covered.” But this reasoning is incomplete. For sure, brokerage firms not quitting research for the reason suggested is a necessary condition for a natural experiment (otherwise the issues with using brokerage closures as a natural experiment are quite apparent). But it is not a sufficient condition. If the firms encountering brokerage closure-related terminations of analyst coverage had different trends in information asymmetry for other reasons, the lack of private information is inadequate to give us a natural experiment.\nIn general, we should be able to evaluate the randomness of treatment assignment much as we would do so with an explicitly randomized experiment. Burt (2000) suggest that “statisticians will compare the homogeneity of the treatment group populations to assess the distribution of the pretreatment demographic characteristics and confounding factors.” With explicit randomization, statistically significant differences in pretreatment variables might prompt checks to ensure that, say, there was not “deliberate alteration of or noncompliance with the random assignment code” or any other anomalies. Otherwise, we might have greater confidence that randomization was implemented effectively, and hence that causal inferences might reliably be drawn from the study.\nSo, a sensible check with a natural experiment would seem to be to compare various pre-treatment variables across treatment groups to gain confidence that “nature” has indeed randomized treatment assignment. In this regard, Table 1 of Kelly and Ljungqvist (2012) is less than assuring. Arguably, one can only encounter brokerage closure-related terminations of analyst coverage if one has analyst coverage in the first place; so the relevant comparison is arguably between the first and third columns of data. There we see that the typical firm in the terminations sample (column 1) is larger, has higher monthly stock turnover, higher daily return volatility, and more brokers covering the stock than does the typical firm in the universe of covered stocks in 2004 (column 3). So clearly “nature” has not randomly selected firms from the universe of covered stocks in 2004.\nHowever, we might come to a similar conclusion if we compared the Reg SHO pilot stocks with the universe of traded stocks or some other broad group. Just as it was essential to correctly identify the population that the SEC considered in randomizing treatment assignment, it is important to identify the population that “nature” considered in assigning treatment in evaluating any natural experiment. While the SEC provided a statement detailing how it constructed the sample, “nature” is not going to do the same and researchers need to consider carefully which units were considered for (as if) random assignment to treatment.\nIn this regard, even assuming that the controls used in Table 2 of Kelly and Ljungqvist (2012, p. 1388) were the ones “nature” herself considered, it seems that the natural experiment did not assign treatment in a sufficiently random way. Table 2 studies four outcomes: bid-ask spreads, the Amihud illiquidity measure, missing- and zero-return days, and measures related to earnings announcements. In each case, there are pre-treatment differences that sometimes exceed the DiD estimates. For example, pre-treatment bid-ask spreads for treatment and control firms are 1.126 and 1.089, a 0.037 difference that is presumably statistically significant given that the smaller DiD estimate of 0.020 has a p-value of 0.011.10 In light of this evidence, it seems that Kelly and Ljungqvist (2012) need to rely on the parallel trends assumption to draw causal inferences and we evaluate the plausibility of this assumption in the next section.\nIt is important to recognize that the shortcomings of broker closures as a natural experiment do not completely undermine the ability of Kelly and Ljungqvist (2012) to draw causal inferences. There appears to be an unfortunate tendency to believe, on the one hand, that without some kind of natural experiment, one cannot draw causal inferences. On the other hand, there is an apparent tendency to view natural experiments as giving carte blanche to researchers to draw all kinds of causal inferences, even when the alleged identification strategies do not, properly understood, support such inferences.\nIn the case of Kelly and Ljungqvist (2012), it seems the authors would like to believe that they have a natural experiment that allows them to draw inferences about the effects of broker closures on information asymmetry (Table 2) and, because broker closures only affect stock prices through their effects on information asymmetry, to conclude from the evidence in Table 3 that increases in information asymmetry reduce stock prices. But Table 2 could have been based on a bullet-proof identification strategy without implying that broker closures only affect stock prices through their effects on information asymmetry. There is really no evidence offered for this claim, one that is arguably very difficult to support.\nAt the same time, it is conceptually possible that Kelly and Ljungqvist (2012) could provide compelling evidence that the only plausible explanation for the abnormal returns in Table 3 is reductions in information asymmetry, even if the results in Table 2 were irredeemably corrupted (e.g., because of failure of parallel trends). Evidence that firms did not “quit research because their analysts possessed negative private information about the stocks they covered” might support drawing certain inferences from Table 3 even without a credible equivalent of Table 2.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#the-parallel-trends-assumption",
    "href": "chap19_Natural_experiments_revisited.html#the-parallel-trends-assumption",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.5 The parallel trends assumption",
    "text": "11.5 The parallel trends assumption\nExamining the studies of the direct effects of Reg SHO discussed in Section 19.2.3, we see that randomization generally provided balance in pre-treatment outcome values. In such settings, we see that DiD can provide unbiased estimates of causal effects, but that it has little appeal when treatment assignment is random. Indeed, if there is little to distinguish treatment and control observations in terms of pre-treatment outcome values, DiD will differ little from simply comparing differences in post-treatment means (the POST estimator discussed above).\nBut in many settings, such as that in Kelly and Ljungqvist (2012), differences in pre-treatment outcome values exist, suggesting that random assignment is not an appropriate assumption. In such settings, it will therefore be necessary to rely on a different assumption to justify the use of DiD for causal inferences. This parallel trends assumption posits that, but for treatment, the expected change in the outcome variable for the treated observations would equal that for control observations. Using this assumption, we can attribute any difference in the change in the outcome variable between the treated and control observations to a treatment effect and random variation and use standard tools of statistical inference to evaluate the null hypothesis that any difference is due to random variation.\nThat DiD can be predicated on an assumption other than (as-if) random assignment may explain its popularity. Cunningham (2021, p. 406) suggests that DiD “has become the single most popular research design in the quantitative social sciences” and Armstrong et al. (2022, p. 4) find rapid “increase in [the number of] papers using quasi-experimental methods to draw causal inferences, that more than 75% of such papers use variations of the classic difference-in-differences design.”\nArguably DiD is more often used in settings that without (as-if) random assignment of treatment. For example, one of the most highly cited papers using DiD is Card and Krueger (1994), which compares the change in employment in the fast-food industry in New Jersey and Philadelphia before and after an increase in the minimum wage in New Jersey. In this case, treatment assignment was very clearly non-random—it was a function of being located in New Jersey.\nUnlike the assumption of random assignment, the parallel trends assumption is not implied by a reasonable description of a physical or economic process and thus is of a fundamentally different nature. Random assignment is widely regarded as a reasonable description of, say, a coin toss or the generation of pseudo-random numbers using a computer. In contrast, it is difficult to think of a mechanism for imposing parallel trends on the data. Because DiD is—unlike instrumental variables or regression discontinuity designs—generally not predicated on “as-if random variation in the explanatory variable of interest”, it is not correct to consider DiD as a quasi-experimental method (Armstrong et al., 2022, p. 3).11\nInstead the basis for the parallel trends assumption appears to be that it is the assumption that is necessary (and sufficient) for the DiD estimator to provide an unbiased estimator of the causal effect of treatment. But “assuming a can-opener” seems to be a weak foundation for an approach as widespread as DiD.\nOn the one hand, as discussed above, there is no obvious economic basis for the parallel trends assumption with general applicability. On the other hand, there are often reasons to believe that the parallel trends assumption will not hold for various outcomes. The parallel trends assumption will be dubious when the outcome variable tends to be mean-reverting. For example, it is well known that accounting-based measures of operating performance tend to revert towards the mean. So if treatment and control observations have different levels of pre-treatment operating performance, the parallel trends assumption will be a highly dubious basis for causal inference.\nAnother reason to doubt the parallel trends assumption is the fact that the measurement of outcomes is often arbitrary. For example, Li et al. (2018) examine the effect of legal changes on disclosure of customer identities using a variant of DiD.12 One primary outcome measure considered by Li et al. (2018) is (), the proportion of significant customers whose identities are not disclosed. But if the parallel trends assumption holds in () then, so long as there are pre-treatment differences between treatment and control observations, it is not mathematically possible for parallel trends to hold in ((1 + )), which is the measure used in the regression analysis in Li et al. (2018).\nThe apparent flimsiness of the parallel trends assumption underlying DiD analysis in non-randomized settings is perhaps reinforced by the treatment of DiD in textbooks. Imbens and Rubin (2015), a significant recent tome on causal inference, buries DiD in endnotes, merely noting that DiD is “widely used” (2015, p. 44) before directing the reader to Angrist and Pischke (2008). While Angrist and Pischke (2008) discuss DiD and its assumptions, and relate it to fixed-effects regressions and panel data methods, they do little to justify the parallel trends assumption. Cunningham (2021) is much more cautious in discussing the parallel trends assumption, which he notes “is by definition untestable since we cannot observe this counterfactual conditional expectation [of post-treatment outcomes absent treatment]”.\nTwo popular approaches to address the parallel trends assumption are discussed by Cunningham (2021) and Huntington-Klein (2021). The first approach compares the trends in pre-treatment outcome values for treatment and control observations. If these trends are similar before treatment, it is perhaps reasonable to assume they are similar after treatment. But Cunningham (2021, p. 426) notes that “pre-treatment similarities are neither necessary nor sufficient to guarantee parallel counterfactual trends” and this seems an especially dubious assumption if treatment is endogenously selected.\nThe second approach is the placebo test, variants of which are discussed by Cunningham (2021) and Huntington-Klein (2021). One placebo test involves evaluating the treatment effect in a setting where prior beliefs hold that there should be no effect. Another approach involves a kind of random assignment of a pseudo-treatment. In either case, not finding an effect is considered as providing support for the parallel trends assumption in the analysis of greater interest to the researcher. Of course, one might be sceptical of such placebo tests in light of the concerns raised at the start of this chapter. If applying DiD to state-level data on spending on science, space, and technology provides evidence of an effect on suicides by hanging, strangulation, and suffocation, not finding an effect on deaths by drowning after falling out of a canoe or kayak may provide limited assurance.\nTo illustrate, we now apply a kind of placebo test to evaluate the parallel trends assumption for bid-ask spreads, one of the variables considered in the DiD analysis of Table 2 of Kelly and Ljungqvist (2012) (we choose spreads in part because it is easy to calculate).\nWe first create the data set spreads, which contains data on the average spread for stocks over three-month periods—aligning with one measure used Table 2 of Kelly and Ljungqvist (2012)—for a sample period running from Q1, 2001 (the first quarter of 2001) to Q1, 2008, which is the sample period in Kelly and Ljungqvist (2012). We will conduct a study of a pseudo-treatment that we will assume applies for periods beginning in Q1, 2004, which is roughly halfway through the sample period and we code post accordingly.\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\ndsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"dsf\"))\n\nspreads &lt;-\n  dsf |&gt;\n  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),\n         quarter = as.Date(floor_date(date, 'quarter'))) |&gt;\n  group_by(permno, quarter) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(post = quarter &gt;= \"2004-01-01\") |&gt;\n  filter(!is.na(spread), \n         between(quarter, \"2000-01-01\", \"2008-01-01\")) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\ndsf &lt;- load_parquet(db, schema = \"crsp\", table = \"dsf\")\n\nspreads &lt;-\n  dsf |&gt;\n  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),\n         quarter = as.Date(floor_date(date, 'quarter'))) |&gt;\n  group_by(permno, quarter) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(post = quarter &gt;= \"2004-01-01\") |&gt;\n  filter(!is.na(spread), \n         between(quarter, \"2000-01-01\", \"2008-01-01\")) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\nWe now randomize treatment assignment. Because we want to evaluate the parallel trends assumption and completely randomized treatment assignment implies a trivial version of the parallel trends assumption, we specify a small difference in the probability of receiving treatment for observations whose pre-treatment spread exceeds the median ((p = 0.55)) from those whose pre-treatment spread is below the median ((p = 0.45)). This ensures that we have pre-treatment differences and thus need to rely on the parallel trends assumption in a meaningful way.13\n\nset.seed(2021)\n\ntreatment &lt;-\n  spreads |&gt;\n  filter(!post) |&gt;\n  group_by(permno) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(treat_prob = if_else(spread &gt; median(spread), 0.55, 0.45),\n         rand = runif(n = nrow(pick(everything()))),\n         treat = rand &lt; treat_prob) |&gt;\n  select(permno, treat)\n\nObviously the null hypothesis of zero treatment effect holds with this “treatment”, but the question is whether the parallel trends assumption holds for spread. If we find evidence of a “treatment effect”, the only sensible interpretation is a failure of the parallel trends assumption for spread.\nMerging in the treatment data set, we estimate a DiD regression (and cluster standard errors by permno for reasons that will be apparent after reading the discussion below). Results are reported in Table 19.2.\n\nreg_data &lt;-\n  spreads |&gt;\n  inner_join(treatment, by = \"permno\") \n\nfm &lt;- feols(spread ~ post * treat, vcov = ~ permno, data = reg_data)\n\n\nmodelsummary(fm,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\nBecause we find a statistically significant effect of -0.343 with a t-statistic of -4.96 with this meaningless “treatment”, we can conclude with some confidence that the parallel trends assumption simply does not hold for spread in this sample. Given that we might have passed this placebo test even if the parallel trends assumption did not hold for a particular treatment, say due to endogenous selection, it seems reasonable to view this test as being better suited to detecting a failure of parallel trends (as it does here) than it is to validation of that assumption.\nCunningham (2021) and Huntington-Klein (2021) provide excellent pathways to a recent literature examining DiD. However, it is important to recognize that some variant of the scientifically flimsy parallel trends assumption imbues all of these treatments. It would seem to be productive for researchers to discard the “quasi-experimental” pretence attached to DiD and to apply techniques appropriate to what some call interrupted time-series designs (e.g., Shadish et al., 2002).14\nWhile a randomized experiment provides a sound basis for attributing observed differences in outcomes to either treatment effects or sampling variation, without such randomization, it is perhaps more appropriate to take a more abductive approach of identifying causal mechanisms, deeper predictions about the timing and nature of causal effects, explicit consideration of alternative explanations, and the like (Armstrong et al., 2022; Heckman and Singer, 2017). Some evidence of this is seen in the discussion of specific papers in Cunningham (2021), perhaps reflecting reluctance to lean too heavily on the parallel trends assumption.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#indirect-effects-of-reg-sho",
    "href": "chap19_Natural_experiments_revisited.html#indirect-effects-of-reg-sho",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.6 Indirect effects of Reg SHO",
    "text": "11.6 Indirect effects of Reg SHO\nThe early studies of Reg SHO discussed above can be viewed as studying the more direct effects of Reg SHO. As a policy change directly affecting the ability of short-sellers to trade in securities, the outcomes studied by these earlier studies are more closely linked to the Reg SHO pilot than are the outcomes considered in later studies. Black et al. (2019, pp. 2–3) point out that “despite little evidence of direct impact of the Reg SHO experiment on pilot firms, over 60 papers in accounting, finance, and economics report that suspension of the price tests had wide-ranging indirect effects on pilot firms, including on earnings management, investments, leverage, acquisitions, management compensation, workplace safety, and more.”\nOne indirect effect of short-selling that has been studied in subsequent research is that on earnings management. To explore this topic, we focus on Fang et al. (2016, p. 1251), who find “that short-selling, or its prospect, curbs earnings management.”\n\n11.6.1 Earnings management after Dechow et al. (1995)\nIn Chapter 16, we saw that early earnings management research used firm-specific regressions in estimating standard models such as the Jones (1991) model. Fang et al. (2016) apply subsequent innovations in measurement of earnings management, such the performance-matched discretionary accruals measure developed in Kothari et al. (2005).\nKothari et al. (2005) replace the firm-specific regressions seen in Dechow et al. (1995) with regressions by industry-year, where industries are defined as firms grouped by two-digit SIC codes. Kothari et al. (2005) also add an intercept term to the Jones Model and estimate discretionary accruals under the Modified Jones Model by applying the coefficients from the Jones Model to the analogous terms of the Modified Jones Model.15\nTo calculate performance-matched discretionary accruals, Kothari et al. (2005, p. 1263) “match each sample firm with the firm from the same fiscal year-industry that has the closest return on assets as the given firm. The performance-matched discretionary accruals … are then calculated as the firm-specific discretionary accruals minus the discretionary accruals of the matched firm.” This is the primary measure of earnings management used by Fang et al. (2016), but note that Fang et al. (2016) use the 48-industry Fama-French groupings, rather than the two-digit SIC codes used in Kothari et al. (2005).\n\n11.6.2 FHK: Data steps\nTo construct measures of discretionary accruals, Fang et al. (2016) obtain data primarily from Compustat, along with data on Fama-French industries from Ken French’s website and data on the SHO pilot indicator from the SEC’s website. The following code is adapted from code posted by the authors of Fang et al. (2016), which was used to produce the results found in Tables 15 and 16 of Fang et al. (2019). The bulk of the Compustat data used in Fang et al. (2016) come from comp.funda. Following Fang et al. (2019), the code below collects data from that table for fiscal years between 1999 and 2012, excluding firms with SIC codes between 6000 and 6999 or between 4900 and 4949.\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nfunda &lt;- tbl(db, Id(schema = \"comp\", table = \"funda\"))\n\ncompustat_annual &lt;-\n  funda |&gt;\n  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', \n         between(fyear, 1999, 2012),\n         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |&gt;\n  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,\n         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, \n         rect, ceq, csho, prcc_f) |&gt;\n  mutate(fyear = as.integer(fyear)) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nfunda &lt;- load_parquet(db, schema = \"comp\", table = \"funda\")\n\ncompustat_annual &lt;-\n  funda |&gt;\n  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', \n         between(fyear, 1999, 2012),\n         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |&gt;\n  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,\n         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, \n         rect, ceq, csho, prcc_f) |&gt;\n  mutate(fyear = as.integer(fyear)) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\nSome regressions in Fang et al. (2019) consider controls for market-to-book, leverage, and return on assets, which are calculated as mtob, leverage, and roa, respectively, in the following code:\n\ncontrols_raw &lt;-\n  compustat_annual |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  mutate(lag_fyear = lag(fyear),\n         mtob = if_else(lag(ceq) != 0, \n                        lag(csho) * lag(prcc_f) / lag(ceq), NA),\n         leverage = if_else(dltt + dlc + seq != 0, \n                            (dltt + dlc) / (dltt + dlc + seq), NA),\n         roa = if_else(lag(at) &gt; 0, oibdp / lag(at), NA)) |&gt;\n  filter(fyear == lag(fyear) + 1) |&gt;\n  ungroup() |&gt;\n  select(gvkey, datadate, fyear, at, mtob, leverage, roa)\n\nFollowing Fang et al. (2019), we create controls_filled, which uses [fill()](https://tidyr.tidyverse.org/reference/fill.html) to remove many missing values for the controls.\n\ncontrols_filled &lt;-\n  controls_raw |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  fill(at, mtob, leverage, roa) |&gt;\n  ungroup()\n\nFollowing Fang et al. (2019), we create controls_fyear_avg, which calculates averages of controls by fiscal year.\n\ncontrols_fyear_avg &lt;-\n  controls_filled |&gt;\n  group_by(fyear) |&gt;\n  summarize(across(c(at, mtob, leverage, roa), \n                   \\(x) mean(x, na.rm = TRUE)))\n\nLike Fang et al. (2019), we use values from controls_fyear_avg to replace missing values for controls.\n\ndf_controls &lt;-\n  controls_filled |&gt;\n  inner_join(controls_fyear_avg, by = \"fyear\", suffix = c(\"\", \"_avg\")) |&gt;\n  mutate(at = coalesce(at, at_avg),\n         mtob = coalesce(mtob, mtob_avg),\n         leverage = coalesce(leverage, leverage_avg),\n         roa = coalesce(roa, roa_avg)) |&gt;\n  select(gvkey, fyear, at, mtob, leverage, roa)\n\nThere are multiple steps in the code above and the reasons for the steps involved in calculating controls_filled from controls_raw and df_controls from controls_filled are explored in the exercises below.\nAs discussed above, FHK estimate discretionary-accrual models by industry and year, where industries are based on the Fama-French 48-industry grouping. To create these industries here, we use [get\\_ff\\_ind()](https://rdrr.io/pkg/farr/man/get_ff_ind.html), introduced in Chapter 9 and provided by the farr package.\n\nff_data &lt;- get_ff_ind(48)\n\nWe now create functions to compile the data we need to estimate performance-matched discretionary accruals. We use a function to compile the data because (i) doing so is easy in R and (ii) it allows us to re-use code much easily, which will be important for completing the exercises in this chapter.\nThe first function we create is get_das(), which takes as its first argument (compustat) a data set derived from Compustat with the requisite data. The second argument (drop_extreme) allows to easily tweak the handling of outliers in a way examined in the exercises.\nWithin get_das(), the first data set we compile is for_disc_accruals, which contains the raw data for estimating discretionary-accruals models. Following FHK, we require each industry-year to have at least 10 observations for inclusion in our analysis and impose additional data requirements, some of which we explore in the exercises below.\nFollowing FHK, we estimate discretionary-accrual models by industry and year and store the results in the data frame fm_da. We then merge the underlying data (for_disc_accruals) with fm_da to use the estimated models to calculate non-discretionary accruals (nda). Because the coefficient on sale_c_at is applied to salerect_c_at, we cannot use [predict()](https://rdrr.io/r/stats/predict.html) or [residuals()](https://rdrr.io/r/stats/residuals.html) in a straightforward fashion and need calculate nda “by hand”. We then calculate discretionary accruals (da = acc_at - nda) and return the data.\n\nget_das &lt;- function(compustat, drop_extreme = TRUE) {\n  \n  for_disc_accruals &lt;-\n    compustat |&gt;\n    inner_join(ff_data, \n               join_by(between(sich, sic_min, sic_max))) |&gt;\n    group_by(gvkey, fyr) |&gt;\n    arrange(fyear) |&gt;\n    filter(lag(at) &gt; 0) |&gt;\n    mutate(lag_fyear = lag(fyear),\n           acc_at = (ibc - (oancf - xidoc)) / lag(at),\n           one_at = 1 / lag(at),\n           ppe_at = ppegt / lag(at),\n           sale_c_at = (sale - lag(sale)) / lag(at),\n           salerect_c_at = ((sale - lag(sale)) - \n                              (rect - lag(rect))) / lag(at)) |&gt;\n    ungroup() |&gt;\n    mutate(keep = case_when(drop_extreme ~ abs(acc_at) &lt;= 1,\n                            .default = TRUE)) |&gt;\n    filter(lag_fyear == fyear - 1,\n           keep, \n           !is.na(salerect_c_at), !is.na(acc_at), !is.na(ppe_at)) |&gt;\n    group_by(ff_ind, fyear) |&gt;\n    mutate(num_obs = n(), .groups = \"drop\") |&gt;\n    filter(num_obs &gt;= 10) |&gt;\n    ungroup()\n  \n  fm_da &lt;-\n    for_disc_accruals |&gt;\n    group_by(ff_ind, fyear) |&gt;\n    do(model = tidy(lm(acc_at ~ one_at + sale_c_at + ppe_at, data = .))) |&gt;\n    unnest(model) |&gt;\n    select(ff_ind, fyear, term, estimate) |&gt;\n    pivot_wider(names_from = \"term\", values_from = \"estimate\", \n                names_prefix = \"b_\")\n  \n  for_disc_accruals |&gt;\n    left_join(fm_da, by = c(\"ff_ind\", \"fyear\")) |&gt;\n    mutate(nda = `b_(Intercept)` + one_at * b_one_at + ppe_at * b_ppe_at + \n                   salerect_c_at * b_sale_c_at,\n           da = acc_at - nda) |&gt;\n    select(gvkey, fyear, ff_ind, acc_at, da) \n}\n\nThe next step in the data preparation process is to match each firm with another based on performance. Following FHK, we calculate performance as lagged “Income Before Extraordinary Items” (ib) divided by lagged “Total Assets” (at) and perf_diff, the absolute difference between performance for each firm-year and each other firm-year in the same industry. We then select the firm (gvkey_other) with the smallest value of perf_diff. We rename the variable containing the discretionary accruals of the matching firm as da_other and calculate performance-matched discretionary accruals (da_adj) as the difference between discretionary accruals for the target firm (da) and discretionary accruals for the matched firm (da_other), and return the results. Note that get_pm() includes the argument pm_lag with default value TRUE. If pm_lag is set to FALSE, then performance for matching is calculated using contemporary values of ib and at (this option is examined in the exercises below).\n\nget_pm &lt;- function(compustat, das, pm_lag = TRUE, drop_extreme = TRUE) {\n  \n  das &lt;- get_das(compustat, drop_extreme = drop_extreme)\n  \n  perf &lt;-\n    compustat |&gt;\n    group_by(gvkey) |&gt;\n    arrange(fyear) |&gt;\n    mutate(ib_at = \n      case_when(pm_lag ~ if_else(lag(at) &gt; 0, lag(ib) / lag(at), NA),\n                .default = if_else(at &gt; 0, ib / at, NA))) |&gt;\n    ungroup() |&gt;\n    inner_join(das, by = c(\"gvkey\", \"fyear\")) |&gt;\n    select(gvkey, fyear, ff_ind, ib_at)\n  \n  perf_match &lt;-\n    perf |&gt;\n    inner_join(perf, by = c(\"fyear\", \"ff_ind\"),\n               suffix = c(\"\", \"_other\")) |&gt;\n    filter(gvkey != gvkey_other) |&gt;\n    mutate(perf_diff = abs(ib_at - ib_at_other)) |&gt;\n    group_by(gvkey, fyear) |&gt;\n    filter(perf_diff == min(perf_diff)) |&gt;\n    select(gvkey, fyear, gvkey_other)\n  \n  perf_matched_accruals &lt;- \n    das |&gt;\n    rename(gvkey_other = gvkey,\n           da_other = da) |&gt;\n    select(fyear, gvkey_other, da_other) |&gt;\n    inner_join(perf_match, by = c(\"fyear\", \"gvkey_other\")) |&gt;\n    select(gvkey, fyear, gvkey_other, da_other)\n  \n  das |&gt;\n    inner_join(perf_matched_accruals, by = c(\"gvkey\", \"fyear\")) |&gt;\n    mutate(da_adj = da - da_other) |&gt;\n    select(gvkey, fyear, acc_at, da, da_adj, da_other, gvkey_other)\n}\n\nThe final step is performed in get_pmdas(). This function gets the needed data using get_pm(), then filters duplicate observations based on (gvkey, fyear) (the rationale for this step is explored in the discussion questions).\n\nget_pmdas &lt;- function(compustat, pm_lag = TRUE, drop_extreme = TRUE) {\n  \n  get_pm(compustat, \n         pm_lag = pm_lag,\n         drop_extreme = drop_extreme) |&gt;\n    group_by(gvkey, fyear) |&gt;\n    filter(row_number() == 1) |&gt;\n    ungroup() \n}\n\nFinally, we simply pass the data set compustat_annual to get_pmdas() and store the result in pmdas.\n\npmdas &lt;- get_pmdas(compustat_annual)\n\nThe remaining data set used by FHK is sho_data, which we discussed in Section 19.2.1.\n\nsho_data &lt;- \n  fhk_pilot |&gt;\n  select(gvkey, pilot) |&gt;\n  distinct() |&gt;\n  group_by(gvkey) |&gt;\n  filter(n() == 1) |&gt;\n  ungroup() |&gt;\n  inner_join(fhk_pilot, by = c(\"gvkey\", \"pilot\")) \n\nThe final sample sho_accruals is created in the following code and involves a number of steps. We first merge data from FHK’s sho_data with fhk_firm_years to produce the sample firm-years and treatment indicator for FHK. As fhk_firm_years can contain multiple years for each firm, so we expect each row in sho_data to match multiple rows in fhk_firm_years. At the same time, some gvkey values link with multiple PERMNOs, so some rows in fhk_firm_years will match multiple rows in sho_data. As such, we set relationship = \"many-to-many\" in this join below. We then merge the resulting data set with df_controls, which contains data on controls. The final data merge brings in data on performance-matched discretionary accruals from pm_disc_accruals_sorted. Finally, following FHK, we winsorize certain variables using the [winsorize()](https://rdrr.io/pkg/farr/man/winsorize.html) function from the farr package to do this here.16\n\nwin_vars &lt;- c(\"at\", \"mtob\", \"leverage\", \"roa\", \"da_adj\", \"acc_at\")\n\nsho_accruals &lt;-\n  sho_data |&gt;\n  inner_join(fhk_firm_years, \n             by = \"gvkey\",\n             relationship = \"many-to-many\") |&gt;\n  select(gvkey, datadate, pilot) |&gt;\n  mutate(fyear = year(datadate) - (month(datadate) &lt;= 5)) |&gt;\n  left_join(df_controls, by = c(\"gvkey\", \"fyear\")) |&gt;\n  left_join(pmdas, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(fyear) |&gt;\n  mutate(across(all_of(win_vars),\n                \\(x) winsorize(x, prob = 0.01))) |&gt;\n  ungroup()\n\n\n11.6.3 Discussion questions and exercises\n\nWhat would be the effect of replacing the code that creates ff_data above with the following code? What changes would we need to make to the code creating for_disc_accruals in get_das() to use this modified version of ff_data?\n\n\nff_data &lt;- \n  get_ff_ind(48) |&gt;\n  rowwise() |&gt;\n  mutate(sich = list(seq(from = sic_min, to = sic_max))) |&gt; \n  unnest(sich)\n\n\nWhat issue is filter(row_number() == 1) addressing in the code above? What assumptions are implicit in this approach? Do these assumptions hold in this case? What would be an alternative approach to address the issue?\nWhy is filter(fyear == lag(fyear) + 1) required in the creation of controls_raw?\nDoes the argument for using salerect_c_at * b_sale_c_at in creating non-discretionary accruals make sense to you? How do Kothari et al. (2005) explain this?\nDoes the code above ensure that a performance-matched control firm is used as a control just once? If so, which aspect of the code ensures this is true? If not, how might you ensure this and does this cause problems? (Just describe the approach in general; no need to do this.)\nWhat are FHK doing in the creation of controls_filled? (Hint: The key “verb” is [fill()](https://tidyr.tidyverse.org/reference/fill.html).) Does this seem appropriate? Does doing this make a difference?\nWhat are FHK doing in the creation of df_controls from controls_fyear? Does this seem appropriate? Does doing this make a difference?\n\n11.6.4 FHK: Regression analysis\nFHK consider a number of regression specifications including: with and without controls, with and without firm fixed effects, and with standard errors clustered by firm alone and clustered by firm and year. We make a small function (reg_year_fe()) that calculates variables used in the regression (like during and post) and allows us to specify each of these different options, to change the dependent variable from the default (dv = \"da_adj\") and to supply a different data set. This function returns a fitted model that is estimated using [feols()](https://lrberge.github.io/fixest/reference/feols.html) from the fixest package.\n\nctrls_list &lt;- c(\"log(at)\", \"mtob\", \"roa\", \"leverage\")\n\nreg_year_fe &lt;- function(df, dv = \"da_adj\",\n                        controls = TRUE, firm_fe = FALSE, cl_2 = TRUE,\n                        vcov = NULL) {\n  df &lt;- \n    df |&gt;\n    mutate(year = year(datadate),\n           during = year %in% c(2005, 2006, 2007),\n           post = year %in% c(2008, 2009, 2010))\n  \n  model &lt;- str_c(dv, \" ~ pilot * (during + post) \",\n                 if_else(controls, \n                         str_c(\" + \", str_c(ctrls_list, \n                                            collapse = \" + \")), \"\"),\n                    if_else(firm_fe, \"| gvkey + year \", \"| year \"))\n  if (is.null(vcov)) {\n    vcov = as.formula(if_else(!cl_2, \"~ gvkey \", \"~ year + gvkey\"))\n  }\n  \n  feols(as.formula(model), \n        vcov = vcov,\n        notes = FALSE,\n        data = df)\n}\n\nTo facilitate the output of variations, we next make a function that runs regressions with and without controls and with and without firm fixed effects and returns a nicely formatted regression table.\n\nmake_reg_table &lt;- function(df, dv = \"da_adj\", cl_2 = TRUE) {\n  omit &lt;- str_c(\"^(\", str_c(str_replace_all(c(\"during\", \"post\", ctrls_list),\n                                            \"[()]\", \".\"), \n                            collapse=\"|\"), \")\")\n  \n  run_reg &lt;- function(controls, firm_fe) {\n    reg_year_fe(df, dv = dv, controls = controls, firm_fe = firm_fe,\n                cl_2 = cl_2)\n  }\n  \n  params &lt;- tibble(controls = c(FALSE, TRUE, FALSE, TRUE),\n                   firm_fe = c(FALSE, FALSE, TRUE, TRUE))\n  \n  fms &lt;- pmap(params, run_reg)\n  \n  notes &lt;- tribble(~term,  ~`1`,  ~`2`, ~`3`, ~`4`,\n                   \"Firm FEs\", \"No\", \"No\", \"Yes\", \"Yes\",\n                   \"Controls\", \"No\", \"Yes\", \"No\", \"Yes\")\n  \n  modelsummary(fms,\n               estimate = \"{estimate}{stars}\",\n               gof_map = \"nobs\",\n               stars = c('*' = .1, '**' = 0.05, '***' = .01),\n               coef_omit = str_c(str_replace_all(ctrls_list, \"[()]\", \".\"),\n                                 collapse = \"|\"),\n               add_rows = notes)\n}\n\nWe now use this function with our version of FHK’s data set (sho_accruals) to create the regression results reported in Table 19.3.\n\nmake_reg_table(sho_accruals)\n\nWe next create a function that allows us to plot by-year coefficients for the treatment and control firms. (We leave the details of what this function is doing as an exercise for the reader below.)\n\nplot_coefficients &lt;- function(model) {\n  tibble(name = names(model$coefficients),\n         value = as.vector(model$coefficients)) |&gt;\n  filter(str_detect(name, \"^year.\")) |&gt;\n  separate(name, into = c(\"year\", \"pilot\"), sep = \":\", fill = \"right\") |&gt;\n  mutate(year = as.integer(str_replace(year, \"^year\", \"\")),\n         pilot = coalesce(pilot == \"pilotTRUE\", FALSE)) |&gt;\n  ggplot(aes(x = year, y = value, \n             linetype = pilot, color = pilot)) +\n  geom_line() +\n  scale_x_continuous(breaks = 2000:2012L) +\n  geom_rect(xmin = 2005, xmax = 2007, ymin = -Inf, ymax = Inf,\n              color = NA, alpha = 0.01) +\n  theme_bw()\n}\n\nTo produce Figure 19.3, we estimate one of the models above by year and feed the fitted model to plot_coefficients().\n\nsho_accruals |&gt;\n  mutate(year = as.factor(year(datadate))) |&gt;\n  feols(da_adj ~ year * pilot - pilot - 1 + log(at) + mtob + roa + leverage,\n        vcov = ~ year + gvkey, data = _) |&gt;\n  plot_coefficients()\n\n\n11.6.5 Exercises\n\nIn words, how does sho_accruals_alt (defined below) differ from sho_accruals? Does using sho_accruals_alt in place of sho_accruals affect the regression results?\n\n\nfirm_years &lt;-\n  controls_raw |&gt;\n  select(gvkey, datadate, fyear)\n\nsho_accruals_alt &lt;-\n  sho_r3000_gvkeys |&gt;\n  inner_join(firm_years, by = \"gvkey\") |&gt;\n  left_join(df_controls, by = c(\"gvkey\", \"fyear\")) |&gt;\n  left_join(pmdas, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(fyear) |&gt;\n  mutate(across(all_of(win_vars), winsorize, prob = 0.01)) |&gt;\n  ungroup()\n\n\nIn an online appendix, BDLYY say “FHK winsorize covariates for their covariate balance table at 1/99%. We inferred that they also winsorized accruals at this level. Whether they winsorize across sample years or within each year, they do not specify.” The code above winsorized within each year. How would you modify the code to winsorize “across sample years”? Does doing so make a difference?\nHow would you modify the code to winsorize at the 2%/98% level? Does this make a difference to the results? (Hint: With the farr package loaded, type [? winsorize](https://rdrr.io/pkg/farr/man/winsorize.html) in the R console to get help on this function.)\nHow would you modify the code to not winsorize at all? Does this make a difference to the results?\nSome of the studies discussed by BDLYY exclude 2004 data from the sample. How would you modify the code above to do this here? Does excluding 2004 here make a significant difference?\nWhat is the range of values for year in sho_accruals? Does this suggest any issues with the code post = year %in% c(2008, 2009, 2010) above? If so, does fixing any issue have an impact on the results reported above?\nWould it make sense, in creating perf above, if we instead calculated ib_at as if_else(at &gt; 0, ib / at, NA))? What is the effect on the regression results if we use this modified calculation of ib_at? What do Kothari et al. (2005) recommend on this point? (Hint: Use pm_lag = FALSE where applicable.)\nFang et al. (2019, p. 10) follow Fang et al. (2016), who “exclude observations for which the absolute value of total accruals-to-total assets exceeds one. This is a standard practice in the accounting literature because firms with such high total accruals-to-total assets are often viewed as extreme outliers. Nonetheless, the FHK results are robust to winsorizing the accrual measures at the 1% and 99% levels instead of excluding extreme outliers.” Does this claim hold up in the reproduction above? What happens if the [filter()](https://dplyr.tidyverse.org/reference/filter.html) on abs(acc_at) &lt;= 1 is removed from the code above? (Hint: Use drop_extreme = FALSE where applicable.)\nExplain what each line of the function plot_coefficients() before the line starting with [ggplot()](https://ggplot2.tidyverse.org/reference/ggplot.html) is doing. (Hint: It may be helpful to store the model that is fed to the function above in the variable model and then run the function line by line.)",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#statistical-inference",
    "href": "chap19_Natural_experiments_revisited.html#statistical-inference",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.7 Statistical inference",
    "text": "11.7 Statistical inference\nOne point of difference between FHK and BDLYY concerns clustered standard errors. Fang et al. (2016) generally use “standard errors clustered by year and firm” (2016, p. 1269), while Black et al. (2019) advocate the use of standard errors clustered by firm. Citing Cameron et al. (2008), Black et al. (2019, p. 30) suggest that “clustered standard errors with a small number of clusters can be downward biased.” In the context of FHK, there are thousands of firms, but a relatively small number of years, so clustering by year (or firm and year) may result in problematic standard error estimates (see Section 5.6.6).\nOne approach to determining the appropriate clustering is more empirical. In this regard, it is useful to note that cluster-robust standard errors are a generalization of an idea from White (1980). White (1980) provides not only an estimator of standard errors that is robust to heteroskedasticity, but also a test of a null hypothesis of homoskedasticity. Intuitively, if the covariance matrix assuming heteroskedasticity is sufficiently different from that assuming homoskedasticity, then we may reject the null hypothesis of homoskedasticity. With a little algebra, it would be possible to develop a test analogous to that of White (1980) of the null hypothesis of no clustering on variable (g). In practice, many researchers will, lacking a formally derived test, compare standard errors with and without clustering on variable (g) and elect to cluster on variable (g) when the standard errors when doing so seem significantly higher than when not doing so. This heuristic breaks down in the case of Fang et al. (2016) because standard errors are generally lower when clustering on firm and year than when clustering firm alone. However, if clustering on firm alone is appropriate, standard errors clustering on firm and year will provide noisier estimates than clustering on firm alone, and thus could be lower or higher in any given data set.\nA more theoretical approach can be used in the setting of FHK because of our deeper understanding of the assignment mechanism. In this regard, it is important to note that cluster-robust standard errors address correlation in both (X) and () across units within clusters. To explore this (slightly) more formally, recall from Chapter 5 that the cluster-robust covariance matrix is estimated using the following expression:\n\n\\begin{aligned}\n\\hat{V}(\\hat{\\beta}) =\n(X'X)^{-1} \\hat{B} (X'X)^{-1},  \\text{where}\\  \\hat{B} = \\sum_{g = 1}^G X'_g u_g u'_g X_g\n\\end{aligned}\n\nwhere the observations grouped into G clusters of N_g observations for g in {1, \\dots, G} , X_g is the N_g \\times K matrix of regressors, and u_g is the N_g -vector of residuals for cluster g .\nIf we have a single regressor, demeaned x with no constant term and two firms ( i and j ) in a cluster, then the contribution of that cluster to \\hat{B} will be\n\n\\begin{aligned}\n\\begin{bmatrix}\nx\\_i & x\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu\\_i \\\\\nu\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu\\_i & u\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\_i \\\\\nx\\_j\n\\end{bmatrix} &=\n\\begin{bmatrix}\nx\\_i & x\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu\\_i^2 & u\\_i u\\_j \\\\\nu\\_i u\\_j & u\\_j^2\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\_i \\\\\nx\\_j\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nx\\_i & x\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\_i u\\_i^2 + x\\_j u\\_i u\\_j \\\\\nx\\_i u\\_i u\\_j + x\\_j u\\_j^2\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nx\\_i^2 u\\_i^2 + x\\_i x\\_j u\\_i u\\_j \\\\\nx\\_i x\\_j u\\_i u\\_j + x\\_j^2 u\\_j^2\n\\end{bmatrix}\n\\end{aligned}\n\nNow, if x_i and x_j are uncorrelated then, even if \\epsilon_i and \\epsilon_j are correlated, this resolves in expectation to\n\n\\begin{bmatrix}\nx\\_i^2 \\sigma\\_i^2 \\\\\nx\\_j^2 \\sigma\\_j^2\n\\end{bmatrix}\n\nwhich is the expectation of the analogous component of the heteroskedasticity-robust estimator from White (1980). In the setting of Fang et al. (2016), the “(x)” of primary interest is the Reg SHO pilot indicator, which is assumed to be randomly assigned, and thus (in expectation) uncorrelated across firms. For this reason, we do not expect cross-sectional dependence to affect standard error estimates on average. On the other hand, the Reg SHO pilot indicator is perfectly correlated over time within firm, so any serial dependence in errors within firm over time will lead to effects of time-series dependence on standard error estimates. This (somewhat loose) theoretical analysis suggests we should cluster by firm (time-series dependence), but not by year (cross-sectional dependence), as suggested by Black et al. (2019, p. 12).\nHowever, the assumed random assignment of treatment allows us to adopt an alternative approach to statistical inference that is agnostic to the form of clustering in the data. This approach is known as randomization inference and builds on the Fisher sharp null hypothesis of no effect of any kind. This is a “sharp null” because it is more restrictive that a null hypothesis of zero mean effect, which could be true even if half the observations had a treatment effect of (+1) and half the observations had a treatment effect of (-1), in which case the Fisher sharp null would not be true even though null hypothesis of zero mean effect is true.\nUnder the Fisher sharp null hypothesis and with random assignment to treatment, in principle we can evaluate the distribution of any given test statistic by considering all possible assignments. Focusing on the 2954 firms that the SEC focused on as its initial sample, if assignment to treatment were purely random, then any other assignment of treatment to 985 was as likely as the one chosen. Given that the Fisher sharp null implies that there was no impact of treatment assignment on outcomes, we know what the distribution of the test statistic would have been if the SEC had chosen any one of those alternative assignments because the outcomes would have been exactly the same. With smaller samples, we might proceed to calculate the test statistic for every possible assignment and thereby construct the exact distribution of the test statistic under the Fisher sharp null.17 But in our case, there will be a huge number of ways to choose 985 treatment firms from 2954 possibilities; so a more feasible approach is to draw a random sample of possible assignments and use the empirical distribution of the test statistic for that random sample as an approximation for the exact distribution.\n\nget_coef_rand &lt;- function(i) {\n  treatment &lt;-\n    sho_accruals |&gt;\n    select(gvkey, pilot) |&gt;\n    distinct() |&gt;\n    mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE))\n  \n  reg_data_alt &lt;-\n    sho_accruals |&gt;\n    select(-pilot) |&gt;\n    inner_join(treatment, by = \"gvkey\")\n  \n  reg_data_alt |&gt; \n    reg_year_fe(controls = TRUE, firm_fe = TRUE) |&gt; \n    broom::tidy() |&gt; \n    select(term, estimate) |&gt;\n    pivot_wider(names_from = \"term\", values_from = \"estimate\") |&gt;\n    mutate(iteration = i) |&gt;\n    suppressWarnings()\n}\n\nThe test statistic we are interested in here is the coefficient on PILOT \\times DURING . Below we calculate the p-value of the coefficients on variables involving PILOT using the empirical distribution of coefficients, and the standard errors associated with the coefficients as the standard deviation of those coefficients.\n\nset.seed(2021)\nrand_results &lt;-\n  1:1000 |&gt;\n  map(get_coef_rand) |&gt; \n  list_rbind() |&gt;\n  system_time()\n\n\nplan(multisession)\n\nrand_results &lt;- \n  1:1000 |&gt; \n  future_map(get_coef_rand, \n             .options = furrr_options(seed = 2021)) |&gt; \n  list_rbind() |&gt;\n  system_time()\n\nIn the following, we run regressions with standard errors based on clustering by firm and year, by firm alone, and using randomization inference. We start by running regressions—with controls and firm fixed effects—with standard errors based on clustering by firm (\"CL-i\") and by firm and year (\"CL-2\").\n\nfms &lt;- list(reg_year_fe(sho_accruals, cl_2 = FALSE),\n            reg_year_fe(sho_accruals, cl_2 = TRUE))\n\nWe extract the variance-covariance matrices for each of these two models and place them in the list vcovs.\n\nvcovs &lt;- list(vcov(fms[[1]]), vcov(fms[[2]]))\n\nNext, we add a third model for which we will calculate standard errors using randomization inference (\"RI\"). The coefficients stored in fms for this third model can be taken from either of the two models already stored there.\n\nfms[[3]] &lt;- fms[[2]]\n\nFor the variance-covariance matrix, we use CL-i standard errors as the starting point. Then we replace the elements for coefficients on variables involving () using the empirical distribution stored in rand_results.\n\nvcov &lt;- vcovs[[1]]\nvcov[\"pilotTRUE:duringTRUE\", \"pilotTRUE:duringTRUE\"] &lt;-\n  var(rand_results[[\"pilotTRUE:duringTRUE\"]])\nvcov[\"pilotTRUE:postTRUE\", \"pilotTRUE:postTRUE\"] &lt;- \n  var(rand_results[[\"pilotTRUE:postTRUE\"]])\nvcovs[[3]] &lt;- vcov\n\nResults of this analysis are provided in Table 19.4.\n\nse_notes &lt;- tribble(~term,  ~`1`,  ~`2`, ~`3`,\n                    \"SEs\", \"CL-i\", \"CL-2\", \"RI\")\n\nmodelsummary(fms, vcov = vcovs, \n             estimate = \"{estimate}{stars}\",\n             gof_map = \"nobs\",\n             stars = c('*' = .1, '**' = 0.05, '***' = .01),\n             coef_omit = \"^(during|post|pilot)TRUE$\",\n             add_rows = se_notes)\n\n\n11.7.1 Exercises\n\nIn the function get_coef_rand(), we first created the data set treatment, then merged this with reg_data_alt. Why did we do it this way rather than simply applying the line mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE)) directly to reg_data_alt?\nUsing randomization inference, calculate a p-value for a one-sided alternative hypothesis that H_1: \\beta &lt; 0 where \\beta is the coefficient on PILOT \\times DURING . (Hint: You should not need to run the randomization again; modifying the calculation of p_value should suffice.)\nWhat is the empirical standard error implied by the distribution of coefficients in rand_results? Is it closer to the two-way cluster robust standard errors obtained in estimating with cl_2 = TRUE or with cl_2 = FALSE? Why might it be preferable to calculate p-values under randomization inference using the empirical distribution of the test statistic, instead of calculating these from t-statistics based on the estimated coefficient and the empirical standard error? Would we get different p-values using the former approach?\nWhy did we not use the empirical standard error implied by the distribution of coefficients in rand_results to calculate standard errors for the control variables (e.g., log(at))?",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#causal-diagrams",
    "href": "chap19_Natural_experiments_revisited.html#causal-diagrams",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.8 Causal diagrams",
    "text": "11.8 Causal diagrams\nIt is important to note that we observe total accruals, not discretionary accruals. Instead we need to construct measures of discretionary accruals. The Jones (1991) model of discretionary accruals “controls for” sales growth and PP&E and the Kothari et al. (2005) model additionally “controls for” performance.\nAssuming that the causal diagram below is correct, we get unbiased estimates of causal effects whether we “control for” pre-treatment outcome values (e.g., using DiD) or not (e.g., using POST), and it is not clear that we need to control for other factors that drive total accruals. If being a Reg SHO pilot firm leads to a reduction in earnings management, we should observe lower total accruals, even if we posit that the effect is through discretionary accruals, which we do not observe directly. If we accept this causal diagram, then the decision as to which factors to control for is—like the choice between DiD, POST, and ANCOVA—a question of statistical efficiency rather than bias.\nIn this context, it is perhaps useful to consider causal diagrams to sharpen our understanding of the issues, which we explore in the discussion questions below, as matters can be more complicated if the causal diagram in Figure 19.4 is incomplete.\n\n11.8.1 Discussion questions\n\nWhat features of Figure 19.4 imply that we do not need to control for performance, sales, and PP&E in estimating the causal effect of Reg SHO on accruals? What is the basis for assuming these features in the causal diagram?\nBlack et al. (2024) report that “over 60 papers in accounting, finance, and economics report that suspension of the price tests had wide-ranging indirect effects on pilot firms, including on earnings management, investments, leverage, acquisitions, management compensation, workplace safety, and more (see Internet Appendix, Table IA-1 for a summary).” In light of the Internet Appendix of Black et al. (2024), is there any evidence that Reg SHO might plausibly have an effect on performance, sales growth, or PP&E? If so, how would Figure 19.4 need to be modified to account for these consequences? What would be the implications of these changes on the appropriate tests for estimating the causal effects of Reg SHO on accruals?\nProduce a regression table like Table 19.3 and a plot like Figure 19.3, but using discretionary accruals without performance matching instead of performance-matched discretionary accruals. How do you interpret these results?\nProduce a regression table and a plot like the ones in the FHK replication above, but using total accruals instead of discretionary accruals and excluding controls (so the coefficients will be simple conditional sample means). How do you interpret these results?\nSuppose you had been brought in by the SEC to design a study examining the research question examined by FHK in the form of a registered report. What analyses would you conduct to try to understand the best research design? For example, how would you choose between DiD, POST, ANCOVA, and other empirical approaches? What controls would you include? How would you decide how to include controls? (For example, one could control for performance by including performance as a regressor in the model of earnings management, by matching on performance, or by including performance in the main regression specification.) How would you calculate standard errors? Discuss how your proposed empirical test differs from that of FHK. Would you have reported similar results to what FHK reported?\nSuppose that FHK’s empirical analysis had produced a positive effect of Reg SHO on earnings management? Would this imply a lack of support for their hypotheses? Do you believe that publication in the Journal of Finance depended on finding a negative effect?\nWhat implications would there have been for publication of FHK in the Journal of Finance if they had failed to find an effect of Reg SHO on earnings management?",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#causal-mechanisms",
    "href": "chap19_Natural_experiments_revisited.html#causal-mechanisms",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.9 Causal mechanisms",
    "text": "11.9 Causal mechanisms\nBlack et al. (2024) suggest a number of possible causal channels through which the Reg SHO experiment could have affected the behavior of firms or third parties, including short interest, returns, price efficient, and “manager fear”. On the last of these, Black et al. (2024, p. 4) suggest that “even if the Reg SHO experiment did not actually affect short interest or returns, pilot firm managers could have feared being targeted by short sellers and taken pre-emptive actions.”\nBlack et al. (2024, p. 5133) argue that “if firm managers were fearful that relaxing the price tests would affect them, one might expect them to voice concerns in various ways: speaking with business news reporters; writing to the SEC when it sought public comments, or seeking meetings with SEC officials to express opposition. … We searched the business press during 2003 when the rule was proposed, in 2004 when the experiment was announced, in 2006 when the SEC proposed repeal … We found no evidence of manager opposition.”\nBlack et al. (2024, p. 5134) suggest that “FHK rely on the manager fear channel. They conjecture that, in response to a greater threat of short selling, pilot firms’ managers reduced earnings management to preemptively deter short sellers.”\n\n11.9.1 Discussion questions\n\nDo you agree with the assertion of Black et al. (2024) that “FHK rely on the manager fear channel”? What causal mechanisms are suggested in Fang et al. (2016)? What evidence do Fang et al. (2016) offer in support of these mechanisms?\nEvaluate the response of Fang et al. (2019) to Black et al. (2024) as it relates to causal mechanisms?\nDo you think evidence of causal mechanisms is more or less important when using a natural experiment (i.e., an experiment outside the control of the researcher that is typically analysed after it has been run) than when conducting a randomized experiment? Explain your reasoning given the various issues raised in this chapter.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#two-step-regressions",
    "href": "chap19_Natural_experiments_revisited.html#two-step-regressions",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.10 Two-step regressions",
    "text": "11.10 Two-step regressions\nChen, Hribar, と Melessa (2018) examine the question of statistical inference when residuals from one regression are used as a dependent variable in a subsequent regression, which they refer to as “the two-step procedure”. For example, discretionary accruals measured using the Jones (1991) model are residuals from a regression of total accruals on changes in sales and PP&E. As we saw in Dechow et al. (1995), which we covered in Chapter 16, many papers examine how Jones (1991) model discretionary accruals relate to various posited incentives for earnings management.\nChen, Hribar, と Melessa (2018, 755) show that “the two-step procedure is likely to generate biased coefficients and t-statistics in many studies” and, drawing on the Frisch-Waugh-Lovell theorem (see Section 3.3), propose using a single regression in place of the two-step procedure. In the case of the Jones (1991) model, this would entail including the regressors from the first step in same regression as the second step and using total accruals in place of discretionary accruals as the dependent variable.\n\n11.10.1 Discussion questions\n\nWhat challenges would exist in implementing the single-regression recommendation of Chen, Hribar, と Melessa (2018) for a researcher using Kothari, Leone, と Wasley (2005) performance-matched discretionary accruals?\nDo you believe the issues raised by Chen, Hribar, と Melessa (2018) with regard to two-step procedures also apply if using randomization inference? Why or why not?\n\n\n\n\n\nChen, Wei, Paul Hribar, と Samuel Melessa. 2018. 「Incorrect inferences when using residuals as dependent variables」. Journal of Accounting Research 56 (3): 751–96.\n\n\nKothari, Sagar P, Andrew J Leone, と Charles E Wasley. 2005. 「Performance matched discretionary accrual measures」. Journal of accounting and economics 39 (1): 163–97.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#he-reg-sho-experiment",
    "href": "chap19_Natural_experiments_revisited.html#he-reg-sho-experiment",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.2 he Reg SHO experiment",
    "text": "11.2 he Reg SHO experiment\nTo better understand the issues raised by the discussion above in a real research setting, we focus on the Reg SHO experiment, which has been the subject of many studies. In July 2004, the SEC adopted Reg SHO, a regulation governing short-selling activities in equity markets. Reg SHO contained a pilot program in which stocks in the Russell 3000 index were ranked by trading volume within each exchange and every third one was designated as a pilot stock. From 2 May 2005 to 6 August 2007, short sales on pilot stocks were exempted from price tests, including the tick test for exchange-listed stocks and the bid test for NASDAQ National Market stocks.\nIn its initial order, the SEC stated that “the Pilot will allow [the SEC] to study trading behavior in the absence of a short sale price test.” The SEC’s plan was to “examine, among other things, the impact of price tests on market quality (including volatility and liquidity), whether any price changes are caused by short selling, costs imposed by a price test, and the use of alternative means to establish short positions.”\n19.2.1 The SHO pilot sample The assignment mechanism in the Reg SHO experiment is unusually transparent, even by the standards of natural experiments. Nonetheless care is needed to identify the treatment and control firms and we believe it is instructive to walk through the steps needed to do so, as we do in this section. (Readers who find the code details tedious could easily skip ahead to Section 19.2.3 on a first reading. We say “first reading” because there are subtle issues with natural experiments that this section helps to highlight, so it may be worth revisiting this section once you have read the later material.)\nThe SEC’s website provides data on the names and tickers of the Reg SHO pilot firms. These data have been parsed and included as the sho_tickers data set in the farr package.\n\nsho_tickers\n\n\n  \n\n\n\nHowever, these are just the pilot firms and we need to use other sources to obtain the identities of the control firms. It might seem perverse for the SEC to have published lists of treatment stocks, but no information on control stocks.4 One explanation for this choice might be that, because special action (i.e., elimination of price tests) was only required for the treatment stocks (for the control stocks, it was business as usual), no lists of controls were needed for the markets to implement the pilot. Additionally, because the SEC had a list of the control stocks that it would use in its own statistical analysis, it had no reason to publish lists for this purpose. Fortunately, while the SEC did not identify the control stocks, it provides enough information for us to do so, as we do below.\nFirst, we know that the pilot stocks were selected from the Russell 3000, the component stocks of which are found in the sho_r3000 data set from the farr package.\n\nsho_r3000\n\n\n  \n\n\n\nWhile the Russell 3000 contains 3,000 securities, the SEC and Black et al. (2019) tell us that, in constructing the pilot sample, the SEC excluded 32 stocks in the Russell 3000 index that, as of 25 June 2004, were not listed on the Nasdaq National Market, NYSE, or AMEX “because short sales in these securities are currently not subject to a price test.” The SEC also excluded 12 stocks that started trading after 30 April 2004 due to IPOs or spin-offs. And, from Black et al. (2019), we know there were two additional stocks that stopped trading after 25 June 2004 but before the SEC constructed its sample on 28 June 2004. We can get the data for each of these criteria from CRSP, but we need to first merge the Russell 3000 data with CRSP to identify the right PERMNO for each security. For this purpose, we will use data from the five CRSP tables below:\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nmse &lt;- tbl(db, Id(schema = \"crsp\", table = \"mse\"))\nmsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"msf\"))\nstocknames &lt;- tbl(db, Id(schema = \"crsp\", table = \"stocknames\"))\ndseexchdates &lt;- tbl(db, Id(schema = \"crsp\", table = \"dseexchdates\"))\nccmxpf_lnkhist &lt;- tbl(db, Id(schema = \"crsp\", table = \"ccmxpf_lnkhist\"))\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nmse &lt;- load_parquet(db, schema = \"crsp\", table = \"mse\")\nmsf &lt;- load_parquet(db, schema = \"crsp\", table = \"msf\")\nstocknames &lt;- load_parquet(db, schema = \"crsp\", table = \"stocknames\")\ndseexchdates &lt;- load_parquet(db, schema = \"crsp\", table = \"dseexchdates\")\nccmxpf_lnkhist &lt;- load_parquet(db, schema = \"crsp\", table = \"ccmxpf_lnkhist\")\n\n\n\n\nOne thing we note is that some of the tickers from the Russell 3000 sample append the class of stock to the ticker. We can detect these cases by looking for a dot (.) using regular expressions. Because a dot has special meaning in regular expressions (regex), we need to escape it using a backslash (). (For more on regular expressions, see Chapter 9 and references cited there.) Because a backslash has a special meaning in strings in R, we need to escape the backslash itself to tell R that we mean a literal backslash. In short, we use the regex \\. to detect dots in strings.\n\nsho_r3000 |&gt; \n  filter(str_detect(russell_ticker, \"\\\\.\"))\n\n\n  \n\n\n\nIn these cases, CRSP takes a different approach. For example, where the Russell 3000 sample has AGR.B, CRSP has ticker equal to AGR and shrcls equal to B.\nThe other issue is that some tickers from the Russell 3000 data have the letter E appended to what CRSP shows as just a four-letter ticker.\n\nsho_r3000 |&gt; \n  filter(str_length(russell_ticker) == 5,\n         str_sub(russell_ticker, 5, 5) == \"E\")\n\n\n  \n\n\n\nA curious reader might wonder how we identified these two issues with tickers, and how we know that they are exhaustive of the issues in the data. We explore these questions in the exercises at the end of this section.\nTo address these issues, we create two functions: one (clean_ticker()) to “clean” each ticker so that it can be matched with CRSP, and one (get_shrcls()) to extract the share class (if any) specified in the Russell 3000 data.\nBoth functions use a regular expression to match cases where the text ends with either “A” or “B” ([AB]$ in regex) preceded by the a dot (\\\\. in regex, as discussed above). The expression uses capturing parentheses (i.e., ( and )) to capture the text before the dot from the beginning of the string (^(.*)) to the dot and to capture the letter “A” or “B” at the end (([AB])$).\n\nregex &lt;- \"^(.*)\\\\.([AB])$\"\n\nThe clean_ticker() function uses case_when(), which first handles cases with an E at the end of five-letter tickers, then applies the regex to extract the “clean” ticker (the first captured text) from cases matching regex. Finally, the original ticker is returned for all other cases.\n\nclean_ticker &lt;- function(x) {\n  case_when(str_length(x) == 5 & str_sub(x, 5, 5) == \"E\" ~ str_sub(x, 1, 4),\n            str_detect(x, regex) ~ str_replace(x, regex, \"\\\\1\"),\n            .default = x)\n}\n\nThe get_shrcls() function extracts the second capture group from the regex (the first value returned by str_match() is the complete match, the second value is the first capture group, so we use [, 3] to get the second capture group).\n\nget_shrcls &lt;- function(x) {\n  str_match(x, regex)[, 3]\n}\n\nWe can use clean_ticker() and get_shrcls() to construct sho_r3000_tickers.\n\nsho_r3000_tickers &lt;-\n  sho_r3000 |&gt;\n  select(russell_ticker, russell_name) |&gt;\n  mutate(ticker = clean_ticker(russell_ticker),\n         shrcls = get_shrcls(russell_ticker))\n\nsho_r3000_tickers |&gt;\n  filter(russell_ticker != ticker)\n\n\n  \n\n\n\nNow that we have “clean” tickers, we can merge with CRSP. The following code proceeds in two steps. First, we create crsp_sample, which contains the permno, ticker, and shrcls values applicable on 2004-06-25, the date on which the Russell 3000 that the SEC used was created.\n\ncrsp_sample &lt;-\n  stocknames |&gt;\n  mutate(test_date = as.Date(\"2004-06-25\")) |&gt;\n  filter(test_date &gt;= namedt, test_date &lt;= nameenddt) |&gt;\n  select(permno, permco, ticker, shrcls) |&gt;\n  distinct() |&gt;\n  collect()\n\nSecond, we merge sho_r3000_tickers with crsp_sample using ticker and then use filter() to retain cases where, if a share class is specified in the SEC-provided ticker, it matches the one row in CRSP with that share class, while retaining all rows where no share class is specified in the SEC-provided ticker.\n\nsho_r3000_merged &lt;-\n  sho_r3000_tickers |&gt;\n  inner_join(crsp_sample, by = \"ticker\", suffix = c(\"\", \"_crsp\")) |&gt;\n  filter(shrcls == shrcls_crsp | is.na(shrcls)) |&gt;\n  select(russell_ticker, permco, permno)\n\nUnfortunately, this approach results in some tickers being matched to multiple PERMNO values.\n\nsho_r3000_merged |&gt;\n  group_by(russell_ticker) |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup()\n\nIn each case, these appear to be cases where there are multiple securities (permno values) for the same company (permco value). To choose the security that is the one most likely included in the Russell 3000 index used by the SEC, we will keep the one with the greatest dollar trading volume for the month of June 2004. We collect the data on dollar trading volumes in the data frame trading_vol.\n\ntrading_vol &lt;-\n  msf |&gt;\n  filter(date == \"2004-06-30\") |&gt; \n  mutate(dollar_vol = coalesce(abs(prc) * vol, 0)) |&gt; \n  select(permno, dollar_vol) |&gt;\n  collect()\n\nWe can now make a new version of the table sho_r3000_merged that includes just the permno value with the greatest trading volume for each ticker.\n\nsho_r3000_merged &lt;-\n  sho_r3000_tickers |&gt;\n  inner_join(crsp_sample, by = \"ticker\", suffix = c(\"\", \"_crsp\")) |&gt;\n  filter(is.na(shrcls) | shrcls == shrcls_crsp) |&gt;\n  inner_join(trading_vol, by = \"permno\") |&gt;\n  group_by(russell_ticker) |&gt;\n  filter(dollar_vol == max(dollar_vol, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(russell_ticker, permno)\n\nBlack et al. (2019) identify 32 stocks that are not listed on the Nasdaq National Market, NYSE, or AMEX firms “using historical exchange code (exchcd) and Nasdaq National Market Indicator (nmsind) from the CRSP monthly stock file” (in practice, these 32 stocks are smaller Nasdaq-listed stocks). However, exchcd and nmsind are not included in the crsp.msf file we use. Black et al. (2019) likely used the CRSP monthly stock file obtained from the web interface provided by WRDS, which often merges in data from other tables.\nFortunately, we can obtain nmsind from the CRSP monthly events file (crsp.mse). This file includes information about delisting events, distributions (such as dividends), changes in NASDAQ information (such as nmsind), and name changes. We get data on nmsind by pulling the latest observation on crsp.mse on or before 2004-06-28 where the event related to NASDAQ status (event == \"NASDIN\").\n\nnmsind_data &lt;-\n  mse |&gt; \n  filter(date &lt;= \"2004-06-28\", event == \"NASDIN\") |&gt;\n  group_by(permno) |&gt;\n  filter(date == max(date, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(permno, date, nmsind) |&gt;\n  collect()\n\nWe can obtain exchcd from the CRSP stock names file (crsp.stocknames), again pulling the value applicable on 2004-06-28.[^5]\n\nexchcd_data &lt;-\n  stocknames |&gt;\n  filter(exchcd &gt; 0) |&gt;\n  mutate(test_date = as.Date(\"2004-06-28\")) |&gt;\n  filter(between(test_date, namedt, nameenddt)) |&gt;\n  select(permno, exchcd) |&gt;\n  distinct() |&gt;\n  collect()\n\nAccording to its website, the SEC “also excluded issuers whose initial public offerings commenced after April 30, 2004.” Following Black et al. (2019), we use CRSP data to identify these firms. Specifically, the table crsp.dseexchdates includes the variable begexchdate.\n\nipo_dates &lt;-\n  dseexchdates |&gt; \n  distinct(permno, begexchdate) |&gt; \n  collect()\n\nFinally, it appears that there were stocks listed in the Russell 3000 file likely used by the SEC (created on 2004-06-25) that were delisted prior to 2004-06-28, the date on which the SEC appears to have finalized the sample for its pilot program. We again use crsp.mse to identify these firms.\n\nrecent_delistings &lt;-\n  mse |&gt;\n  filter(event == \"DELIST\", \n         between(date, \"2004-06-25\", \"2004-06-28\")) |&gt;\n  rename(delist_date = date) |&gt;\n  select(permno, delist_date) |&gt;\n  collect()\n\nNow, we put all these pieces together and create variables nasdaq_small, recent_listing, and delisted corresponding to the three exclusion criteria discussed above.\n\nsho_r3000_permno &lt;-\n  sho_r3000_merged |&gt;\n  left_join(nmsind_data, by = \"permno\") |&gt;\n  left_join(exchcd_data, by = \"permno\") |&gt;\n  left_join(ipo_dates, by = \"permno\") |&gt;\n  left_join(recent_delistings, by = \"permno\") |&gt;\n  mutate(nasdaq_small = coalesce(nmsind == 3 & exchcd == 3, FALSE), \n         recent_listing = begexchdate &gt; \"2004-04-30\",\n         delisted = !is.na(delist_date),\n         keep = !nasdaq_small & !recent_listing & !delisted)\n\nAs can be seen in Table 19.1, we have a final sample of 2954 stocks that we can merge with sho_tickers to create the pilot indicator.\n\nsho_r3000_permno |&gt; \n  count(nasdaq_small, recent_listing, delisted, keep)\n\n\nsho_r3000_sample &lt;-\n  sho_r3000_permno |&gt;\n  filter(keep) |&gt;\n  rename(ticker = russell_ticker) |&gt;\n  left_join(sho_tickers, by = \"ticker\") |&gt;\n  mutate(pilot = !is.na(co_name)) |&gt;\n  select(ticker, permno, pilot)\n\nAs can be seen the number of treatment and control firms in sho_r3000_sample corresponds exactly with the numbers provided in Black et al. (2019, p. 42).\n\nsho_r3000_sample |&gt;\n  count(pilot)\n\nFinally, we will want to link these data with data from Compustat, which means linking these observations with GVKEYs. For this, we use ccm_link (as used and discussed in Chapter 7) to produce sho_r3000_gvkeys, the sample we can use in later analysis.\nFinally, we will want to link these data with data from Compustat, which means linking these observations with GVKEYs. For this, we use ccm_link (as used and discussed in Chapter 7) to produce sho_r3000_gvkeys, the sample we can use in later analysis.\n\nccm_link &lt;-\n  ccmxpf_lnkhist |&gt;\n  filter(linktype %in% c(\"LC\", \"LU\", \"LS\"),\n         linkprim %in% c(\"C\", \"P\")) |&gt;\n  rename(permno = lpermno) |&gt;\n  mutate(linkenddt = coalesce(linkenddt, max(linkenddt, na.rm = TRUE))) |&gt;\n  select(gvkey, permno, linkdt, linkenddt)\n\nBecause we focus on a single “test date”, our final link table includes just two variables: gvkey and permno.[^6]\n\ngvkeys &lt;-\n  ccm_link |&gt;\n  mutate(test_date = as.Date(\"2004-06-28\")) |&gt;\n  filter(between(test_date, linkdt, linkenddt)) |&gt;\n  select(gvkey, permno) |&gt;\n  collect()\n\nFinally, we can add gvkey to sho_r3000_sample to create sho_r3000_gvkeys.\n\nsho_r3000_gvkeys &lt;-\n  sho_r3000_sample |&gt;\n  inner_join(gvkeys, by = \"permno\")\n\nsho_r3000_gvkeys\n\nTo better understand the potential issues with constructing the pilot indicator variable, it is useful to compare the approach above with that taken in Fang et al. (2016). To construct sho_data as Fang et al. (2016) do, we use fhk_pilot from the farr package.7 We compare sho_r3000_sample and sho_r3000_gvkeys with sho_data in the exercises below.\n\nsho_data &lt;- \n  fhk_pilot |&gt;\n  select(gvkey, pilot) |&gt;\n  distinct() |&gt;\n  group_by(gvkey) |&gt;\n  filter(n() == 1) |&gt;\n  ungroup() |&gt;\n  inner_join(fhk_pilot, by = c(\"gvkey\", \"pilot\")) \n\n\n11.2.1 Exercises\n\nBefore running the following code, can you tell from output above how many rows this query will return? What is this code doing? At what stage would code like this have been used in process of creating the sample above? Why is code like this not included above?\n\n\nsho_r3000 |&gt;\n  anti_join(crsp_sample, join_by(russell_ticker == ticker)) |&gt;\n  collect()\n\n\nFocusing on the values of ticker and pilot in fhk_pilot, what differences do you observe between fhk_pilot and sho_r3000_sample? What do you believe is the underlying cause for these discrepancies?\nWhat do the following observations represent? Choose a few observations from this output and examine whether these reveal issues in the sho_r3000_sample or in fhk_pilot.\n\n\nsho_r3000_sample |&gt;\n  inner_join(fhk_pilot, by = \"ticker\", suffix = c(\"_ours\", \"_fhk\")) |&gt;\n  filter(permno_ours != permno_fhk)\n\n\nIn constructing the pilot indicator, FHK omit cases (gvkey values) where there is more than one distinct value for the indicator. A question is: Who are these firms? Why is there more than one value for pilot for these firms? And does omission of these make sense? (Hint: Identify duplicates in fhk_pilot and compare sho_r3000_gvkeys for these firms.)\nWhat issue is implicit in the output from the code below? How could you fix this issue? Would you expect a fix for this issue to significantly affect the regression results? Why or why not?\n\n\nsho_data |&gt; \n  count(gvkey, ticker) |&gt; \n  arrange(desc(n))\n\n\n11.2.2 Early studies of Reg SHO\nThe first study of the effects of Reg SHO was conducted by the SEC’s own Office of Economic Analysis. The SEC study examines the “effect of pilot on short selling, liquidity, volatility, market efficiency, and extreme price changes” [p. 86].\nThe authors of the 2007 SEC study “find that price restrictions reduce the volume of executed short sales relative to total volume, indicating that price restrictions indeed act as a constraint to short selling. However, in neither market do we find significant differences in short interest across pilot and control stocks. … We find no evidence that short sale price restrictions in equities have an impact on option trading or open interest. … We find that quoted depths are augmented by price restrictions but realized liquidity is unaffected. Further, we find some evidence that price restrictions dampen short term within-day return volatility, but when measured on average, they seem to have no effect on daily return volatility.”\nThe SEC researchers conclude “based on the price reaction to the initiation of the pilot, we find limited evidence that the tick test distorts stock prices—on the day the pilot went into effect, Listed Stocks in the pilot sample underperformed Listed Stocks in the control sample by approximately 24 basis points. However, the pilot and control stocks had similar returns over the first six months of the pilot.”\nIn summary, it seems fair to say that the SEC found that exemption from price tests had relatively limited effect on the market outcomes of interest, with no apparent impact on several outcomes.\nAlexander and Peterson (2008, p. 84) “examine how price tests affect trader behavior and market quality, which are areas of interest given by the [SEC] in evaluating these tests.” Alexander and Peterson (2008, p. 86) find that NYSE pilot stocks have similar spreads, but smaller trade sizes, more short trades, more short volume, and smaller ask depths. With regard to Nasdaq, Alexander and Peterson (2008, p. 86) find that the removed “bid test is relatively inconsequential.”\nDiether et al. (2009, p. 37) find that “while short-selling activity increases both for NYSE- and Nasdaq-listed Pilot stocks, returns and volatility at the daily level are unaffected.”\n\n11.2.3 Discussion questions and exercises\n\nEarlier we identified one feature of a randomized controlled trial (RCT) as that “proposed analyses are specified in advance”, as in a registered reports process. Why do you think the SEC did not use a registered report for its 2007 paper? Do you think the analyses of the SEC would be more credible if conducted as part of a registered reports process? Why or why not?\nDo you have concerns that the results Alexander and Peterson (2008) have been p-hacked? What factors increase or reduce your concerns in this regard?\nEvaluate the hypotheses found in the section of Diether et al. (2009, pp. 41–45) entitled Testable Hypotheses with particular sensitivity to concerns about HARKing. What kind of expertise is necessary in evaluating hypotheses in this way?\nHow might the SEC have conducted Reg SHO as part of a registered reports process open to outside research teams, such as Alexander and Peterson (2008) and Diether et al. (2009)? How might such a process have been run? What challenges would such a process face?",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#nalysing-natural-experiments",
    "href": "chap19_Natural_experiments_revisited.html#nalysing-natural-experiments",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.3 nalysing natural experiments",
    "text": "11.3 nalysing natural experiments\nBoth Alexander and Peterson (2008) and Diether et al. (2009) use the difference-in-differences estimator (“DiD”) of the causal effect that we saw in Chapter 3. The typical approach to DiD involves estimating a regression of the following form:\n\n\\begin{aligned}\nY_{it} = &\\beta_0 + \\beta_1 \\times POST_t + \\beta_2 \\times TREAT _i + \\\\\n&\\beta _3 \\times POST_t \\times TREAT_i + \\varepsilon_{it}\n\\end{aligned}\n\nIn this specification, the estimated treatment effect is given by the fitted coefficient \\hat{\\beta} _3 .\nWhile DiD is clearly popular among researchers in economics and adjacent fields, it is important to note that it is not obvious that it is the best choice in every experimental setting and that credible alternatives exist.\nAnother approach would be to limit the sample to the post-treatment period and estimate the following regression:\n\nY_{it} = \\beta_0 + \\beta_1 \\times TREAT_i + \\varepsilon_{it}\n\nIn this specification, the estimated treatment effect is given by the fitted coefficient \\hat{\\beta}_1 . This approach is common in drug trials, which are typically conducted as RCTs. For example, in the paxlovid trial “participants were randomised 1:1, with half receiving paxlovid and the other half receiving a placebo orally every 12 hours for five days. Of those who were treated within three days of symptom onset, 0.8% (3/389) of patients who received paxlovid were admitted to hospital up to day 28 after randomization, with no deaths. In comparison, 7% (27/385) of patients who received placebo were admitted, with seven deaths.” (Mahase, 2021, p. 1). For the hospital admission outcome, it would have been possible to incorporate prior hospitalization rates in a difference-in-differences analysis, but this would only make sense if hospitalization rates in one period had a high predictive power for subsequent hospitalization rates.[^8]\nYet another approach would include pre-treatment values of the outcome variable as a control:\n\nY_{it} = \\beta_0 + \\beta_1 \\times TREAT _i + \\beta_2 \\times Y_{i,t-1} + \\varepsilon_{it}\n\nTo evaluate each of these approaches, we can use simulation analysis. The following analysis is somewhat inspired by Frison and Pocock (1992), who use different assumptions about their data more appropriate to their (medical) setting and who focus on mathematical analysis instead of simulations.\nFrison and Pocock (1992) assume a degree of correlation in measurements of outcome variables for a given unit (e.g., patient) that is independent of the time between observations. A more plausible model in many business settings would be correlation in outcome measures for a given unit (e.g., firm) that fades as observations become further apart in time. Along these lines, we create get_outcomes() below to generate data for outcomes in the absence of treatment. Specifically, we assume that, if there are no treatment or period effects, the outcome in question follows the autoregressive process embedded in get_outcomes(), which has the key parameter \\rho (rho).[^9]\n\nget_outcomes &lt;- function(rho = 0, periods = 7) {\n  e &lt;- rnorm(periods)\n  y &lt;- rep(NA, periods)\n  y[1] &lt;- e[1]\n  for (i in 2:periods) {\n    y[i] &lt;- rho * y[i - 1] + e[i]\n  }\n  tibble(t = 1:periods, y = y)\n}\n\nThe get_sample() function below uses get_outcomes() for n firms for given values of rho, periods (the number of periods observed for each firm), and effect (the underlying size of the effect of treatment on y). Here treatment is randomly assigned to half the firms in the sample and the effect is added to y when both treat and post are true. We also add a time-specific effect (t_effect) for each period, which is common to all observations (a common justification for the use of DiD is the existence of such period effects).\n\nget_sample &lt;- function(n = 100, rho = 0, periods = 7, effect = 0) {\n  treat &lt;- sample(1:n, size = floor(n / 2), replace = FALSE)\n  \n  t_effects &lt;- tibble(t = 1:periods, t_effect = rnorm(periods))\n  \n  f &lt;- function(x) tibble(id = x, get_outcomes(rho = rho, \n                                               periods = periods))\n  df &lt;- \n    map(1:n, f) |&gt;\n    list_rbind() |&gt; \n    inner_join(t_effects, by = \"t\") |&gt;\n    mutate(treat = id %in% treat,\n           post = t &gt; periods / 2,\n           y = y + if_else(treat & post, effect, 0) + t_effect) |&gt;\n    select(-t_effect)\n}\n\nThe est_effect() function below applies a number of estimators to a given data set and returns the estimated treatment effect for each estimator. The estimators we consider are the following (the labels POST, CHANGE, and ANCOVA come from Frison and Pocock, 1992):\n\n\nDiD, the difference-in-differences estimator estimated by regressing y on the treatment indicator, treat interacted with the post-treatment indicator, post (with the [lm()](https://rdrr.io/r/stats/lm.html) function automatically including the main effects of treat and post), as in Equation 19.1.\n\nPOST, which is based on OLS regression of y on treat, but with the sample restricted to the post-treatment observations, as in Equation 19.2.\n\nCHANGE, which is based on OLS regression of the change in the outcome on treat. The change in outcome (y_change) is calculated as the mean of post-treatment outcome value (y_post) minus the mean of the pre-treatment outcome value (y_pre) for each unit.\n\nANCOVA, which is a regression of y_post on treat and y_pre, as in Equation 19.3.\n\n\nest_effect &lt;- function(df) {\n  \n  fm_DiD &lt;- lm(y ~ treat * post, data = df)\n  \n  df_POST &lt;- \n    df |&gt; \n    filter(post) |&gt;\n    group_by(id, treat) |&gt;\n    summarize(y = mean(y), .groups = \"drop\")\n    \n  fm_POST &lt;- lm(y ~ treat, data = df_POST)\n  \n  df_CHANGE &lt;- \n    df |&gt; \n    group_by(id, treat, post) |&gt;\n    summarize(y = mean(y), .groups = \"drop\") |&gt;\n    pivot_wider(names_from = \"post\", values_from = \"y\") |&gt;\n    rename(y_pre = `FALSE`,\n           y_post = `TRUE`) |&gt;\n    mutate(y_change = y_post - y_pre) \n  \n  fm_CHANGE &lt;- lm(I(y_post - y_pre) ~ treat, data = df_CHANGE)\n  fm_ANCOVA &lt;- lm(y_post ~ y_pre + treat, data = df_CHANGE)\n  \n  tibble(est_DiD = fm_DiD$coefficients[[\"treatTRUE:postTRUE\"]],\n         est_POST = fm_POST$coefficients[[\"treatTRUE\"]], \n         est_CHANGE = fm_CHANGE$coefficients[[\"treatTRUE\"]], \n         est_ANCOVA = fm_ANCOVA$coefficients[[\"treatTRUE\"]])\n}\n\nThe run_sim() function below calls get_sample() for supplied parameter values to create a data set, and returns a data frame containing the results of applying est_effect() to that data set.\n\nrun_sim &lt;- function(i, n = 100, rho = 0, periods = 7, effect = 0) {\n  df &lt;- get_sample(n = n, rho = rho, periods = periods, effect = effect)\n  tibble(i = i, est_effect(df))\n}\n\nTo facilitate running of the simulation for various values of effect and rho, we create a data frame (params) with effect sizes running from 0 to 1 and \\rho \\in \\{ 0, 0.18, 0.36, 0.54, 0.72, 0.9 \\} .\n\nrhos &lt;- seq(from = 0, to = 0.9, length.out = 6) \neffects &lt;- seq(from = 0, to = 1, length.out = 5)\nparams &lt;- expand_grid(effect = effects, rho = rhos)\n\nThe run_sim_n() function below runs 1000 simulations for the supplied values of effect and rho and returns a data frame with the results.\n\nrun_sim_n &lt;- function(effect, rho, ...) {\n  n_sims &lt;- 1000\n  set.seed(2021)\n  \n  res &lt;- \n    1:n_sims |&gt;\n    map(\\(x) run_sim(x, rho = rho, effect = effect)) |&gt;\n    list_rbind()\n  \n  tibble(effect, rho, res)\n                                    \n}\n\n\n\n\n\n\n\nThe following code takes several minutes to run. Using future_pmap from the furrr package in place of pmap() reduces the time needed to run the simulation significantly. Fortunately, nothing in the subsequent exercises requires that you run either variant of this code, so only do so if you have time and want to examine results directly.\n\n\n\n\nplan(multisession)\n\nresults &lt;-\n  params |&gt; \n  future_pmap(run_sim_n, \n              .options = furrr_options(seed = 2021)) |&gt; \n  list_rbind() |&gt; \n  system_time()\n\nWith results in hand, we can do some analysis. The first thing to note is that est_CHANGE is equivalent to est_DiD, as all estimates are within rounding error of each other for these two methods.\n\nresults |&gt; filter(abs(est_DiD - est_CHANGE) &gt; 0.00001) |&gt; nrow()\n\nThus we just use the label DiD in subsequent analysis.\nThe second thing we check is that the methods provide unbiased estimates of the causal effect. Figure 19.1 suggests that the estimates are very close to the true values of causal effects for all three methods.\n\nresults |&gt;\n  pivot_longer(starts_with(\"est\"), \n               names_to = \"method\", values_to = \"est\") |&gt;\n  mutate(method = str_replace(method, \"^est.(.*)$\", \"\\\\1\")) |&gt;\n  group_by(rho, method) |&gt;\n  summarize(bias = mean(est - effect), .groups = \"drop\") |&gt;\n  filter(method != \"CHANGE\") |&gt;\n  ggplot(aes(x = rho, y = bias, \n             colour = method, linetype = method)) +\n  geom_line() +\n  ylim(-0.1, 0.1)\n\n\n\nBias as a function of rho parameter. Plot shows only modest amounts of bias (plausibly consistent with no bias) with no discernible difference between the three methods considered. For each method, the line is close to horizontal and below 0.02.\n\nHaving confirmed that there is no apparent bias in any of the estimators in this setting, we next consider the empirical standard errors for each method. Because we get essentially identical plots with each value of the true effect, we focus on effect == 0.5 in the following analysis. Here we rearrange the data so that we have a method column and an est column for the estimated causal effect. We then calculate, for each method and value of rho, the standard deviation of est, which is the empirical standard error we seek. Finally, we plot the values for each value of rho in Figure 19.2.\n\nresults |&gt;\n  filter(effect == 0.5) |&gt;\n  pivot_longer(starts_with(\"est\"), \n               names_to = \"method\", values_to = \"est\") |&gt;\n  mutate(method = str_replace(method, \"^est.(.*)$\", \"\\\\1\")) |&gt;\n  filter(method != \"CHANGE\") |&gt;\n  group_by(method, rho) |&gt;\n  summarize(se = sd(est), .groups = \"drop\") |&gt;\n  ggplot(aes(x = rho, y = se, \n             colour = method, linetype = method)) +\n  geom_line()\n\n\n\nStandard errors as a function of rho parameter. For POST, standard errors start at around 0.10, but increase to over 0.30 as rho approaches 1. For DiD, standard errors start at around 0.15, but increase to over 0.25 as rho approaches 1. For ANCOVA, standard errors start at around 0.10 and increase to over 0.25 as rho approaches 1. Thus the plot suggests that ANCOVA generally dominates both POST and DiD.\n\nFrom the above, we can see that for low values of \\rho , subtracting pre-treatment outcome values adds noise to our estimation of treatment effects. We actually have lower standard errors when we throw away the pre-treatment data and just compare post-treatment outcomes. But for higher levels of \\rho , we see that DiD outperforms POST; by subtracting pre-treatment outcome values, we get a more precise estimate of the treatment effect. However, we see that both DiD and POST are generally outperformed by ANCOVA, which in effect allows for a flexible, data-driven relation between pre- and post-treatment outcome values.\nIn short, notwithstanding its popularity, it is far from clear that DiD is the best approach to use for all analyses of causal effects based on experimental data. Even in the context of the Reg SHO experiment, the appropriate method may depend on the outcome of interest. For a persistent outcome, DiD may be better than POST, but for a less persistent outcome, POST may be better than DiD. And ANCOVA may be a better choice than either POST or DiD unless there are strong a priori reasons to believe that DiD or POST is more appropriate (and such reasons seem more likely to hold for POST than for DiD).",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#valuating-natural-experiments",
    "href": "chap19_Natural_experiments_revisited.html#valuating-natural-experiments",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.4 valuating natural experiments",
    "text": "11.4 valuating natural experiments\nBecause of the plausibly random assignment mechanism used by the SEC, Reg SHO provides a very credible natural experiment. However, in many claimed natural experiments, it will be “nature” who is assigning treatment. In Michels (2017), it was literally nature doing the assignment through the timing of natural disasters. While natural disasters are clearly not completely random, as hurricanes are more likely to strike certain locations at certain times of year, this is not essential for the natural experiment to provide a setting from which causal inferences can be credibly drawn. What is necessary in Michels (2017) is that treatment assignment is as-if random with regard to the timing of the natural disaster before or after the end of the fiscal period and discussion questions in Chapter 17 explored these issues.\nMore often in claimed natural experiments, it will be economic actors or forces, rather than nature, assigning treatment. While such economic actors and forces are unlikely to act at random, again the critical question is whether treatment assignment is as-if random. To better understand the issues, we consider a well-studied setting, that of brokerage closures as studied in Kelly and Ljungqvist (2012).\nKelly and Ljungqvist (2012, p. 1368) argue that “brokerage closures are a plausibly exogenous source of variation in the extent of analyst coverage, as long as two conditions are satisfied. First, the resulting coverage terminations must correlate with an increase in information asymmetry. … Second, the terminations must only affect price and demand through their effect on information asymmetry.” Interestingly, these are essentially questions 2 and 3 that we ask in evaluating instrumental variables in Section 20.3. The analogue with instrumental variables applies because Kelly and Ljungqvist (2012) are primarily interested in the effects of changes in information asymmetry, not the effects of brokerage closures per se. In principle, brokerage closures could function much like an instrumental variable, except that Kelly and Ljungqvist (2012) estimate reduced-form regressions whereby outcomes are related directly to brokerage closures, such as in Table 3 (Kelly and Ljungqvist, 2012, p. 1391). But the first of the three questions from Section 20.3 remains relevant, and this is the critical question for evaluating any natural experiment: Is treatment assignment (as-if) random?\nLike many researchers, Kelly and Ljungqvist (2012) do not address the (as-if) randomness of treatment assignment directly. Instead, Kelly and Ljungqvist (2012) focus on whether brokerage closure-related terminations of analyst coverage “constitute a suitably exogenous shock to the investment environment”. Kelly and Ljungqvist (2012) argue that they do “unless brokerage firms quit research because their analysts possessed negative private information about the stocks they covered.” But this reasoning is incomplete. For sure, brokerage firms not quitting research for the reason suggested is a necessary condition for a natural experiment (otherwise the issues with using brokerage closures as a natural experiment are quite apparent). But it is not a sufficient condition. If the firms encountering brokerage closure-related terminations of analyst coverage had different trends in information asymmetry for other reasons, the lack of private information is inadequate to give us a natural experiment.\nIn general, we should be able to evaluate the randomness of treatment assignment much as we would do so with an explicitly randomized experiment. Burt (2000) suggest that “statisticians will compare the homogeneity of the treatment group populations to assess the distribution of the pretreatment demographic characteristics and confounding factors.” With explicit randomization, statistically significant differences in pretreatment variables might prompt checks to ensure that, say, there was not “deliberate alteration of or noncompliance with the random assignment code” or any other anomalies. Otherwise, we might have greater confidence that randomization was implemented effectively, and hence that causal inferences might reliably be drawn from the study.\nSo, a sensible check with a natural experiment would seem to be to compare various pre-treatment variables across treatment groups to gain confidence that “nature” has indeed randomized treatment assignment. In this regard, Table 1 of Kelly and Ljungqvist (2012) is less than assuring. Arguably, one can only encounter brokerage closure-related terminations of analyst coverage if one has analyst coverage in the first place; so the relevant comparison is arguably between the first and third columns of data. There we see that the typical firm in the terminations sample (column 1) is larger, has higher monthly stock turnover, higher daily return volatility, and more brokers covering the stock than does the typical firm in the universe of covered stocks in 2004 (column 3). So clearly “nature” has not randomly selected firms from the universe of covered stocks in 2004.\nHowever, we might come to a similar conclusion if we compared the Reg SHO pilot stocks with the universe of traded stocks or some other broad group. Just as it was essential to correctly identify the population that the SEC considered in randomizing treatment assignment, it is important to identify the population that “nature” considered in assigning treatment in evaluating any natural experiment. While the SEC provided a statement detailing how it constructed the sample, “nature” is not going to do the same and researchers need to consider carefully which units were considered for (as if) random assignment to treatment.\nIn this regard, even assuming that the controls used in Table 2 of Kelly and Ljungqvist (2012, p. 1388) were the ones “nature” herself considered, it seems that the natural experiment did not assign treatment in a sufficiently random way. Table 2 studies four outcomes: bid-ask spreads, the Amihud illiquidity measure, missing- and zero-return days, and measures related to earnings announcements. In each case, there are pre-treatment differences that sometimes exceed the DiD estimates. For example, pre-treatment bid-ask spreads for treatment and control firms are 1.126 and 1.089, a 0.037 difference that is presumably statistically significant given that the smaller DiD estimate of 0.020 has a p-value of 0.011.10 In light of this evidence, it seems that Kelly and Ljungqvist (2012) need to rely on the parallel trends assumption to draw causal inferences and we evaluate the plausibility of this assumption in the next section.\nIt is important to recognize that the shortcomings of broker closures as a natural experiment do not completely undermine the ability of Kelly and Ljungqvist (2012) to draw causal inferences. There appears to be an unfortunate tendency to believe, on the one hand, that without some kind of natural experiment, one cannot draw causal inferences. On the other hand, there is an apparent tendency to view natural experiments as giving carte blanche to researchers to draw all kinds of causal inferences, even when the alleged identification strategies do not, properly understood, support such inferences.\nIn the case of Kelly and Ljungqvist (2012), it seems the authors would like to believe that they have a natural experiment that allows them to draw inferences about the effects of broker closures on information asymmetry (Table 2) and, because broker closures only affect stock prices through their effects on information asymmetry, to conclude from the evidence in Table 3 that increases in information asymmetry reduce stock prices. But Table 2 could have been based on a bullet-proof identification strategy without implying that broker closures only affect stock prices through their effects on information asymmetry. There is really no evidence offered for this claim, one that is arguably very difficult to support.\nAt the same time, it is conceptually possible that Kelly and Ljungqvist (2012) could provide compelling evidence that the only plausible explanation for the abnormal returns in Table 3 is reductions in information asymmetry, even if the results in Table 2 were irredeemably corrupted (e.g., because of failure of parallel trends). Evidence that firms did not “quit research because their analysts possessed negative private information about the stocks they covered” might support drawing certain inferences from Table 3 even without a credible equivalent of Table 2.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#he-parallel-trends-assumption",
    "href": "chap19_Natural_experiments_revisited.html#he-parallel-trends-assumption",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.5 he parallel trends assumption",
    "text": "11.5 he parallel trends assumption\nExamining the studies of the direct effects of Reg SHO discussed in Section 19.2.3, we see that randomization generally provided balance in pre-treatment outcome values. In such settings, we see that DiD can provide unbiased estimates of causal effects, but that it has little appeal when treatment assignment is random. Indeed, if there is little to distinguish treatment and control observations in terms of pre-treatment outcome values, DiD will differ little from simply comparing differences in post-treatment means (the POST estimator discussed above).\nBut in many settings, such as that in Kelly and Ljungqvist (2012), differences in pre-treatment outcome values exist, suggesting that random assignment is not an appropriate assumption. In such settings, it will therefore be necessary to rely on a different assumption to justify the use of DiD for causal inferences. This parallel trends assumption posits that, but for treatment, the expected change in the outcome variable for the treated observations would equal that for control observations. Using this assumption, we can attribute any difference in the change in the outcome variable between the treated and control observations to a treatment effect and random variation and use standard tools of statistical inference to evaluate the null hypothesis that any difference is due to random variation.\nThat DiD can be predicated on an assumption other than (as-if) random assignment may explain its popularity. Cunningham (2021, p. 406) suggests that DiD “has become the single most popular research design in the quantitative social sciences” and Armstrong et al. (2022, p. 4) find rapid “increase in [the number of] papers using quasi-experimental methods to draw causal inferences, that more than 75% of such papers use variations of the classic difference-in-differences design.”\nArguably DiD is more often used in settings that without (as-if) random assignment of treatment. For example, one of the most highly cited papers using DiD is Card and Krueger (1994), which compares the change in employment in the fast-food industry in New Jersey and Philadelphia before and after an increase in the minimum wage in New Jersey. In this case, treatment assignment was very clearly non-random—it was a function of being located in New Jersey.\nUnlike the assumption of random assignment, the parallel trends assumption is not implied by a reasonable description of a physical or economic process and thus is of a fundamentally different nature. Random assignment is widely regarded as a reasonable description of, say, a coin toss or the generation of pseudo-random numbers using a computer. In contrast, it is difficult to think of a mechanism for imposing parallel trends on the data. Because DiD is—unlike instrumental variables or regression discontinuity designs—generally not predicated on “as-if random variation in the explanatory variable of interest”, it is not correct to consider DiD as a quasi-experimental method (Armstrong et al., 2022, p. 3).11\nInstead the basis for the parallel trends assumption appears to be that it is the assumption that is necessary (and sufficient) for the DiD estimator to provide an unbiased estimator of the causal effect of treatment. But “assuming a can-opener” seems to be a weak foundation for an approach as widespread as DiD.\nOn the one hand, as discussed above, there is no obvious economic basis for the parallel trends assumption with general applicability. On the other hand, there are often reasons to believe that the parallel trends assumption will not hold for various outcomes. The parallel trends assumption will be dubious when the outcome variable tends to be mean-reverting. For example, it is well known that accounting-based measures of operating performance tend to revert towards the mean. So if treatment and control observations have different levels of pre-treatment operating performance, the parallel trends assumption will be a highly dubious basis for causal inference.\nAnother reason to doubt the parallel trends assumption is the fact that the measurement of outcomes is often arbitrary. For example, Li et al. (2018) examine the effect of legal changes on disclosure of customer identities using a variant of DiD.12 One primary outcome measure considered by Li et al. (2018) is (), the proportion of significant customers whose identities are not disclosed. But if the parallel trends assumption holds in () then, so long as there are pre-treatment differences between treatment and control observations, it is not mathematically possible for parallel trends to hold in ((1 + )), which is the measure used in the regression analysis in Li et al. (2018).\nThe apparent flimsiness of the parallel trends assumption underlying DiD analysis in non-randomized settings is perhaps reinforced by the treatment of DiD in textbooks. Imbens and Rubin (2015), a significant recent tome on causal inference, buries DiD in endnotes, merely noting that DiD is “widely used” (2015, p. 44) before directing the reader to Angrist and Pischke (2008). While Angrist and Pischke (2008) discuss DiD and its assumptions, and relate it to fixed-effects regressions and panel data methods, they do little to justify the parallel trends assumption. Cunningham (2021) is much more cautious in discussing the parallel trends assumption, which he notes “is by definition untestable since we cannot observe this counterfactual conditional expectation [of post-treatment outcomes absent treatment]”.\nTwo popular approaches to address the parallel trends assumption are discussed by Cunningham (2021) and Huntington-Klein (2021). The first approach compares the trends in pre-treatment outcome values for treatment and control observations. If these trends are similar before treatment, it is perhaps reasonable to assume they are similar after treatment. But Cunningham (2021, p. 426) notes that “pre-treatment similarities are neither necessary nor sufficient to guarantee parallel counterfactual trends” and this seems an especially dubious assumption if treatment is endogenously selected.\nThe second approach is the placebo test, variants of which are discussed by Cunningham (2021) and Huntington-Klein (2021). One placebo test involves evaluating the treatment effect in a setting where prior beliefs hold that there should be no effect. Another approach involves a kind of random assignment of a pseudo-treatment. In either case, not finding an effect is considered as providing support for the parallel trends assumption in the analysis of greater interest to the researcher. Of course, one might be sceptical of such placebo tests in light of the concerns raised at the start of this chapter. If applying DiD to state-level data on spending on science, space, and technology provides evidence of an effect on suicides by hanging, strangulation, and suffocation, not finding an effect on deaths by drowning after falling out of a canoe or kayak may provide limited assurance.\nTo illustrate, we now apply a kind of placebo test to evaluate the parallel trends assumption for bid-ask spreads, one of the variables considered in the DiD analysis of Table 2 of Kelly and Ljungqvist (2012) (we choose spreads in part because it is easy to calculate).\nWe first create the data set spreads, which contains data on the average spread for stocks over three-month periods—aligning with one measure used Table 2 of Kelly and Ljungqvist (2012)—for a sample period running from Q1, 2001 (the first quarter of 2001) to Q1, 2008, which is the sample period in Kelly and Ljungqvist (2012). We will conduct a study of a pseudo-treatment that we will assume applies for periods beginning in Q1, 2004, which is roughly halfway through the sample period and we code post accordingly.\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\ndsf &lt;- tbl(db, Id(schema = \"crsp\", table = \"dsf\"))\n\nspreads &lt;-\n  dsf |&gt;\n  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),\n         quarter = as.Date(floor_date(date, 'quarter'))) |&gt;\n  group_by(permno, quarter) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(post = quarter &gt;= \"2004-01-01\") |&gt;\n  filter(!is.na(spread), \n         between(quarter, \"2000-01-01\", \"2008-01-01\")) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\ndsf &lt;- load_parquet(db, schema = \"crsp\", table = \"dsf\")\n\nspreads &lt;-\n  dsf |&gt;\n  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),\n         quarter = as.Date(floor_date(date, 'quarter'))) |&gt;\n  group_by(permno, quarter) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(post = quarter &gt;= \"2004-01-01\") |&gt;\n  filter(!is.na(spread), \n         between(quarter, \"2000-01-01\", \"2008-01-01\")) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\nWe now randomize treatment assignment. Because we want to evaluate the parallel trends assumption and completely randomized treatment assignment implies a trivial version of the parallel trends assumption, we specify a small difference in the probability of receiving treatment for observations whose pre-treatment spread exceeds the median ((p = 0.55)) from those whose pre-treatment spread is below the median ((p = 0.45)). This ensures that we have pre-treatment differences and thus need to rely on the parallel trends assumption in a meaningful way.13\n\nset.seed(2021)\n\ntreatment &lt;-\n  spreads |&gt;\n  filter(!post) |&gt;\n  group_by(permno) |&gt;\n  summarize(spread = mean(spread, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(treat_prob = if_else(spread &gt; median(spread), 0.55, 0.45),\n         rand = runif(n = nrow(pick(everything()))),\n         treat = rand &lt; treat_prob) |&gt;\n  select(permno, treat)\n\nObviously the null hypothesis of zero treatment effect holds with this “treatment”, but the question is whether the parallel trends assumption holds for spread. If we find evidence of a “treatment effect”, the only sensible interpretation is a failure of the parallel trends assumption for spread.\nMerging in the treatment data set, we estimate a DiD regression (and cluster standard errors by permno for reasons that will be apparent after reading the discussion below). Results are reported in Table 19.2.\n\nreg_data &lt;-\n  spreads |&gt;\n  inner_join(treatment, by = \"permno\") \n\nfm &lt;- feols(spread ~ post * treat, vcov = ~ permno, data = reg_data)\n\n\nmodelsummary(fm,\n             estimate = \"{estimate}{stars}\",\n             gof_map = c(\"nobs\"),\n             stars = c('*' = .1, '**' = 0.05, '***' = .01))\n\nBecause we find a statistically significant effect of -0.343 with a t-statistic of -4.96 with this meaningless “treatment”, we can conclude with some confidence that the parallel trends assumption simply does not hold for spread in this sample. Given that we might have passed this placebo test even if the parallel trends assumption did not hold for a particular treatment, say due to endogenous selection, it seems reasonable to view this test as being better suited to detecting a failure of parallel trends (as it does here) than it is to validation of that assumption.\nCunningham (2021) and Huntington-Klein (2021) provide excellent pathways to a recent literature examining DiD. However, it is important to recognize that some variant of the scientifically flimsy parallel trends assumption imbues all of these treatments. It would seem to be productive for researchers to discard the “quasi-experimental” pretence attached to DiD and to apply techniques appropriate to what some call interrupted time-series designs (e.g., Shadish et al., 2002).14\nWhile a randomized experiment provides a sound basis for attributing observed differences in outcomes to either treatment effects or sampling variation, without such randomization, it is perhaps more appropriate to take a more abductive approach of identifying causal mechanisms, deeper predictions about the timing and nature of causal effects, explicit consideration of alternative explanations, and the like (Armstrong et al., 2022; Heckman and Singer, 2017). Some evidence of this is seen in the discussion of specific papers in Cunningham (2021), perhaps reflecting reluctance to lean too heavily on the parallel trends assumption.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#ndirect-effects-of-reg-sho",
    "href": "chap19_Natural_experiments_revisited.html#ndirect-effects-of-reg-sho",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.6 ndirect effects of Reg SHO",
    "text": "11.6 ndirect effects of Reg SHO\nThe early studies of Reg SHO discussed above can be viewed as studying the more direct effects of Reg SHO. As a policy change directly affecting the ability of short-sellers to trade in securities, the outcomes studied by these earlier studies are more closely linked to the Reg SHO pilot than are the outcomes considered in later studies. Black et al. (2019, pp. 2–3) point out that “despite little evidence of direct impact of the Reg SHO experiment on pilot firms, over 60 papers in accounting, finance, and economics report that suspension of the price tests had wide-ranging indirect effects on pilot firms, including on earnings management, investments, leverage, acquisitions, management compensation, workplace safety, and more.”\nOne indirect effect of short-selling that has been studied in subsequent research is that on earnings management. To explore this topic, we focus on Fang et al. (2016, p. 1251), who find “that short-selling, or its prospect, curbs earnings management.”\n\n11.6.1 Earnings management after Dechow et al. (1995)\nIn Chapter 16, we saw that early earnings management research used firm-specific regressions in estimating standard models such as the Jones (1991) model. Fang et al. (2016) apply subsequent innovations in measurement of earnings management, such the performance-matched discretionary accruals measure developed in Kothari et al. (2005).\nKothari et al. (2005) replace the firm-specific regressions seen in Dechow et al. (1995) with regressions by industry-year, where industries are defined as firms grouped by two-digit SIC codes. Kothari et al. (2005) also add an intercept term to the Jones Model and estimate discretionary accruals under the Modified Jones Model by applying the coefficients from the Jones Model to the analogous terms of the Modified Jones Model.15\nTo calculate performance-matched discretionary accruals, Kothari et al. (2005, p. 1263) “match each sample firm with the firm from the same fiscal year-industry that has the closest return on assets as the given firm. The performance-matched discretionary accruals … are then calculated as the firm-specific discretionary accruals minus the discretionary accruals of the matched firm.” This is the primary measure of earnings management used by Fang et al. (2016), but note that Fang et al. (2016) use the 48-industry Fama-French groupings, rather than the two-digit SIC codes used in Kothari et al. (2005).\n\n11.6.2 FHK: Data steps\nTo construct measures of discretionary accruals, Fang et al. (2016) obtain data primarily from Compustat, along with data on Fama-French industries from Ken French’s website and data on the SHO pilot indicator from the SEC’s website. The following code is adapted from code posted by the authors of Fang et al. (2016), which was used to produce the results found in Tables 15 and 16 of Fang et al. (2019). The bulk of the Compustat data used in Fang et al. (2016) come from comp.funda. Following Fang et al. (2019), the code below collects data from that table for fiscal years between 1999 and 2012, excluding firms with SIC codes between 6000 and 6999 or between 4900 and 4949.\n\n\nPostgreSQL\nparquet\n\n\n\n\ndb &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nfunda &lt;- tbl(db, Id(schema = \"comp\", table = \"funda\"))\n\ncompustat_annual &lt;-\n  funda |&gt;\n  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', \n         between(fyear, 1999, 2012),\n         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |&gt;\n  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,\n         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, \n         rect, ceq, csho, prcc_f) |&gt;\n  mutate(fyear = as.integer(fyear)) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nfunda &lt;- load_parquet(db, schema = \"comp\", table = \"funda\")\n\ncompustat_annual &lt;-\n  funda |&gt;\n  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', \n         between(fyear, 1999, 2012),\n         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |&gt;\n  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,\n         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, \n         rect, ceq, csho, prcc_f) |&gt;\n  mutate(fyear = as.integer(fyear)) |&gt;\n  collect()\n\ndbDisconnect(db)\n\n\n\n\nSome regressions in Fang et al. (2019) consider controls for market-to-book, leverage, and return on assets, which are calculated as mtob, leverage, and roa, respectively, in the following code:\n\ncontrols_raw &lt;-\n  compustat_annual |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  mutate(lag_fyear = lag(fyear),\n         mtob = if_else(lag(ceq) != 0, \n                        lag(csho) * lag(prcc_f) / lag(ceq), NA),\n         leverage = if_else(dltt + dlc + seq != 0, \n                            (dltt + dlc) / (dltt + dlc + seq), NA),\n         roa = if_else(lag(at) &gt; 0, oibdp / lag(at), NA)) |&gt;\n  filter(fyear == lag(fyear) + 1) |&gt;\n  ungroup() |&gt;\n  select(gvkey, datadate, fyear, at, mtob, leverage, roa)\n\nFollowing Fang et al. (2019), we create controls_filled, which uses [fill()](https://tidyr.tidyverse.org/reference/fill.html) to remove many missing values for the controls.\n\ncontrols_filled &lt;-\n  controls_raw |&gt;\n  group_by(gvkey) |&gt;\n  arrange(fyear) |&gt;\n  fill(at, mtob, leverage, roa) |&gt;\n  ungroup()\n\nFollowing Fang et al. (2019), we create controls_fyear_avg, which calculates averages of controls by fiscal year.\n\ncontrols_fyear_avg &lt;-\n  controls_filled |&gt;\n  group_by(fyear) |&gt;\n  summarize(across(c(at, mtob, leverage, roa), \n                   \\(x) mean(x, na.rm = TRUE)))\n\nLike Fang et al. (2019), we use values from controls_fyear_avg to replace missing values for controls.\n\ndf_controls &lt;-\n  controls_filled |&gt;\n  inner_join(controls_fyear_avg, by = \"fyear\", suffix = c(\"\", \"_avg\")) |&gt;\n  mutate(at = coalesce(at, at_avg),\n         mtob = coalesce(mtob, mtob_avg),\n         leverage = coalesce(leverage, leverage_avg),\n         roa = coalesce(roa, roa_avg)) |&gt;\n  select(gvkey, fyear, at, mtob, leverage, roa)\n\nThere are multiple steps in the code above and the reasons for the steps involved in calculating controls_filled from controls_raw and df_controls from controls_filled are explored in the exercises below.\nAs discussed above, FHK estimate discretionary-accrual models by industry and year, where industries are based on the Fama-French 48-industry grouping. To create these industries here, we use [get\\_ff\\_ind()](https://rdrr.io/pkg/farr/man/get_ff_ind.html), introduced in Chapter 9 and provided by the farr package.\n\nff_data &lt;- get_ff_ind(48)\n\nWe now create functions to compile the data we need to estimate performance-matched discretionary accruals. We use a function to compile the data because (i) doing so is easy in R and (ii) it allows us to re-use code much easily, which will be important for completing the exercises in this chapter.\nThe first function we create is get_das(), which takes as its first argument (compustat) a data set derived from Compustat with the requisite data. The second argument (drop_extreme) allows to easily tweak the handling of outliers in a way examined in the exercises.\nWithin get_das(), the first data set we compile is for_disc_accruals, which contains the raw data for estimating discretionary-accruals models. Following FHK, we require each industry-year to have at least 10 observations for inclusion in our analysis and impose additional data requirements, some of which we explore in the exercises below.\nFollowing FHK, we estimate discretionary-accrual models by industry and year and store the results in the data frame fm_da. We then merge the underlying data (for_disc_accruals) with fm_da to use the estimated models to calculate non-discretionary accruals (nda). Because the coefficient on sale_c_at is applied to salerect_c_at, we cannot use [predict()](https://rdrr.io/r/stats/predict.html) or [residuals()](https://rdrr.io/r/stats/residuals.html) in a straightforward fashion and need calculate nda “by hand”. We then calculate discretionary accruals (da = acc_at - nda) and return the data.\n\nget_das &lt;- function(compustat, drop_extreme = TRUE) {\n  \n  for_disc_accruals &lt;-\n    compustat |&gt;\n    inner_join(ff_data, \n               join_by(between(sich, sic_min, sic_max))) |&gt;\n    group_by(gvkey, fyr) |&gt;\n    arrange(fyear) |&gt;\n    filter(lag(at) &gt; 0) |&gt;\n    mutate(lag_fyear = lag(fyear),\n           acc_at = (ibc - (oancf - xidoc)) / lag(at),\n           one_at = 1 / lag(at),\n           ppe_at = ppegt / lag(at),\n           sale_c_at = (sale - lag(sale)) / lag(at),\n           salerect_c_at = ((sale - lag(sale)) - \n                              (rect - lag(rect))) / lag(at)) |&gt;\n    ungroup() |&gt;\n    mutate(keep = case_when(drop_extreme ~ abs(acc_at) &lt;= 1,\n                            .default = TRUE)) |&gt;\n    filter(lag_fyear == fyear - 1,\n           keep, \n           !is.na(salerect_c_at), !is.na(acc_at), !is.na(ppe_at)) |&gt;\n    group_by(ff_ind, fyear) |&gt;\n    mutate(num_obs = n(), .groups = \"drop\") |&gt;\n    filter(num_obs &gt;= 10) |&gt;\n    ungroup()\n  \n  fm_da &lt;-\n    for_disc_accruals |&gt;\n    group_by(ff_ind, fyear) |&gt;\n    do(model = tidy(lm(acc_at ~ one_at + sale_c_at + ppe_at, data = .))) |&gt;\n    unnest(model) |&gt;\n    select(ff_ind, fyear, term, estimate) |&gt;\n    pivot_wider(names_from = \"term\", values_from = \"estimate\", \n                names_prefix = \"b_\")\n  \n  for_disc_accruals |&gt;\n    left_join(fm_da, by = c(\"ff_ind\", \"fyear\")) |&gt;\n    mutate(nda = `b_(Intercept)` + one_at * b_one_at + ppe_at * b_ppe_at + \n                   salerect_c_at * b_sale_c_at,\n           da = acc_at - nda) |&gt;\n    select(gvkey, fyear, ff_ind, acc_at, da) \n}\n\nThe next step in the data preparation process is to match each firm with another based on performance. Following FHK, we calculate performance as lagged “Income Before Extraordinary Items” (ib) divided by lagged “Total Assets” (at) and perf_diff, the absolute difference between performance for each firm-year and each other firm-year in the same industry. We then select the firm (gvkey_other) with the smallest value of perf_diff. We rename the variable containing the discretionary accruals of the matching firm as da_other and calculate performance-matched discretionary accruals (da_adj) as the difference between discretionary accruals for the target firm (da) and discretionary accruals for the matched firm (da_other), and return the results. Note that get_pm() includes the argument pm_lag with default value TRUE. If pm_lag is set to FALSE, then performance for matching is calculated using contemporary values of ib and at (this option is examined in the exercises below).\n\nget_pm &lt;- function(compustat, das, pm_lag = TRUE, drop_extreme = TRUE) {\n  \n  das &lt;- get_das(compustat, drop_extreme = drop_extreme)\n  \n  perf &lt;-\n    compustat |&gt;\n    group_by(gvkey) |&gt;\n    arrange(fyear) |&gt;\n    mutate(ib_at = \n      case_when(pm_lag ~ if_else(lag(at) &gt; 0, lag(ib) / lag(at), NA),\n                .default = if_else(at &gt; 0, ib / at, NA))) |&gt;\n    ungroup() |&gt;\n    inner_join(das, by = c(\"gvkey\", \"fyear\")) |&gt;\n    select(gvkey, fyear, ff_ind, ib_at)\n  \n  perf_match &lt;-\n    perf |&gt;\n    inner_join(perf, by = c(\"fyear\", \"ff_ind\"),\n               suffix = c(\"\", \"_other\")) |&gt;\n    filter(gvkey != gvkey_other) |&gt;\n    mutate(perf_diff = abs(ib_at - ib_at_other)) |&gt;\n    group_by(gvkey, fyear) |&gt;\n    filter(perf_diff == min(perf_diff)) |&gt;\n    select(gvkey, fyear, gvkey_other)\n  \n  perf_matched_accruals &lt;- \n    das |&gt;\n    rename(gvkey_other = gvkey,\n           da_other = da) |&gt;\n    select(fyear, gvkey_other, da_other) |&gt;\n    inner_join(perf_match, by = c(\"fyear\", \"gvkey_other\")) |&gt;\n    select(gvkey, fyear, gvkey_other, da_other)\n  \n  das |&gt;\n    inner_join(perf_matched_accruals, by = c(\"gvkey\", \"fyear\")) |&gt;\n    mutate(da_adj = da - da_other) |&gt;\n    select(gvkey, fyear, acc_at, da, da_adj, da_other, gvkey_other)\n}\n\nThe final step is performed in get_pmdas(). This function gets the needed data using get_pm(), then filters duplicate observations based on (gvkey, fyear) (the rationale for this step is explored in the discussion questions).\n\nget_pmdas &lt;- function(compustat, pm_lag = TRUE, drop_extreme = TRUE) {\n  \n  get_pm(compustat, \n         pm_lag = pm_lag,\n         drop_extreme = drop_extreme) |&gt;\n    group_by(gvkey, fyear) |&gt;\n    filter(row_number() == 1) |&gt;\n    ungroup() \n}\n\nFinally, we simply pass the data set compustat_annual to get_pmdas() and store the result in pmdas.\n\npmdas &lt;- get_pmdas(compustat_annual)\n\nThe remaining data set used by FHK is sho_data, which we discussed in Section 19.2.1.\n\nsho_data &lt;- \n  fhk_pilot |&gt;\n  select(gvkey, pilot) |&gt;\n  distinct() |&gt;\n  group_by(gvkey) |&gt;\n  filter(n() == 1) |&gt;\n  ungroup() |&gt;\n  inner_join(fhk_pilot, by = c(\"gvkey\", \"pilot\")) \n\nThe final sample sho_accruals is created in the following code and involves a number of steps. We first merge data from FHK’s sho_data with fhk_firm_years to produce the sample firm-years and treatment indicator for FHK. As fhk_firm_years can contain multiple years for each firm, so we expect each row in sho_data to match multiple rows in fhk_firm_years. At the same time, some gvkey values link with multiple PERMNOs, so some rows in fhk_firm_years will match multiple rows in sho_data. As such, we set relationship = \"many-to-many\" in this join below. We then merge the resulting data set with df_controls, which contains data on controls. The final data merge brings in data on performance-matched discretionary accruals from pm_disc_accruals_sorted. Finally, following FHK, we winsorize certain variables using the [winsorize()](https://rdrr.io/pkg/farr/man/winsorize.html) function from the farr package to do this here.16\n\nwin_vars &lt;- c(\"at\", \"mtob\", \"leverage\", \"roa\", \"da_adj\", \"acc_at\")\n\nsho_accruals &lt;-\n  sho_data |&gt;\n  inner_join(fhk_firm_years, \n             by = \"gvkey\",\n             relationship = \"many-to-many\") |&gt;\n  select(gvkey, datadate, pilot) |&gt;\n  mutate(fyear = year(datadate) - (month(datadate) &lt;= 5)) |&gt;\n  left_join(df_controls, by = c(\"gvkey\", \"fyear\")) |&gt;\n  left_join(pmdas, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(fyear) |&gt;\n  mutate(across(all_of(win_vars),\n                \\(x) winsorize(x, prob = 0.01))) |&gt;\n  ungroup()\n\n\n11.6.3 Discussion questions and exercises\n\nWhat would be the effect of replacing the code that creates ff_data above with the following code? What changes would we need to make to the code creating for_disc_accruals in get_das() to use this modified version of ff_data?\n\n\nff_data &lt;- \n  get_ff_ind(48) |&gt;\n  rowwise() |&gt;\n  mutate(sich = list(seq(from = sic_min, to = sic_max))) |&gt; \n  unnest(sich)\n\n\nWhat issue is filter(row_number() == 1) addressing in the code above? What assumptions are implicit in this approach? Do these assumptions hold in this case? What would be an alternative approach to address the issue?\nWhy is filter(fyear == lag(fyear) + 1) required in the creation of controls_raw?\nDoes the argument for using salerect_c_at * b_sale_c_at in creating non-discretionary accruals make sense to you? How do Kothari et al. (2005) explain this?\nDoes the code above ensure that a performance-matched control firm is used as a control just once? If so, which aspect of the code ensures this is true? If not, how might you ensure this and does this cause problems? (Just describe the approach in general; no need to do this.)\nWhat are FHK doing in the creation of controls_filled? (Hint: The key “verb” is [fill()](https://tidyr.tidyverse.org/reference/fill.html).) Does this seem appropriate? Does doing this make a difference?\nWhat are FHK doing in the creation of df_controls from controls_fyear? Does this seem appropriate? Does doing this make a difference?\n\n11.6.4 FHK: Regression analysis\nFHK consider a number of regression specifications including: with and without controls, with and without firm fixed effects, and with standard errors clustered by firm alone and clustered by firm and year. We make a small function (reg_year_fe()) that calculates variables used in the regression (like during and post) and allows us to specify each of these different options, to change the dependent variable from the default (dv = \"da_adj\") and to supply a different data set. This function returns a fitted model that is estimated using [feols()](https://lrberge.github.io/fixest/reference/feols.html) from the fixest package.\n\nctrls_list &lt;- c(\"log(at)\", \"mtob\", \"roa\", \"leverage\")\n\nreg_year_fe &lt;- function(df, dv = \"da_adj\",\n                        controls = TRUE, firm_fe = FALSE, cl_2 = TRUE,\n                        vcov = NULL) {\n  df &lt;- \n    df |&gt;\n    mutate(year = year(datadate),\n           during = year %in% c(2005, 2006, 2007),\n           post = year %in% c(2008, 2009, 2010))\n  \n  model &lt;- str_c(dv, \" ~ pilot * (during + post) \",\n                 if_else(controls, \n                         str_c(\" + \", str_c(ctrls_list, \n                                            collapse = \" + \")), \"\"),\n                    if_else(firm_fe, \"| gvkey + year \", \"| year \"))\n  if (is.null(vcov)) {\n    vcov = as.formula(if_else(!cl_2, \"~ gvkey \", \"~ year + gvkey\"))\n  }\n  \n  feols(as.formula(model), \n        vcov = vcov,\n        notes = FALSE,\n        data = df)\n}\n\nTo facilitate the output of variations, we next make a function that runs regressions with and without controls and with and without firm fixed effects and returns a nicely formatted regression table.\n\nmake_reg_table &lt;- function(df, dv = \"da_adj\", cl_2 = TRUE) {\n  omit &lt;- str_c(\"^(\", str_c(str_replace_all(c(\"during\", \"post\", ctrls_list),\n                                            \"[()]\", \".\"), \n                            collapse=\"|\"), \")\")\n  \n  run_reg &lt;- function(controls, firm_fe) {\n    reg_year_fe(df, dv = dv, controls = controls, firm_fe = firm_fe,\n                cl_2 = cl_2)\n  }\n  \n  params &lt;- tibble(controls = c(FALSE, TRUE, FALSE, TRUE),\n                   firm_fe = c(FALSE, FALSE, TRUE, TRUE))\n  \n  fms &lt;- pmap(params, run_reg)\n  \n  notes &lt;- tribble(~term,  ~`1`,  ~`2`, ~`3`, ~`4`,\n                   \"Firm FEs\", \"No\", \"No\", \"Yes\", \"Yes\",\n                   \"Controls\", \"No\", \"Yes\", \"No\", \"Yes\")\n  \n  modelsummary(fms,\n               estimate = \"{estimate}{stars}\",\n               gof_map = \"nobs\",\n               stars = c('*' = .1, '**' = 0.05, '***' = .01),\n               coef_omit = str_c(str_replace_all(ctrls_list, \"[()]\", \".\"),\n                                 collapse = \"|\"),\n               add_rows = notes)\n}\n\nWe now use this function with our version of FHK’s data set (sho_accruals) to create the regression results reported in Table 19.3.\n\nmake_reg_table(sho_accruals)\n\nWe next create a function that allows us to plot by-year coefficients for the treatment and control firms. (We leave the details of what this function is doing as an exercise for the reader below.)\n\nplot_coefficients &lt;- function(model) {\n  tibble(name = names(model$coefficients),\n         value = as.vector(model$coefficients)) |&gt;\n  filter(str_detect(name, \"^year.\")) |&gt;\n  separate(name, into = c(\"year\", \"pilot\"), sep = \":\", fill = \"right\") |&gt;\n  mutate(year = as.integer(str_replace(year, \"^year\", \"\")),\n         pilot = coalesce(pilot == \"pilotTRUE\", FALSE)) |&gt;\n  ggplot(aes(x = year, y = value, \n             linetype = pilot, color = pilot)) +\n  geom_line() +\n  scale_x_continuous(breaks = 2000:2012L) +\n  geom_rect(xmin = 2005, xmax = 2007, ymin = -Inf, ymax = Inf,\n              color = NA, alpha = 0.01) +\n  theme_bw()\n}\n\nTo produce Figure 19.3, we estimate one of the models above by year and feed the fitted model to plot_coefficients().\n\nsho_accruals |&gt;\n  mutate(year = as.factor(year(datadate))) |&gt;\n  feols(da_adj ~ year * pilot - pilot - 1 + log(at) + mtob + roa + leverage,\n        vcov = ~ year + gvkey, data = _) |&gt;\n  plot_coefficients()\n\n\n11.6.5 Exercises\n\nIn words, how does sho_accruals_alt (defined below) differ from sho_accruals? Does using sho_accruals_alt in place of sho_accruals affect the regression results?\n\n\nfirm_years &lt;-\n  controls_raw |&gt;\n  select(gvkey, datadate, fyear)\n\nsho_accruals_alt &lt;-\n  sho_r3000_gvkeys |&gt;\n  inner_join(firm_years, by = \"gvkey\") |&gt;\n  left_join(df_controls, by = c(\"gvkey\", \"fyear\")) |&gt;\n  left_join(pmdas, by = c(\"gvkey\", \"fyear\")) |&gt;\n  group_by(fyear) |&gt;\n  mutate(across(all_of(win_vars), winsorize, prob = 0.01)) |&gt;\n  ungroup()\n\n\nIn an online appendix, BDLYY say “FHK winsorize covariates for their covariate balance table at 1/99%. We inferred that they also winsorized accruals at this level. Whether they winsorize across sample years or within each year, they do not specify.” The code above winsorized within each year. How would you modify the code to winsorize “across sample years”? Does doing so make a difference?\nHow would you modify the code to winsorize at the 2%/98% level? Does this make a difference to the results? (Hint: With the farr package loaded, type [? winsorize](https://rdrr.io/pkg/farr/man/winsorize.html) in the R console to get help on this function.)\nHow would you modify the code to not winsorize at all? Does this make a difference to the results?\nSome of the studies discussed by BDLYY exclude 2004 data from the sample. How would you modify the code above to do this here? Does excluding 2004 here make a significant difference?\nWhat is the range of values for year in sho_accruals? Does this suggest any issues with the code post = year %in% c(2008, 2009, 2010) above? If so, does fixing any issue have an impact on the results reported above?\nWould it make sense, in creating perf above, if we instead calculated ib_at as if_else(at &gt; 0, ib / at, NA))? What is the effect on the regression results if we use this modified calculation of ib_at? What do Kothari et al. (2005) recommend on this point? (Hint: Use pm_lag = FALSE where applicable.)\nFang et al. (2019, p. 10) follow Fang et al. (2016), who “exclude observations for which the absolute value of total accruals-to-total assets exceeds one. This is a standard practice in the accounting literature because firms with such high total accruals-to-total assets are often viewed as extreme outliers. Nonetheless, the FHK results are robust to winsorizing the accrual measures at the 1% and 99% levels instead of excluding extreme outliers.” Does this claim hold up in the reproduction above? What happens if the [filter()](https://dplyr.tidyverse.org/reference/filter.html) on abs(acc_at) &lt;= 1 is removed from the code above? (Hint: Use drop_extreme = FALSE where applicable.)\nExplain what each line of the function plot_coefficients() before the line starting with [ggplot()](https://ggplot2.tidyverse.org/reference/ggplot.html) is doing. (Hint: It may be helpful to store the model that is fed to the function above in the variable model and then run the function line by line.)",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#tatistical-inference",
    "href": "chap19_Natural_experiments_revisited.html#tatistical-inference",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.7 tatistical inference",
    "text": "11.7 tatistical inference\nOne point of difference between FHK and BDLYY concerns clustered standard errors. Fang et al. (2016) generally use “standard errors clustered by year and firm” (2016, p. 1269), while Black et al. (2019) advocate the use of standard errors clustered by firm. Citing Cameron et al. (2008), Black et al. (2019, p. 30) suggest that “clustered standard errors with a small number of clusters can be downward biased.” In the context of FHK, there are thousands of firms, but a relatively small number of years, so clustering by year (or firm and year) may result in problematic standard error estimates (see Section 5.6.6).\nOne approach to determining the appropriate clustering is more empirical. In this regard, it is useful to note that cluster-robust standard errors are a generalization of an idea from White (1980). White (1980) provides not only an estimator of standard errors that is robust to heteroskedasticity, but also a test of a null hypothesis of homoskedasticity. Intuitively, if the covariance matrix assuming heteroskedasticity is sufficiently different from that assuming homoskedasticity, then we may reject the null hypothesis of homoskedasticity. With a little algebra, it would be possible to develop a test analogous to that of White (1980) of the null hypothesis of no clustering on variable (g). In practice, many researchers will, lacking a formally derived test, compare standard errors with and without clustering on variable (g) and elect to cluster on variable (g) when the standard errors when doing so seem significantly higher than when not doing so. This heuristic breaks down in the case of Fang et al. (2016) because standard errors are generally lower when clustering on firm and year than when clustering firm alone. However, if clustering on firm alone is appropriate, standard errors clustering on firm and year will provide noisier estimates than clustering on firm alone, and thus could be lower or higher in any given data set.\nA more theoretical approach can be used in the setting of FHK because of our deeper understanding of the assignment mechanism. In this regard, it is important to note that cluster-robust standard errors address correlation in both (X) and () across units within clusters. To explore this (slightly) more formally, recall from Chapter 5 that the cluster-robust covariance matrix is estimated using the following expression:\n\n\\begin{aligned}\n\\hat{V}(\\hat{\\beta}) =\n(X'X)^{-1} \\hat{B} (X'X)^{-1},  \\text{where}\\  \\hat{B} = \\sum_{g = 1}^G X'_g u_g u'_g X_g\n\\end{aligned}\n\nwhere the observations grouped into G clusters of N_g observations for g in {1, \\dots, G} , X_g is the N_g \\times K matrix of regressors, and u_g is the N_g -vector of residuals for cluster g .\nIf we have a single regressor, demeaned x with no constant term and two firms ( i and j ) in a cluster, then the contribution of that cluster to \\hat{B} will be\n\n\\begin{aligned}\n\\begin{bmatrix}\nx\\_i & x\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu\\_i \\\\\nu\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu\\_i & u\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\_i \\\\\nx\\_j\n\\end{bmatrix} &=\n\\begin{bmatrix}\nx\\_i & x\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nu\\_i^2 & u\\_i u\\_j \\\\\nu\\_i u\\_j & u\\_j^2\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\_i \\\\\nx\\_j\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nx\\_i & x\\_j\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\_i u\\_i^2 + x\\_j u\\_i u\\_j \\\\\nx\\_i u\\_i u\\_j + x\\_j u\\_j^2\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nx\\_i^2 u\\_i^2 + x\\_i x\\_j u\\_i u\\_j \\\\\nx\\_i x\\_j u\\_i u\\_j + x\\_j^2 u\\_j^2\n\\end{bmatrix}\n\\end{aligned}\n\nNow, if x_i and x_j are uncorrelated then, even if \\epsilon_i and \\epsilon_j are correlated, this resolves in expectation to\n\n\\begin{bmatrix}\nx\\_i^2 \\sigma\\_i^2 \\\\\nx\\_j^2 \\sigma\\_j^2\n\\end{bmatrix}\n\nwhich is the expectation of the analogous component of the heteroskedasticity-robust estimator from White (1980). In the setting of Fang et al. (2016), the “(x)” of primary interest is the Reg SHO pilot indicator, which is assumed to be randomly assigned, and thus (in expectation) uncorrelated across firms. For this reason, we do not expect cross-sectional dependence to affect standard error estimates on average. On the other hand, the Reg SHO pilot indicator is perfectly correlated over time within firm, so any serial dependence in errors within firm over time will lead to effects of time-series dependence on standard error estimates. This (somewhat loose) theoretical analysis suggests we should cluster by firm (time-series dependence), but not by year (cross-sectional dependence), as suggested by Black et al. (2019, p. 12).\nHowever, the assumed random assignment of treatment allows us to adopt an alternative approach to statistical inference that is agnostic to the form of clustering in the data. This approach is known as randomization inference and builds on the Fisher sharp null hypothesis of no effect of any kind. This is a “sharp null” because it is more restrictive that a null hypothesis of zero mean effect, which could be true even if half the observations had a treatment effect of (+1) and half the observations had a treatment effect of (-1), in which case the Fisher sharp null would not be true even though null hypothesis of zero mean effect is true.\nUnder the Fisher sharp null hypothesis and with random assignment to treatment, in principle we can evaluate the distribution of any given test statistic by considering all possible assignments. Focusing on the 2954 firms that the SEC focused on as its initial sample, if assignment to treatment were purely random, then any other assignment of treatment to 985 was as likely as the one chosen. Given that the Fisher sharp null implies that there was no impact of treatment assignment on outcomes, we know what the distribution of the test statistic would have been if the SEC had chosen any one of those alternative assignments because the outcomes would have been exactly the same. With smaller samples, we might proceed to calculate the test statistic for every possible assignment and thereby construct the exact distribution of the test statistic under the Fisher sharp null.17 But in our case, there will be a huge number of ways to choose 985 treatment firms from 2954 possibilities; so a more feasible approach is to draw a random sample of possible assignments and use the empirical distribution of the test statistic for that random sample as an approximation for the exact distribution.\n\nget_coef_rand &lt;- function(i) {\n  treatment &lt;-\n    sho_accruals |&gt;\n    select(gvkey, pilot) |&gt;\n    distinct() |&gt;\n    mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE))\n  \n  reg_data_alt &lt;-\n    sho_accruals |&gt;\n    select(-pilot) |&gt;\n    inner_join(treatment, by = \"gvkey\")\n  \n  reg_data_alt |&gt; \n    reg_year_fe(controls = TRUE, firm_fe = TRUE) |&gt; \n    broom::tidy() |&gt; \n    select(term, estimate) |&gt;\n    pivot_wider(names_from = \"term\", values_from = \"estimate\") |&gt;\n    mutate(iteration = i) |&gt;\n    suppressWarnings()\n}\n\nThe test statistic we are interested in here is the coefficient on PILOT \\times DURING . Below we calculate the p-value of the coefficients on variables involving PILOT using the empirical distribution of coefficients, and the standard errors associated with the coefficients as the standard deviation of those coefficients.\n\nset.seed(2021)\nrand_results &lt;-\n  1:1000 |&gt;\n  map(get_coef_rand) |&gt; \n  list_rbind() |&gt;\n  system_time()\n\n\nplan(multisession)\n\nrand_results &lt;- \n  1:1000 |&gt; \n  future_map(get_coef_rand, \n             .options = furrr_options(seed = 2021)) |&gt; \n  list_rbind() |&gt;\n  system_time()\n\nIn the following, we run regressions with standard errors based on clustering by firm and year, by firm alone, and using randomization inference. We start by running regressions—with controls and firm fixed effects—with standard errors based on clustering by firm (\"CL-i\") and by firm and year (\"CL-2\").\n\nfms &lt;- list(reg_year_fe(sho_accruals, cl_2 = FALSE),\n            reg_year_fe(sho_accruals, cl_2 = TRUE))\n\nWe extract the variance-covariance matrices for each of these two models and place them in the list vcovs.\n\nvcovs &lt;- list(vcov(fms[[1]]), vcov(fms[[2]]))\n\nNext, we add a third model for which we will calculate standard errors using randomization inference (\"RI\"). The coefficients stored in fms for this third model can be taken from either of the two models already stored there.\n\nfms[[3]] &lt;- fms[[2]]\n\nFor the variance-covariance matrix, we use CL-i standard errors as the starting point. Then we replace the elements for coefficients on variables involving () using the empirical distribution stored in rand_results.\n\nvcov &lt;- vcovs[[1]]\nvcov[\"pilotTRUE:duringTRUE\", \"pilotTRUE:duringTRUE\"] &lt;-\n  var(rand_results[[\"pilotTRUE:duringTRUE\"]])\nvcov[\"pilotTRUE:postTRUE\", \"pilotTRUE:postTRUE\"] &lt;- \n  var(rand_results[[\"pilotTRUE:postTRUE\"]])\nvcovs[[3]] &lt;- vcov\n\nResults of this analysis are provided in Table 19.4.\n\nse_notes &lt;- tribble(~term,  ~`1`,  ~`2`, ~`3`,\n                    \"SEs\", \"CL-i\", \"CL-2\", \"RI\")\n\nmodelsummary(fms, vcov = vcovs, \n             estimate = \"{estimate}{stars}\",\n             gof_map = \"nobs\",\n             stars = c('*' = .1, '**' = 0.05, '***' = .01),\n             coef_omit = \"^(during|post|pilot)TRUE$\",\n             add_rows = se_notes)\n\n\n11.7.1 Exercises\n\nIn the function get_coef_rand(), we first created the data set treatment, then merged this with reg_data_alt. Why did we do it this way rather than simply applying the line mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE)) directly to reg_data_alt?\nUsing randomization inference, calculate a p-value for a one-sided alternative hypothesis that H_1: \\beta &lt; 0 where \\beta is the coefficient on PILOT \\times DURING . (Hint: You should not need to run the randomization again; modifying the calculation of p_value should suffice.)\nWhat is the empirical standard error implied by the distribution of coefficients in rand_results? Is it closer to the two-way cluster robust standard errors obtained in estimating with cl_2 = TRUE or with cl_2 = FALSE? Why might it be preferable to calculate p-values under randomization inference using the empirical distribution of the test statistic, instead of calculating these from t-statistics based on the estimated coefficient and the empirical standard error? Would we get different p-values using the former approach?\nWhy did we not use the empirical standard error implied by the distribution of coefficients in rand_results to calculate standard errors for the control variables (e.g., log(at))?",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#ausal-diagrams",
    "href": "chap19_Natural_experiments_revisited.html#ausal-diagrams",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.8 ausal diagrams",
    "text": "11.8 ausal diagrams\nIt is important to note that we observe total accruals, not discretionary accruals. Instead we need to construct measures of discretionary accruals. The Jones (1991) model of discretionary accruals “controls for” sales growth and PP&E and the Kothari et al. (2005) model additionally “controls for” performance.\nAssuming that the causal diagram below is correct, we get unbiased estimates of causal effects whether we “control for” pre-treatment outcome values (e.g., using DiD) or not (e.g., using POST), and it is not clear that we need to control for other factors that drive total accruals. If being a Reg SHO pilot firm leads to a reduction in earnings management, we should observe lower total accruals, even if we posit that the effect is through discretionary accruals, which we do not observe directly. If we accept this causal diagram, then the decision as to which factors to control for is—like the choice between DiD, POST, and ANCOVA—a question of statistical efficiency rather than bias.\nIn this context, it is perhaps useful to consider causal diagrams to sharpen our understanding of the issues, which we explore in the discussion questions below, as matters can be more complicated if the causal diagram in Figure 19.4 is incomplete.\n\n11.8.1 Discussion questions\n\nWhat features of Figure 19.4 imply that we do not need to control for performance, sales, and PP&E in estimating the causal effect of Reg SHO on accruals? What is the basis for assuming these features in the causal diagram?\nBlack et al. (2024) report that “over 60 papers in accounting, finance, and economics report that suspension of the price tests had wide-ranging indirect effects on pilot firms, including on earnings management, investments, leverage, acquisitions, management compensation, workplace safety, and more (see Internet Appendix, Table IA-1 for a summary).” In light of the Internet Appendix of Black et al. (2024), is there any evidence that Reg SHO might plausibly have an effect on performance, sales growth, or PP&E? If so, how would Figure 19.4 need to be modified to account for these consequences? What would be the implications of these changes on the appropriate tests for estimating the causal effects of Reg SHO on accruals?\nProduce a regression table like Table 19.3 and a plot like Figure 19.3, but using discretionary accruals without performance matching instead of performance-matched discretionary accruals. How do you interpret these results?\nProduce a regression table and a plot like the ones in the FHK replication above, but using total accruals instead of discretionary accruals and excluding controls (so the coefficients will be simple conditional sample means). How do you interpret these results?\nSuppose you had been brought in by the SEC to design a study examining the research question examined by FHK in the form of a registered report. What analyses would you conduct to try to understand the best research design? For example, how would you choose between DiD, POST, ANCOVA, and other empirical approaches? What controls would you include? How would you decide how to include controls? (For example, one could control for performance by including performance as a regressor in the model of earnings management, by matching on performance, or by including performance in the main regression specification.) How would you calculate standard errors? Discuss how your proposed empirical test differs from that of FHK. Would you have reported similar results to what FHK reported?\nSuppose that FHK’s empirical analysis had produced a positive effect of Reg SHO on earnings management? Would this imply a lack of support for their hypotheses? Do you believe that publication in the Journal of Finance depended on finding a negative effect?\nWhat implications would there have been for publication of FHK in the Journal of Finance if they had failed to find an effect of Reg SHO on earnings management?",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  },
  {
    "objectID": "chap19_Natural_experiments_revisited.html#ausal-mechanisms",
    "href": "chap19_Natural_experiments_revisited.html#ausal-mechanisms",
    "title": "\n11  自然実験:再訪\n",
    "section": "\n11.9 ausal mechanisms",
    "text": "11.9 ausal mechanisms\nBlack et al. (2024) suggest a number of possible causal channels through which the Reg SHO experiment could have affected the behavior of firms or third parties, including short interest, returns, price efficient, and “manager fear”. On the last of these, Black et al. (2024, p. 4) suggest that “even if the Reg SHO experiment did not actually affect short interest or returns, pilot firm managers could have feared being targeted by short sellers and taken pre-emptive actions.”\nBlack et al. (2024, p. 5133) argue that “if firm managers were fearful that relaxing the price tests would affect them, one might expect them to voice concerns in various ways: speaking with business news reporters; writing to the SEC when it sought public comments, or seeking meetings with SEC officials to express opposition. … We searched the business press during 2003 when the rule was proposed, in 2004 when the experiment was announced, in 2006 when the SEC proposed repeal … We found no evidence of manager opposition.”\nBlack et al. (2024, p. 5134) suggest that “FHK rely on the manager fear channel. They conjecture that, in response to a greater threat of short selling, pilot firms’ managers reduced earnings management to preemptively deter short sellers.”\n\n11.9.1 Discussion questions\n\nDo you agree with the assertion of Black et al. (2024) that “FHK rely on the manager fear channel”? What causal mechanisms are suggested in Fang et al. (2016)? What evidence do Fang et al. (2016) offer in support of these mechanisms?\nEvaluate the response of Fang et al. (2019) to Black et al. (2024) as it relates to causal mechanisms?\nDo you think evidence of causal mechanisms is more or less important when using a natural experiment (i.e., an experiment outside the control of the researcher that is typically analysed after it has been run) than when conducting a randomized experiment? Explain your reasoning given the various issues raised in this chapter.",
    "crumbs": [
      "因果推論",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>自然実験:再訪</span>"
    ]
  }
]