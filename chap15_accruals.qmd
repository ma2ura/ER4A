# アクルーアル


<!-- In this chapter, we use Sloan (1996) to provide a focus for a study of accrual processes.  -->
本章では、アクルーアルのプロセスの研究への焦点を提供するために、Sloan (1996) を利用する。
<!-- We use simulation analysis to understand better accounting processes, with a particular focus on accruals, which for this chapter we define as the portion of earnings in excess of operating cash flows.1  -->
**アクルーアル**へに注目することで、会計プロセスより理解するためにシミュレーション分析を用いる。
この章では、アクルーアルを営業キャッシュフローを超える利益の部分と定義する。[^1]
<!-- We finish up the chapter with an examination of the so-called accrual anomaly. -->
最後に、いわゆる**アクルーアル・アノマリー**(accrual anomaly)の検討を行う。

:::{.callout-tip}
<!-- The code in this chapter uses the packages listed below.  -->
この章のコードは、以下のパッケージを使用する。
<!-- For instructions on how to set up your computer to use the code found in this book, see Section 1.2.  -->
この書籍のコードを使用するためのコンピュータのセットアップ方法については、1.2節を参照してください。
<!-- Quarto templates for the exercises below are available on GitHub. -->
以下の練習問題のQuartoテンプレートは、GitHubで入手できます。
:::


```{r}
#| eval: false

pacman::p_load(tidyverse, DBI, farr, modelsummary, dbplyr, car, furrr)
```

## Sloan (1996)


<!-- Sloan (1996) points out that a number of practitioners provide investment advice predicated on identifying firms whose earnings depend on accruals rather than cash flows.  -->
Sloan (1996) は、いくつかの実務家がキャッシュフローではなくアクルーアルに依存する企業を特定することに基づいて投資アドバイスを提供していることを指摘している。
<!-- Such investment advice is based on a claimed tendency for capital markets to “fixate” on earnings and to fail to recognize differences in the properties of cash flow and accrual components of earnings.  -->
このような投資アドバイスは、資本市場が利益に「固執」し、キャッシュフローとアクルーアルの成分の特性の違いを認識しない傾向に基づいていると主張されている。
<!-- In some respects, Sloan (1996) provides a rigorous evaluation of such investment advice and the premises underlying it. -->
ある意味で、Sloan (1996) はそのような投資アドバイスとそれに基づく前提を厳密に評価している。

<!-- ### Discussion questions -->
### ディスカッション問題

<!-- The following discussion questions provide an approach to reading Sloan (1996).  -->
以下のディスカッション質問は、Sloan (1996) を読むためのアプローチを提供します。

<!-- 1. Read the material preceding the formal statement of H1.  -->
1. $H_1$ のフォーマルな記述の前にこの資料を読みなさい。
<!-- What reasons for differential persistence of earnings components does Sloan (1996) offer?  -->
Sloan (1996) は、利益の構成要素の差異の持続性に関してどのような理由を提供しているのか？
<!-- How important is it for these reasons to be correct in light of the empirical support for H1 provided in Table 3?  -->
これらの理由が正しいことが、表3で提供されているH1の実証的な支持にとってどれほど重要か？
<!-- How important is the empirical support for H1 to H2(i)? -->
H1の実証的な支持がH2(i)にとってどれほど重要か？

<!-- 2. Which hypothesis (if any) does Table 4 test? How would you interpret the results of Table 4 in words? -->
2. 表4はどの仮説をテストしているのか？ 表4の結果を言葉でどのように解釈しているか？

<!-- 3. Which hypothesis (if any) does Table 5 test? How would you interpret the results of Table 5 in words? -->
3. 表5はどの仮説をテストしているのか？ 表5の結果を言葉でどのように解釈しているか？

<!-- 4. Which hypothesis (if any) does Table 6 test? How would you interpret the results of Table 6 in words?  -->
4. 表6はどの仮説をテストしているのか？ 表6の結果を言葉でどのように解釈しているか？
<!-- There are similarities between the results of Table 6 of Sloan (1996) and the results in Bernard and Thomas (1989).  -->
Sloan (1996) の表6の結果と Bernard and Thomas (1989) の結果には類似点がある。
<!-- Both involve forming portfolios of firms based on deciles of some variable (accruals in Sloan, 1996; earnings surprise in Bernard and Thomas, 1989) and examining how those portfolios perform subsequently.  -->
両論文は、ある変数(1996年のSloanのアクルーアル、1989年のBernard and Thomasの利益サプライズ)の十分位数に基づいて企業のポートフォリオを形成し、その後のポートフォリオのパフォーマンスを調査することを含んでいる。
<!-- Apart from the measure used to form portfolios, what are the significant differences between the analyses in the two papers that you can think of looking at Table 6? -->
ポートフォリオを形成するために使用される尺度以外に、表6を見て考えられる2つの論文の分析の重要な違いは何か？

<!-- 5. With which hypothesis (if any) is Figure 2 related? What does Figure 2 show according to Sloan (1996)? -->
5. Figure 2 は(あれば)どの仮説と関連していますか？ Sloan (1996) によると、Figure 2 は何を示していますか？

<!-- 6. With which hypothesis (if any) is Figure 3 related? What does Figure 3 show according to Sloan (1996)? -->
6. Figure 3 は(あれば)どの仮説と関連していますか？ Sloan (1996) によると、Figure 3 は何を示していますか？

<!-- ## Measuring accruals -->
## アクルーアルの測定

<!-- Hribar and Collins (2002) include a definition of accruals similar to that used in Sloan (1996).  -->
Hribar and Collins (2002) は、Sloan (1996) で使用されているものに類似したアクルーアルの定義を含んでいる。
<!-- Referring to prior research, they state (2002, p. 10): -->
先行研究を参照して、彼らは次のように述べている(2002, p. 10)。

<!-- > Specifically, accruals ( $ACC_{bs}$ ) are typically calculated (firm and time subscripts omitted for convenience): -->
> 具体的には、アクルーアル($ACC_{bs}$)は通常次のように計算される(便宜上、企業と時間の添字は省略されている)。
> $$
> ACC_{bs} = (\Delta CA - \Delta CL - \Delta Cash + \Delta STDEBT - DEP)
> $$
>
<!-- > where -->
> ここで
>
<!-- > - $\Delta CA$ = the change in current assets during period $t$  (Compustat #4) -->
> - $\Delta CA$ = 期間 $t$ 中の流動資産の変化(Compustat #4)
<!-- > - $\Delta CL$ = the change in current liabilities during period $t$  (Compustat #5) -->
> - $\Delta CL$ = 期間 $t$ 中の流動負債の変化(Compustat #5)
<!-- > - $Delta Cash$ = the change in cash and cash equivalents during period $t$  (Compustat #1); -->
> - $\Delta Cash$ = 期間 $t$ 中の現金及び現金同等物の変化(Compustat #1)
<!-- > - $\Delta STDEBT$ = the [change in] current maturities of long-term debt and other short-term debt included in current liabilities during period $t$ (Compustat #34); -->
> - $\Delta STDEBT$ = 期間 $t$ 中の流動負債に含まれる長期債務の短期債務の変化(Compustat #34)
<!-- > - and $\Delta DEP$ = depreciation and amortization expense during period $t$ (Compustat #14). -->
> - そして $\Delta DEP$ = 期間 $t$ 中の減価償却費と無形資産償却費(Compustat #14)。
>
<!-- > All variables are deflated by lagged total assets ( $TA_{t-1}$ ) to control for scale differences. -->
> すべての変数は、規模の違いを調整するために遅れた総資産( $TA_{t-1}$ )で割っている。

<!-- The first thing you may ask is “what does (say) ‘Compustat #4’ mean?”.  -->
最初に尋ねるべきことは、「(例えば) ‘Compustat #4’ とは何か？」ということだろう。
<!-- Prior to 2006, Compustat data items were referred to using numbers such as Compustat #4 or data4.  -->
2006年以前、Compustatのデータ項目は、Compustat #4やdata4などの数字を使って参照されていた。
<!-- So older papers may refer to such items.  -->
そのため、古い論文ではそのような項目が参照されることがある。
<!-- Fortunately, Wharton Research Data Services (WRDS) provides translation tables from these items to the current variables and relevant translations are provided in Table 15.1. -->
幸いなことに、Wharton Research Data Services (WRDS) はこれらの項目から現在の変数への変換テーブルを提供しており、関連する変換は表15.1に示されている。

|  Old item | Current item | Item description |
|:---------:|:------------:|:-----------------:|
| #1 | che | Cash and Short-Term Investments |
| #4 | act | Current Assets—Total |
| #5 | lct | Current Liabilities—Total |
| #14 | dp | Depreciation and Amortization |
| #34 | dlc | Debt in Current Liabilities—Total |
: Table 15.1: Translation of key pre-2006 Compustat items

<!-- Hribar and Collins (2002) point out that calculating current accruals by subtracting the change in current liabilities from the change in noncash current assets is incorrect “because other non-operating events (e.g., mergers, divestitures) impact the current asset and liability accounts with no earnings impact.” -->
Hribar and Collins (2002) は、流動資産の変化から流動負債の変化を引いて現在のアクルーアルを計算することは間違っていると指摘している。「他の非営業イベント(例えば、合併、売却)が現在の資産と負債の勘定に影響を与えるが、利益への影響はない」と述べている。


<!-- ### Discussion questions -->
### 議論する問題

<!-- 1. In the equation above, why is $|Delta Cash$ subtracted? -->
1. 上記の式では、なぜ $\Delta Cash$ が引かれているのか？

<!-- 2. In the equation above, why is $\Delta STDEBT$ added? -->
2. 上記の式では、なぜ $\Delta STDEBT$ が加算されているのか？

<!-- 3. Is it true that mergers and divestitures have “no earnings impact”? Is the absence of earnings impact important to the estimation issue? Are there transactions that have no earnings impact, but do affect cash flow from operations? -->
3. 合併や売却が「利益への影響がない」というのは本当だろうか？ 利益への影響の欠如は推定問題にとって重要だろうか？ 利益への影響がないが、営業キャッシュフローに影響を与える取引はあるだろうか？

<!-- 4. Are there any differences between Hribar and Collins (2002) (above) and Sloan (1996) in their definitions of accruals? Which definition makes more sense to you? Why? -->
4. Hribar and Collins (2002) (上記) と Sloan (1996) のアクルーアルの定義にはどのような違いがあるか？ どちらの定義がより理にかなっていると思いますか？ なぜですか？


<!-- ## Simulation analysis -->
## シミュレーション分析

<!-- We now consider some simulation analysis. -->
ここでは、いくつかのシミュレーション分析を考える。
<!-- One reason for this analysis is to better understand the basis for H1 of Sloan (1996). -->
Sloan (1996) のH1の根拠をよりよく理解するための分析の一つの理由である。

<!-- A second reason for conducting simulation analysis here is to illustrate the power it offers. -->
ここでシミュレーション分析を行う第二の理由は、その提供する力を示すことである。
<!-- In many contexts, derivation of the properties of estimators or understanding how phenomena interact is very complex. -->
多くの文脈では、推定量の性質を導出したり、現象がどのように相互作用するかを理解することは非常に複雑である。
<!-- While many researchers rely on intuition to guide their analyses, such intuition can be unreliable. -->
多くの研究者は、自分の分析を導くために直感に頼るが、そのような直感は信頼性に欠けることがある。
<!-- As an example, the idea that the “FM-NW” method provides standard errors robust to both time-series and cross-sectional dependence has strong intuitive appeal, but we saw in Chapter 5 that this intuition is simply wrong. -->
例えば、「FM-NW」法が時系列依存性と断面依存性の両方に対して頑健な標準誤差を提供するという考えは、強い直感的な魅力があるが、第5章でその直感が単純に間違っていることを見た。

<!-- ### Vectors -->
### ベクトル

<!-- In our simulation analysis, we make more extensive use of base R functionality than we have in prior chapters. -->
シミュレーション分析では、これまでの章よりも、基本的なRの機能をより広範に使用する。
<!-- Chapter 28 of R for Data Science - "A field guide to base R" - provides material that might be helpful if code in the next section is unclear. -->
R for Data Science の第28章「基本Rのフィールドガイド」は、次のセクションのコードがわかりにくい場合に役立つかもしれません。
<!-- Here we are simulating the cash flows and accounting for a simple firm that buys goods for cash and sells them on account after adding a mark-up. -->
ここでは、現金フローをシミュレーションし、現金で商品を購入し、マークアップを加えて売却する単純な企業の会計を考慮している。

<!-- ### Simulation function -->
### シミュレーション関数

<!-- As we have seen before, it is often a good coding practice to use functions liberally in analysis.2  -->
前に見たように、分析では関数を自由に使用することが良いコーディングの習慣である。[^2]
<!-- In this case, we embed the core of the simulation in a function. -->
この場合、シミュレーションの中核を関数に埋め込む。

<!-- The simulation function get_data() below generates a time-series of data for a single "firm" and accepts two arguments. -->
以下のシミュレーション関数 `get_data()` は、単一の「企業」のデータの時系列を生成し、2つの引数を受け入れる。
<!-- The first argument to get_data() is add_perc, which has a default value of 0.03. -->
`get_data()` の最初の引数は `add_perc` で、デフォルト値は`0.03`である。
<!-- The value of add_perc drives the amount of allowance for doubtful debts. -->
`add_perc` の値は、貸倒引当金の額を決定する。
<!-- The second argument is n_years, which has a default value of 20. -->
2番目の引数は `n_years` で、デフォルト値は`20`である。
<!-- The value of n_years drives the number of years of data generated by the simulation. -->
`n_years` の値は、シミュレーションで生成されるデータの年数を決定する。

<!-- The simulation generates various cash flows and the financial statements to represent them. -->
シミュレーションは、さまざまなキャッシュフローとそれを表す財務諸表を生成する。
<!-- The main driver of the model is sales, which follows an autoregressive process. -->
モデルの主要なドライバーは売上であり、自己回帰プロセスに従う。
<!-- Denoting sales in period $t$ as $S_t$, we have -->
$t$ 期の売上を $S_t$ と表すと、次のようになる。

$$
S_t - \bar S = \rho (S_{t-1} - \bar S) + \varepsilon _t
$$

<!-- where -->
ここで， $\rho \in (0,1)$ と $\varepsilon _t \sim N(0, \sigma^2)$ である。


<!-- Sales then drives both cost of goods sold, which are assumed to require cash outlays in the period of sale, and accounts receivable, as all sales are assumed to be on account. -->
売上は、売上時に現金支出が必要とされると仮定される売買費用と、すべての売上が勘定に記載されると仮定される売掛金を推進する。
<!-- The model also addresses collections, write-offs, and dividends. -->
モデルは、回収、債権放棄、配当にも対応している。
<!-- There are no inventories in our model. -->
当社のモデルには在庫はありません。

<!-- In the simulation function, we use “base R” functionality to a fair degree. -->
シミュレーション関数では、「基本R」の機能をかなり使用している。
<!-- Rather than using mutate() to generate variables, we refer to variables using $ notation, which returns the variable as a vector. -->
変数を生成するために `mutate()` を使用する代わりに、変数をベクトルとして返す `$` 表記を使用して変数を参照する。
<!-- For example df |> select(ni) returns a data frame with a single column. -->
例えば、`df |> select(ni)` は、単一の列を持つデータフレームを返す。
<!-- In contrast, df$ni gets the same underlying data, but as a vector. -->
対照的に、`df$ni` は、同じ基礎データを取得するが、ベクトルとして取得する。
<!-- To calculate shareholders’ equity (se), we set the initial ( $t=0$ ) value to beg_se. -->
株主資本（se）を計算するために、初期値（ $t=0$ ）を `beg_se` に設定する。
<!-- Then we calculate the ending balance of shareholders’ equity as beginning shareholders’ equity plus net income minus dividends. -->
次に、株主資本の期末残高を、初期株主資本に純利益を加え、配当を引いたものとして計算する。

```{r}
#| eval: false

get_data <- function(add_perc = 0.03, n_years = 20) {

    # パラメータの設定
    add_true   <- 0.03
    gross_margin <- 0.8
    beg_cash   <- beg_se <- 1500
    div_payout <- 1
    mean_sale  <- 1000
    sd_sale    <- 100
    rho        <- 0.9

    # Generate sales as an AR(1) process around mean_sale
    sale_err <- rnorm(n_years, sd = sd_sale) # 売上の誤差を作成
    sales <- vector("double", n_years) # 売上を格納するベクトルを作成
    sales[1] <- mean_sale + sale_err[1] # 平均売上高に誤差を加える
    for (i in 2:n_years) {              # AR(1) 仮定に基づいて売上を生成
      sales[i] = mean_sale + rho * ( sales[i-1] - mean_sale ) + sale_err[i]
    }

    # データを結合してdata.frameを作成
    df <- tibble(year = 1:n_years,
                 add_perc = add_perc,
                 sales,
                 # 初期値は欠損値で
                 writeoffs = NA, collect = NA,
                 div = NA, se = NA, ni = NA,
                 bde = NA, cash = NA)

    # COGSは売上高の一定割合と仮定
    df$cogs <- (1 - gross_margin) * df$sales

    # 売上は全て掛売上で，回収・貸倒損失は翌期
    df$ar <- df$sales

    # 貸倒引当金addを設定
    df$add <- add_perc * df$sales

    # 前期の値を設定
    df$writeoffs[1] <- 0
    df$collect[1] <- 0
    df$bde[1] <- df$add[1]
    df$ni[1] <- df$sales[1] - df$cogs[1] - df$bde[1]
    df$div[1] <- df$ni[1] * div_payout
    df$cash[1] <- beg_cash + df$collect[1] - df$cogs[1] - df$div[1]
    df$se[1] <- beg_se + df$ni[1] - df$div[1]

    # 2年目からループ設定
    for (i in 2:n_years) {
      df$writeoffs[i] <- add_true * df$ar[i-1]
      df$collect[i] <- (1 - add_true) * df$ar[i-1]
      df$bde[i] = df$add[i] - df$add[i-1] + df$writeoffs[i]
      df$ni[i] <- df$sales[i] - df$cogs[i] - df$bde[i]
      df$div[i] <- df$ni[i] * div_payout
      df$cash[i] <- df$cash[i-1] + df$collect[i] - df$cogs[i] - df$div[i]
      df$se[i] <- df$se[i-1] + df$ni[i] - df$ni[i]
    }

    df
}
```


<!-- To understand a function like this, it can be helpful to set values for the arguments (e.g., add_perc <- 0.03; n_years <- 20) and step through the lines of code one by one, intermittently inspecting the content of variables such as df as you do so.3 -->
このような関数を理解するためには、引数の値を設定することが役立つ（例：`add_perc <- 0.03; n_years <- 20`）し、コードの各行を一つずつ進めながら、その際に `df` などの変数の内容を確認することが役立つ。[^3]

<!-- Let’s generate 1000 years of data. -->
1000年分のデータを生成しよう。


```{r}
#| eval: false

set.seed(2021)
df_1000 <- get_data(n_years = 1000)
```

The first 20 years are shown in Figure 15.1.

```{r}
#| eval: false
df_1000 |>
  filter(year <= 20) |>
  ggplot(aes(x = year)) +
  geom_line(aes(y = sales), colour = "red") +
  geom_line(aes(y = mean(sales)), colour = "blue")
```

<!-- Now, let’s generate 5,000 random values for the add_perc parameter that we can use to generate simulated data. -->
次に、シミュレーションデータを生成するために使用できる `add_perc` パラメータの5,000個のランダム値を生成しよう。

```{r}
#| eval: false
add_percs <- runif(n = 5000, min = 0.01, max = 0.05)
```

<!-- We will generate simulated data for each value add_percs and store these data in a list called res_list. -->
`add_percs` の各値に対してシミュレーションデータを生成し、これらのデータを `res_list` というリストに格納する。

```{r}
#| eval: false
set.seed(2021)

res_list <-
  map(add_percs, get_data) |>
  system_time()
```

<!-- While production of res_list takes just a few seconds, because each iteration of get_data() is independent of the others, we could use the future package and plan(multisession) to do it even more quickly. -->
`res_list` の生成にはわずか数秒しかかからないが、`get_data()` の各反復が他の反復と独立しているため、さらに高速に行うために `future` パッケージと `plan(multisession)` を使用することができる。

```{r}
#| eval: false

plan(multisession)

res_list <-
  future_map(add_percs, get_data,
             .options = furrr_options(seed = 2021)) |>
  system_time()
```

<!-- We then make two data frames.  -->
次に、2つのデータフレームを作成する。
<!-- The first data frame (res_df) stores all the data in a single data frame using the field id to distinguish one simulation run from another.  -->
最初のデータフレーム（`res_df`）は、`id` フィールドを使用して、1つのシミュレーション実行を他の実行と区別するためにすべてのデータを1つのデータフレームに格納する。
<!-- These runs might be considered as “firms” with each run being independent of the other. -->
これらの実行は、それぞれが他の実行と独立していると考えられるかもしれない。

```{r}
#| eval: false

res_df <- list_rbind(res_list, names_to = "id")
```

<!-- To make it easier to compile results, we create get_coefs(), which calculates persistence as the coefficient in a regression of income on its lagged value - a specification similar to that in Sloan (1996) - and returns that value. -->
結果をまとめやすくするために、`get_coefs()` を作成し、収益をその遅延値に回帰した際の係数として持続性を計算し、その値を返す。
これはSloan (1996) と似た仕様の関数である。

```{r}
#| eval: false

get_coefs <- function(df) {
  fm <-
    df |>
    arrange(year) |>
    mutate(lag_ni = lag(ni)) |>
    lm(ni ~ lag_ni, data = _)

  tibble(add_perc = mean(df$add_perc),
         persistence = fm$coefficients[2])
}
```

<!-- We apply get_coefs() to res_list and store the results in the second data frame, results. -->
`get_coefs()` を `res_list` に適用し、その結果を2番目のデータフレーム `results` に格納する。

```{r}
#| eval: false

results <-
  res_list |>
  map(get_coefs) |>
  list_rbind(names_to = "id")
```

<!-- We plot the estimated persistence value against the assumed value for add_perc in Figure 15.2. -->
推定された*持続性*の値を `add_perc` の仮定値に対してプロットする。

```{r}
#| eval: false

results |>
  ggplot(aes(x = add_perc, y = persistence)) +
  geom_point()
```


<!-- ### Exercises -->
### 練習問題

<!-- When generating simulated financial statement data, it is generally important to ensure that the generated data meet some basic requirements.  -->
1. シミュレーションされた財務諸表データを生成する際には、一般的に生成されたデータがいくつかの基本的な要件を満たしていることを確認することが重要となる。
<!-- What is one fundamental relation that we expect to hold for these data? Does it hold for the data in df_1000? -->
これらのデータについて成立すると期待される基本的な関係は何か？`df_1000`のデータについては成立しているか？

<!-- Calculate values for cash flows from operating activities and cash flows from financing activities.  -->
2. 営業活動によるキャッシュフローと財務活動によるキャッシュフローの値を計算せよ。
<!-- (Treat payment of dividends as a financing activity. Hint: You may find it easier to use the direct method to calculate cash flows from operating activities.)
Does the cash flow statement articulate as it should? -->

<!-- 3. How evident are the details of the underlying process generating sales from Figure 15.1? Does looking at more data help? (Obviously, having a thousand years of data on a firm with a stationary process is not common.) -->
3. 図15.1から売上を生成する基礎プロセスの詳細はどの程度明確か？
   より多くのデータを見ることが有用か？
    (明らかに、定常プロセスを持つ企業の1000年分のデータを持つことは一般的ではない。)

<!-- 4. What is the “correct” value of `add_perc` that should be used? -->
4. 用いられるべき「正しい」`add_perc` の値は何か？
<!-- Using the plot from results above, what is the relation between values of departing from that value and persistence?  -->
上記の結果からのプロットを使用して、その値からの離れた値と持続性の関係は何か？
<!-- Does this agree with your intuition?  -->
これはあなたの直感と一致しているか？
<!-- What’s going on?  -->
何が起こっているのか？
<!-- What aspects of the `add_perc` - related accounting seem unrealistic?  -->
`add_perc` に関連する会計のどの側面が非現実的に見えるか？
<!-- (Hint: It may help to use variant of the following code set.seed(2021); get_data(0.03) for various values in place of 0.03 and to examine how the earnings process is affected.) -->
(ヒント: $0.03$ の代わりにさまざまな値を使用して `set.seed(2021); get_data(0.03)` の変形を使用し、利益プロセスがどのように影響を受けるかを調べると役立つかもしれない。)

<!-- 5. Does the simulation analysis speak to the underlying rationale for H1 of Sloan (1996)?  -->
5. シミュレーション分析は、Sloan (1996) のH1の根拠についての基本的な理由に言及していますか？
<!-- If so, why?  -->
もしそうならなぜか？
<!-- If not, what might be missing from the analysis?  -->
そうでない場合、分析に欠けている要素は何か？
<!-- How might we modify the simulation to incorporate the missing elements? -->
どのようにしてシミュレーションを変更して、欠落している要素を組み込むことができるか？

## Replicating Sloan (1996)

<!-- To better understand some elements of the empirical analysis of Sloan (1996), we conduct a replication analysis. -->
Sloan (1996) の実証分析のいくつかの要素をよりよく理解するために、分析の再現を行う。

<!-- We start by collating the data. -->
まず、データをまとめる。
<!-- We first connect to the tables we will use in our analysis. -->
最初に、分析で使用するテーブルに接続する。


::: {.panel-tabset}
## PostgreSQL

```{r}
#| eval: false
db <- dbConnect(RPostgres::Postgres(), bigint = "integer")

funda <- tbl(db, Id(schema = "comp", table = "funda"))
company <- tbl(db, Id(schema = "comp", table = "company"))
ccmxpf_lnkhist <- tbl(db, Id(schema = "crsp", table = "ccmxpf_lnkhist"))
msf <- tbl(db, Id(schema = "crsp", table = "msf"))
```

## parquet

``` {r}
#| eval: false
db <- dbConnect(duckdb::duckdb())

funda <- load_parquet(db, schema = "comp", table = "funda")
company <- load_parquet(db, schema = "comp", table = "company")
ccmxpf_lnkhist <- load_parquet(db, schema = "crsp", table = "ccmxpf_lnkhist")
msf <- load_parquet(db, schema = "crsp", table = "msf")
```

:::


<!-- We then make a subset of comp.funda, add SIC data from comp.company, and call the result funda_mod. -->
次に、`comp.funda`のサブセットを作成し、`comp.company`からSICデータを追加して、その結果を `funda_mod` と呼ぶ。

<!-- We construct SIC codes using sich, which provides the “historical” SIC code (sich), where available, but we use the “header” SIC code (sic) found on comp.company when sich is unavailable.4  -->
利用可能な場合は「歴史的」SICコード（sich）を提供する `sich` を使用して SICコードを構築しますが、`sich` が利用できない場合は `comp.company` で見つかる「ヘッダー」SICコード（`sic`）を使用します。[^4]
<!-- For some reason, sic is a character variable on comp.company and sich is an integer on comp.funda. -->
何らかの理由で、`comp.company` では `sic` が文字変数であり、`comp.funda` では `sich` が整数です。
<!-- So, we convert sic to an integer before merging the two tables here. -->
したがって、ここで2つのテーブルをマージする前に `sic` を整数に変換します。


```{r}
#| eval: false
sics <-
  company |>
  select(gvkey, sic) |>
  mutate(sic = as.integer(sic))

funda_mod <-
  funda |>
  filter(indfmt == "INDL", datafmt == "STD",
         consol == "C", popsrc == "D") |>
  left_join(sics, by = "gvkey") |>
  mutate(sic = coalesce(sich, sic))
```


<!-- We next apply the same sample selection criteria as Sloan (1996). -->
次に、Sloan (1996) と同じサンプル選択基準を適用する。
<!-- We focus on NYSE and AMEX firm-years (i.e., ones with exchg equal to 11 and 12, respectively) and years between 1962 and 1991. -->
NYSE と AMEX の企業年（つまり、それぞれ exchg が 11 と 12 に等しいもの）および 1962 年から 1991 年の間の年に焦点を当てる。

<!-- Sloan (1996, p.1) suggests that “the financial statement data required to compute operating accruals are not available … on Compustat for banks, life insurance or property and casualty companies.”  -->
Sloan (1996, p.1) は、「銀行、生命保険、損害保険会社については、運転アクルーアルを計算するために必要な財務諸表データがCompustatにはない」と述べている。
<!-- However, it is not clear if these firms are explicitly excluded (e.g., by filtering on SIC codes) or implicitly excluded by simply requiring that data for calculating accruals be available. -->
ただし、これらの企業が明示的に除外されているか（例：SICコードでフィルタリングされているか）、単にアクルーアルを計算するためのデータが利用可能であることを要求しているかどうかは明確ではない。
<!-- As such, we retain these firms and merely create a related indicator variable (finance). -->
そのため、これらの企業を保持し、関連する指標変数（`finance`）を作成する。

<!-- The next step is to create variables to reflect changes in key variables. -->
次のステップは、主要な変数の変化を反映する変数を作成することです。
<!-- We can use lag() to do this.5 -->
これには `lag()` を使用できます。[^5]

```{r}
#| eval: false
acc_data_raw <-
  funda_mod |>
  filter(!is.na(at),
         pddur == 12,
         exchg %in% c(11L, 12L)) |>
  mutate(finance = between(sic, 6000, 6999),
         across(c(che, dlc, txp), \(x) coalesce(x, 0))) |>
  group_by(gvkey) |>
  window_order(datadate) |>
  mutate(avg_at = (at + lag(at)) / 2,
         d_ca = act - lag(act),
         d_cash = che - lag(che),
         d_cl = lct - lag(lct),
         d_std = dlc - lag(dlc),
         d_tp = txp - lag(txp)) |>
  select(gvkey, datadate, fyear, avg_at, at, oiadp, dp, finance,
         starts_with("d_"), sic, pddur) |>
  mutate(acc_raw =  (d_ca - d_cash) - (d_cl - d_std - d_tp) - dp) |>
  ungroup() |>
  filter(between(fyear, 1962, 1991),
         avg_at > 0)
```

<!-- The final step in our data preparation calculates the core variables earn, acc, and cfo according to the definitions found in Sloan (1996), creates a variable to store the leading value of earn (using the lead() function, which is a window function that complements the lag() function we used above), creates deciles for acc, earn, cfo, and lead_earn, creates a two-digit SIC code, and finally filters out finance firms and observations without values for acc. -->
データの準備の最終ステップでは、Sloan (1996) で見つかる定義に従って、earn、acc、cfoのコア変数を計算し、earnの先行値を格納する変数を作成する（上記で使用した `lag()` 関数を補完するウィンドウ関数である `lead()` 関数を使用）、acc、earn、cfo、lead_earnのデシルを作成し、2桁のSICコードを作成し、最後に、金融企業とaccの値がない観測をフィルタリングする。

```{r}
#| eval: false
acc_data <-
  acc_data_raw |>
  mutate(earn = oiadp / avg_at,
         acc = acc_raw / avg_at,
         cfo = earn - acc) |>
  group_by(gvkey) |>
  window_order(datadate) |>
  mutate(lead_earn = lead(earn)) |>
  ungroup() |>
  collect() |>
  mutate(acc_decile = ntile(acc, 10),
         earn_decile = ntile(earn, 10),
         cfo_decile = ntile(cfo, 10),
         lead_earn_decile = ntile(lead_earn, 10),
         sic2 = str_sub(as.character(sic), 1, 2)) |>
  filter(!finance, !is.na(acc))
```

<!-- The next step is to collect data on stock returns for each firm-year. -->
次のステップは、各企業年の株価収益データを収集することである。
<!-- We use ccm_link, which we saw in Chapter 7, to link GVKEYs (Compustat) to PERMNOs (CRSP). -->
`GVKEY`（Compustat）を`PERMNO`（CRSP）にリンクするために、第7章で見た `ccm_link` を使用する。

```{r}
#| eval: false
ccm_link <-
  ccmxpf_lnkhist |>
    filter(linktype %in% c("LC", "LU", "LS"),
           linkprim %in% c("C", "P")) |>
    rename(permno = lpermno) |>
    mutate(linkenddt = coalesce(linkenddt, max(linkenddt, na.rm = TRUE))) |>
  select(gvkey, permno, linkdt, linkenddt)
```


<!-- Following Sloan (1996), we link (`gvkey`, `datadate`) combinations with permnos and a date range for the twelve-month period beginning four months after the end of the fiscal period. -->
Sloan (1996) に従い、（`gvkey`、`datadate`）の組み合わせを`permno`とリンクし、決算期間の終了後4か月から始まる12か月間の期間を指定する。

```{r}
#| eval: false
crsp_link <-
  acc_data_raw |>
  select(gvkey, datadate) |>
  inner_join(ccm_link,
             join_by(gvkey, between(datadate, linkdt, linkenddt))) |>
  select(gvkey, datadate, permno) |>
  mutate(start_month = as.Date(floor_date(datadate + months(4L), "month")),
         end_month = as.Date(floor_date(datadate + months(16L) - days(1L),
                                        "month")),
         month = floor_date(datadate, 'month'))
```

<!-- We then calculate compounded returns over this window for each (gvkey, datadate, permno) combination. -->
次に、このウィンドウに対して各（`gvkey`、`datadate`、`permno`）の組み合わせに対して複利収益を計算する。

```{r}
#| eval: false
crsp_data <-
  msf |>
  inner_join(crsp_link,
             by = join_by(permno, between(date, start_month, end_month))) |>
  group_by(gvkey, permno, datadate) |>
  summarize(ret = exp(sum(log(1 + ret), na.rm = TRUE)) - 1,
            n_months = n(),
            .groups = "drop") |>
  collect()
```

<!-- Table 4 of Sloan (1996, p. 304) uses abnormal returns, which are “computed by taking the raw buy-hold return … and subtracting the buy-hold return of a size-matched, value-weighted portfolio of firms. -->
Sloan (1996, p.304) の表4では、異常収益が使用されており、「生の買いホールドリターンを取り、市場価値加重ポートフォリオの買いホールドリターンを引くことによって計算される。
<!-- The size portfolios are based on market value of equity deciles of NYSE and AMEX firms.”  -->
サイズポートフォリオは、NYSEおよびAMEX企業の株式の時価総額デシルに基づいている。
<!-- We obtained the returns of individual firms above, but need to collect data on size portfolios, both the returns for each portfolio and the market capitalization cut-offs. -->
上記で個々の企業の収益を取得したが、サイズポートフォリオのデータ、各ポートフォリオの収益と時価総額のカットオフを収集する必要がある。

<!-- Data for size portfolios come from Ken French’s website, as we saw in Chapter 11. -->
サイズポートフォリオのデータは、第11章で見たように、Ken Frenchのウェブサイトから取得される。
<!-- Code like that used in Chapter 11 is included in the farr package in two functions: get_size_rets_monthly() and get_me_breakpoints(). -->
第11章で使用されたコードのようなコードは、`farr`パッケージに2つの関数 `get_size_rets_monthly()` と `get_me_breakpoints()` として含まれている。

```{r}
#| eval: false
size_rets <- get_size_rets_monthly()
size_rets
```


<!-- The table returned by get_size_rets_monthly() has four columns, including two measures of returns: one based on equal-weighted portfolios (ew_ret) and one based on value-weighted portfolios (vw_ret). -->
get_size_rets_monthly() によって返されるテーブルには、等加重ポートフォリオ（`ew_ret`）と価値加重ポートフォリオ（`vw_ret`）に基づく2つの収益の測定値を含む4つの列がある。
<!-- Like Sloan (1996), we use vw_ret. -->
Sloan (1996) と同様に、`vw_ret` を使用する。


```{r}
#| eval: false
me_breakpoints <- get_me_breakpoints()
me_breakpoints
```


<!-- The table returned by get_me_breakpoints() identifies the size decile (decile) to which firms with market capitalization between me_min and me_max in a given month should be assigned. -->
get_me_breakpoints() によって返されるテーブルは、特定の月において me_min と me_max の間の時価総額を持つ企業が割り当てられるサイズデシル（`decile`）を識別する。

<!-- To join CRSP with crsp_link, we construct the variable month for each value of date on crsp.msf. -->
CRSPをcrsp_linkと結合するために、crsp.msfのdateの各値に対して変数monthを構築する。
<!-- This prevents non-matches due to non-alignment of datadate values with date values on crsp.msf. -->
これにより、`datadate`の値が`crsp.msf`の`date`の値と整合しないことによる非一致を防ぐ。

```{r}
#| eval: false
crsp_dates <-
  msf |>
  distinct(date) |>
  mutate(month = floor_date(date, 'month'))
```

<!-- The following code assigns firm-years (i.e., (permno, datadate) combinations) to size deciles according to market capitalization and size cut-offs applicable during the month of datadate. -->
次のコードは、datadateの月に適用される時価総額とサイズのカットオフに従って、企業年（つまり、（`permno`、`datadate`）の組み合わせ）をサイズデシルに割り当てる。

```{r}
#| eval: false
me_values <-
  crsp_link |>
  inner_join(crsp_dates, by = "month") |>
  inner_join(msf, by = c("permno", "date")) |>
  mutate(mktcap = abs(prc) * shrout / 1000) |>
  select(permno, datadate, month, mktcap) |>
  collect()

me_decile_assignments <-
  me_breakpoints |>
  inner_join(me_values,
             join_by(month, me_min <= mktcap, me_max > mktcap)) |>
  select(permno, datadate, decile)
```


<!-- For each datadate and size decile, the following code calculates the cumulative returns over the twelve-month period beginning four months after datadate. -->
各`datadate`とサイズデシルについて、以下のコードは、`datadate`の4か月後から始まる12か月間の累積収益を計算する。

```{r}
#| eval: false
cum_size_rets <-
  me_decile_assignments |>
  select(datadate, decile) |>
  distinct() |>
  mutate(start_month = datadate + months(4),
         end_month =  datadate + months(16)) |>
  inner_join(size_rets,
             join_by(decile, start_month <= month, end_month >= month)) |>
  group_by(datadate, decile) |>
  summarize(ew_ret = exp(sum(log(1 + ew_ret), na.rm = TRUE)) - 1,
            vw_ret = exp(sum(log(1 + vw_ret), na.rm = TRUE)) - 1,
            n_size_months = n(),
            .groups = "drop")
```

<!-- Now we have the data we need to calculate size-adjusted returns. -->
これで、サイズ調整収益を計算するために必要なデータが揃った。
<!-- We simply combine crsp_data with me_decile_assignments and then with cum_size_rets and calculate size_adj_ret as a simple difference. -->
単純な差として `size_adj_ret` を計算するために、`crsp_data` と `me_decile_assignments`、そして `cum_size_rets` を組み合わせるだけである。


```{r}
#| eval: false
size_adj_rets <-
  crsp_data |>
  inner_join(me_decile_assignments, by = c("permno", "datadate")) |>
  inner_join(cum_size_rets, by = c("datadate", "decile")) |>
  mutate(size_adj_ret = ret - vw_ret) |>
  select(gvkey, datadate, size_adj_ret, n_months, n_size_months)
```


<!-- For our regression analysis, we simply join our processed data from Compustat (acc_data) with our new data on size-adjusted returns (size_adj_rets). -->
本稿の回帰分析では、Compustatの処理されたデータ（`acc_data`）をサイズ調整収益（`size_adj_rets`）の新しいデータと結合するだけである。


```{r}
#| eval: false
reg_data <-
  acc_data |>
  inner_join(size_adj_rets, by = c("gvkey", "datadate"))
```


<!-- Before running regression analyses, it is important to examine our data. -->
回帰分析を実行する前に、データを調査することが重要である。
<!-- One useful benchmark is the set of descriptive statistics reported in Table 1 of Sloan (1996). -->
有用な基準の1つは、Sloan (1996) の Table 1 に報告されている記述統計である。
<!-- Some degree of assurance is provided by the similar of the values seen in Table 15.2 with those reported in Sloan (1996). -->
Table 15.2 で見られる値が Sloan (1996) で報告されている値と類似していることにより、ある程度の保証が提供される。

```{r}
#| eval: false
reg_data |>
  group_by(acc_decile) |>
  summarize(across(c(acc, earn, cfo), \(x) mean(x, na.rm = TRUE)))
```



<!--------- ここまで15.3 --------->




### Sloan (1996)の表2

<!-- Having done a very basic check of our data, we can create analogues of some of the regression analyses found in Sloan (1996). -->
データの非常に基本的なチェックを行った後、Sloan (1996) に見られるいくつかの回帰分析の類似物を作成することができる。

<!-- The output shown in Table 15.3 parallels the “pooled” results in Table 2 of Sloan (1996). -->
Table 15.3 に示されている出力は、Sloan (1996) の Table 2 にある「プールされた」結果に対応している。

```{r}
#| eval: false
fms <- list(lm(lead_earn ~ earn, data = reg_data),
            lm(lead_earn_decile ~ earn_decile, data = reg_data))

modelsummary(fms,
             estimate = "{estimate}{stars}",
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .1, '**' = 0.05, '***' = .01))
```


<!-- To produce “industry level” analysis like that in Table 2 of Sloan (1996), we create run_table_ind(), a small function to produce regression coefficients by industry. -->
Sloan (1996) の Table 2 のような「業界レベル」の分析を生成するために、業界ごとに回帰係数を生成する小さな関数 `run_table_ind()` を作成する。
<!-- As seen in Chapter 14, we use !! to distinguish the value sic2 supplied to the function from the variable sic2 found in reg_data.6 -->
第14章で見たように、関数に供給される `sic2` の値を `reg_data` で見つかる変数 `sic2` と区別するために `!!` を使用する。[^6]

```{r}
#| eval: false

run_table_ind <- function(sic2, lhs = "lead_earn", rhs = "earn") {
  df <-
    reg_data |>
    filter(sic2 == !!sic2)

  fm <- lm(as.formula(str_c(lhs, " ~ ", rhs)), data = df)

  coefs <- as_tibble(t(fm$coefficients))
  names(coefs) <- colnames(t(fm$coefficients))
  bind_cols(sic2 = sic2, coefs)
}
```

<!-- The function stats_for_table() compiles descriptive statistics. -->
関数 `stats_for_table()` は記述統計をまとめる。

```{r}
#| eval: false
stats_for_table <- function(x) {
  qs <- quantile(x, probs = c(0.25, 0.50, 0.75), na.rm = TRUE)

  tibble(mean = mean(x, na.rm = TRUE),
         q1 = qs[1], median = qs[2], q3 = qs[3])
}
```

<!-- Finally, summ_for_table() calls run_table_ind() and stats_for_table() and produces a summary table. -->
最後に、`summ_for_table()` は `run_table_ind()` と `stats_for_table()` を呼び出し、サマリーテーブルを生成する。

```{r}
#| eval: false
summ_for_table <- function(lhs = "lead_earn", rhs = "earn") {
  reg_data |>
    distinct(sic2) |>
    pull() |>
    map(run_table_ind, lhs = lhs, rhs = rhs) |>
    list_rbind() |>
    select(-sic2) |>
    map(stats_for_table) |>
    list_rbind(names_to = "term")
}
```

<!-- Tables 15.4 and 15.5 parallel the “industry level” results reported in Table 2 of Sloan (1996). -->
Table 15.4 と Table 15.5 は、Sloan (1996) の Table 2 に報告されている「業界レベル」の結果に対応している。

```{r}
#| eval: false
summ_for_table(lhs = "lead_earn", rhs = "earn")

summ_for_table(lhs = "lead_earn_decile", rhs = "earn_decile")
```


<!-- Our results thus far might be described as “qualitatively similar” to those in Table 2 of Sloan (1996). -->
これまでの結果は、Sloan (1996) の Table 2 と「質的に類似している」と言えるかもしれない。
<!-- The main difference may be in the magnitude of the pooled coefficient on earn in the regression with lead_earn as the dependent variable. -->
従属変数が `lead_earn` である回帰分析における `earn` のプールされた係数の大きさが異なる点が主な違いであるかもしれない。
<!-- Table 2 of Sloan (1996) reports a coefficient of 0.841, notably higher than the mean coefficient from the industry-level regressions (0.773). -->
Sloan (1996) のTable 2では、 $0.841$ という係数が報告されており、これは業界レベルの回帰分析の平均係数（ $0.773$ ）よりも著しく高い。
<!-- In contrast, the mean coefficients in our pooled and industry-level analyses are much closer to each other. -->
一方、本稿のプールされた分析と業界レベルの分析の平均係数はお互いに非常に近い。



<!-- 15.4.2 Table of Sloan (1996) -->
### Sloan (1996)の表3

<!-- In Table 3, Sloan (1996) decomposes the right-hand side variables from Table 15.3 into accrual and cash-flow components. -->
Sloan (1996) では、Table 15.3 の右辺の変数を貸借対照表とキャッシュフローのコンポーネントに分解している。
<!-- We replicate these analyses in Table 15.6. -->
これらの分析を Table 15.6 で再現する。

```{r}
#| eval: false
fms <- list(lm(lead_earn ~ acc + cfo, data = reg_data),
            lm(lead_earn_decile ~ acc_decile + cfo_decile, data = reg_data))

modelsummary(fms,
             estimate = "{estimate}{stars}",
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .1, '**' = 0.05, '***' = .01))
```


<!-- Tables 15.7 and 15.8 parallel the “industry level” results reported in Table 3 of Sloan (1996). -->
Table 15.7 と Table 15.8 は、Sloan (1996) の Table 3 に報告されている「業界レベル」の結果に対応している。
<!-- Again we have “qualitatively similar” results to those found in Sloan (1996). -->
再び、Sloan (1996) で見られる結果と「質的に類似した」結果が得られている。

```{r}
#| eval: false
summ_for_table(lhs = "lead_earn", rhs = "acc + cfo")

summ_for_table(lhs = "lead_earn_decile", rhs = "acc_decile + cfo_decile")
```

<!-- 15.4.3 Pricing of Earnings components -->
### Pricing of earnings components

<!-- An element of the analysis reported in Table 5 of Sloan (1996) regresses abnormal returns on contemporaneous earnings and components of lagged earnings and we do likewise. -->
Sloan (1996) の Table 5 に報告されている分析の要素は、異常リターンを同時期の収益および遅れた収益のコンポーネントに回帰させることであり、私たちも同様に行う。

```{r}
#| eval: false
mms <- list(lm(size_adj_ret ~ lead_earn + acc + cfo,
               data = reg_data),
            lm(size_adj_ret ~ lead_earn_decile + acc_decile + cfo_decile,
               data = reg_data))
```

<!-- Table 15.9 provides results from this regression. -->
Table 15.9 は、この回帰分析の結果を示している。

```{r}
#| eval: false
modelsummary(mms,
             estimate = "{estimate}{stars}",
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .1, '**' = 0.05, '***' = .01))
```


<!-- In the notation of Sloan (1996), the coefficient on acc can be expressed as $-\beta \gamma ^*_1$ , which is minus one times the product of $\beta$ , the coefficient on `lead_earn` (i.e., earnings roughly contemporaneous with `size_adj_ret`), and $\gamma ^*_1$ , the implied market coefficient on accruals. -->
Sloan (1996) の表記法では、`acc` の係数は $- \beta \gamma_1^*$ と表すことができ、これは $\beta$ （`lead_earn` に対する係数、つまり `size_adj_ret` とほぼ同時期の収益）と $\gamma ^*_1$ （帳簿価額の暗黙の市場係数）の積をマイナスにしたものである。

<!-- With estimates of $\hat{\beta} = 2.538$ and $\hat{\beta \gamma _1^*} = 1.961$ , we have an implied estimate of $\hat{\gamma}_1^* = 0.7726$ . -->
$\hat{\beta} = 2.538$ および $\hat{\beta \gamma _1^*} = 1.961$ の推定値から、暗黙の推定値 $\hat{\gamma}_1^* = 0.7726$ を得る。
<!-- This estimate $\hat{\gamma }_1^* = 0.7726$ is higher than the estimate of $\hat{\gamma }_1 = 0.6399$ . -->
この推定値 $\hat{\gamma }_1^* = 0.7726$ は、$\hat{\gamma }_1 = 0.6399$ の推定値よりも高い。
<!-- But can we conclude that the difference between these two coefficients is statistically significant? -->
しかし、これら2つの係数の差が統計的に有意であると結論付けることができるだろうか？

<!-- One approach to this question would be to evaluate whether $\hat{\beta \gamma _1^*} = 1.961$ as estimated from the market regression is statistically different from the value implied by $\hat{\beta} \times \hat{\gamma}_1 = 2.538 \times 0.6399 = 1.624$ . -->
この問いに対する1つのアプローチは、市場回帰から推定された $\hat{\beta \gamma _1^*} = 1.961$ が、$\hat{\beta} \times \hat{\gamma}_1 = 2.538 \times 0.6399 = 1.624$ によって暗示される値と統計的に異なるかどうかを評価することである。


<!-- However, as pointed out by Mishkin (1983), this procedure “implicitly assumes that there is no uncertainty in the estimate of $\hat{\gamma}_1$ . -->
しかし、Mishkin (1983) が指摘しているように、この手順は「 $\hat{\gamma}_1$ の推定値に不確実性がないという暗黙の前提を含んでいる。
<!-- This results in inconsistent estimates of the standard errors of the parameters and hence test statistics that do not have the assumed F distribution. -->
これにより、パラメータの標準誤差の一貫性のない推定値が生じ、したがって、仮定されたF分布を持たない検定統計量が得られる。
<!-- This can lead to inappropriate inference ….”7 -->
これは「不適切な推論につながる可能性がある...」 [^7] 。

<!-- Given the issue of “inappropriate inference” described above, Mishkin (1983) uses “iterative weighted non-linear least squares” (Sloan, 1996, p.1) to estimate a system of equations and then calculates an F-statistic based on comparison of goodness-of-fit of an unconstrained system of equations with that of a constrained system of equations (i.e., one in which $\gamma$ is constrained equal in both equations). -->
上記の「不適切な推論」の問題を考慮すると、Mishkin (1983) は「反復重み付き非線形最小二乗法」（Sloan, 1996, p.1）を使用して方程式系を推定し、制約のない方程式系と制約のある方程式系（つまり、両方の方程式で $\gamma$ が等しいと制約された方程式系）の適合度を比較してF統計量を計算する。
<!-- While Sloan (1996) uses this “Mishkin (1983)” test in his analysis reported in Tables 4 and 5, this approach involves significant complexity.8 -->
Sloan (1996) は、表4および表5に報告されている分析でこの「Mishkin (1983)」テストを使用しているが、このアプローチは複雑さが大きい。[^8]

<!-- Fortunately, Abel and Mishkin (1983) suggest a simpler approach that they show is asymptotically equivalent to the Mishkin test. -->
幸いなことに、Abel and Mishkin (1983) は、Mishkinテストと漸近的に等価であることを示すより簡単なアプローチを提案している。
<!-- The intuition for this approach is that if components of lagged earnings (accruals and cash flows) are mispriced in a way that predicts stock returns, then this should be apparent from a regression of stock returns on those lagged earnings components. -->
このアプローチの直感は、遅れた収益のコンポーネント（負債計上額とキャッシュフロー）が株価リターンを予測する方法で過大評価されている場合、株価リターンを遅れた収益のコンポーネントに回帰させることでこれが明らかになるはずであるということである。
<!-- Kraft et al. (2007) provide additional discussion of the Mishkin test and the approach used by Abel and Mishkin (1983).9  -->
Kraft et al. (2007) は、MishkinテストとAbel and Mishkin (1983)が使用するアプローチについての追加の議論を提供している。[^9]
<!-- In effect, this allows us to skip the “middleman” of contemporaneous earnings in the regression analysis. -->
実際には、このアプローチにより、回帰分析において同時期の収益を経由することなく、遅れた収益のコンポーネントを直接使用することができる。

<!-- The regression results in Table 15.10 come from applying the approach suggested by Abel and Mishkin (1983). -->
Table 15.10 の回帰結果は、Abel and Mishkin (1983) が提案したアプローチを適用して得られたものである。

```{r}
#| eval: false
eff <- list(lm(size_adj_ret ~ acc + cfo, data = reg_data),
            lm(size_adj_ret ~ acc_decile + cfo_decile, data = reg_data))

modelsummary(eff,
             estimate = "{estimate}{stars}",
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .1, '**' = 0.05, '***' = .01))
```


<!-- From Table 15.10, we see that lagged accruals are negatively associated with abnormal returns. -->
Table 15.10 から、前期のアクルーアルは異常リターンと負の関連があることがわかる。
<!-- This result is consistent with the market overpricing accruals because it assumes a level of persistence that is too high. -->
この結果は、市場が負債計上額を過大評価しているということを示しており、これはあまりに高い持続性を前提としている。

### Exercises

1. In creating acc_data_raw, we used coalesce() to set the value of certain variables to zero when missing on Compustat.
Does this seem appropriate here? Are the issues similar to those observed with regard to R&D in Chapter 8? It may be helpful to find some observations from recent years where this use of the coalesce() function has an effect and think about the issues in context of financial statements for those firm-years.

2. Can you reconcile the results from the Abel and Mishkin (1983) test with those from the previous regressions? (Hint: Pay attention to sample composition; you may need to tweak these regressions.)

3. The equations estimated in Table 5 of Sloan (1996) could be viewed as a structural (causal) model.
Can you represent this model using a causal diagram? In light of the apparent econometric equivalence between that structural model and the estimation approach used in Abel and Mishkin (1983), how might the coefficients from the structural model be recovered from the latter approach?


<!-- 4. This critique implies a causal interpretation of the coefficients in Sloan (1996).  -->
4. この批判は、Sloan (1996) の係数に因果関係の解釈が含まれていることを示唆している。
<!-- How might the critique of Kraft et al. (2007) be represented on the causal diagrams above?  -->
Kraft et al. (2007) の批判は、上記の因果関係図にどのように表現されるだろうか？
<!-- How persuasive do you find the critique of Kraft et al. (2007) to be? -->
Kraft et al. (2007) の批判はどの程度説得力があると感じるだろうか？

5. Apart from the different data sources used, another difference between the simulation analysis earlier in this chapter and the regression analysis in Table 3 of Sloan (1996) is the regression model used.
Modify the code below to incorporate the appropriate formulas for cash flow from operating activities (cfo) and accruals (acc).
Then replicate the pooled analysis of Panel A of Table 3 of Sloan (1996) using the resulting sim_reg_data data frame.
What do you observe?

```{r}
#| eval: false
sim_reg_data <-
  res_df |>
  mutate(cfo = [PUT CALC HERE], acc = [PUT CALC HERE]) |>
  group_by(id) |>
  arrange(id, year) |>
  mutate(lag_cfo = lag(cfo),
         lag_acc = lag(acc)) |>
  ungroup()
```

6. Which hypothesis does Figure 1 of Sloan (1996) relate to?
What aspects of the plot make it easier or more difficult to interpret the results?
The following code replicates a version of Figure 1 from Sloan (1996) using our simulated data.
On the basis of Figures 15.3–15.5 and the arguments given in Sloan (1996), is H1 true in our simulated data? Given the other analysis above, is H1 true in our simulated data?

```{r}
#| eval: false
year_of_event <- 10

decile_data <-
  sim_reg_data |>
  filter(year == year_of_event) |>
  mutate(cfo_decile = ntile(cfo, 10),
         ni_decile = ntile(ni, 10),
         acc_decile = ntile(acc, 10)) |>
  select(id, ends_with("decile"))

reg_data_deciles <-
  sim_reg_data |>
  inner_join(decile_data, by = "id")
```

<!-- The following code produces Figure 15.3: -->
次のコードは、図15.3を生成する。

```{r}
#| eval: false

reg_data_deciles |>
  filter(ni_decile %in% c(1, 10)) |>
  mutate(ni_decile = as.factor(ni_decile),
         event_year = year - year_of_event) |>
  group_by(ni_decile, year) |>
  summarize(ni = mean(ni, na.rm = TRUE), .groups = "drop") |>
  ggplot(aes(x = year, y = ni, group= ni_decile, color = ni_decile)) +
  geom_line()
```

<!-- The following code produces Figure 15.4: -->
次のコードは、図15.4を生成する。

```{r}
#| eval: false

reg_data_deciles |>
  filter(cfo_decile %in% c(1, 10)) |>
  mutate(cfo_decile = as.factor(cfo_decile),
         event_year = year - year_of_event) |>
  group_by(cfo_decile, year) |>
  summarize(ni = mean(ni, na.rm = TRUE), .groups = "drop") |>
  ggplot(aes(x = year, y = ni, group = cfo_decile, color = cfo_decile)) +
  geom_line()
```

<!-- The following code produces Figure 15.5: -->
次のコードは、図15.5を生成する。

```{r}
#| eval: false

reg_data_deciles |>
  filter(acc_decile %in% c(1, 10)) |>
  mutate(acc_decile = as.factor(acc_decile),
         event_year = year - year_of_event) |>
  group_by(acc_decile, year) |>
  summarize(ni = mean(ni, na.rm = TRUE), .groups = "drop") |>
  ggplot(aes(x = year, y = ni, group = acc_decile, color = acc_decile)) +
  geom_line()
```

<!-- ##  Accrual anomaly -->
## アクルーアル・アノマリー

<!-- Table 6 of Sloan (1996) provides evidence that the market’s apparent mispricing of accruals implies trading strategies that give rise to abnormal returns. -->
Sloan (1996) の Table 6 は、市場がアクルーアルを誤って価格設定していることから、異常リターンを生む取引戦略を示している。
<!-- Such strategies are generally termed anomalies because they seem inconsistent with the efficient markets hypothesis (see Section 10.1). -->
このような戦略は一般的にアノマリーと呼ばれ、効率的市場仮説と矛盾しているように見える（10.1節を参照）。
<!-- Fama and French (2008, p. 1653) define anomalies as "patterns in average stock returns that … are not explained by the Capital Asset Pricing Model (CAPM)." -->
Fama and French (2008, p. 1653) は、アノマリーを「平均株価リターンのパターンであり、…資本資産価格モデル(CAPM)で説明されない」と定義している。
<!-- Implicit in the Fama and French (2008) seems to be the notion that the CAPM is the true model of market risk and a general version of the definition of Fama and French (2008) would replace the CAPM with the posited true model of market risk. -->
Fama and French (2008) には、CAPMが市場リスクの真のモデルであるという概念が含まれているように思われ、Fama and French (2008) の定義の一般的なバージョンでは、CAPMを仮定された市場リスクの真のモデルに置き換えることになる。
<!-- Dechow et al. (2011, p. 23) argue that “the accrual anomaly is not really an anomaly at all. -->
Dechow et al. (2011, p. 23) は、「アクルーアル異常は実際には全く異常ではない」と主張している。
<!-- In fact, the original research documenting the accrual anomaly predicted that it would be there. -->
実際、アクルーアル異常を文書化した最初の研究は、それが存在すると予測していた。
<!-- The term anomaly is usually reserved for behavior that deviates from existing theories, but when Sloan (1996) first documented the accrual anomaly, he was testing a well-known theory and found that it was supported.” -->
アノマリーという用語は、既存の理論から逸脱する行動に通常予約されているが、Sloan (1996) が最初にアクルーアル異常を文書化したとき、彼はよく知られた理論を検証して、それが支持されたことを発見した。

<!-- While Table 6 provides portfolio returns for years $t+1$, $t+2$, and $t+3$, we only collected returns for year $t+1$ in the steps above. -->
Table 6 は、年 $t+1$ ， $t+2$ ，および $t+3$ のポートフォリオリターンを提供しているが、上記の手順では年 $t+1$ のリターンのみを収集している。
<!-- Therefore, Table 15.11 only replicates the first column of Table 6 of Sloan (1996). -->
したがって、Table 15.11 は Sloan (1996) の Table 6 の最初の列のみを再現している。

```{r}
#| eval: false
fm <-
  reg_data |>
  group_by(fyear, acc_decile) |>
  summarize(size_adj_ret = mean(size_adj_ret, na.rm = TRUE),
            .groups = "drop") |>
  mutate(acc_decile = as.factor(acc_decile)) |>
  lm(size_adj_ret ~ acc_decile - 1, data = _)

modelsummary(fm,
             estimate = "{estimate}{stars}",
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = .1, '**' = 0.05, '***' = .01))
```


```{r}
#| eval: false
hedge_ret <- fm$coefficients["acc_decile1"] - fm$coefficients["acc_decile10"]
p_val <- linearHypothesis(fm, "acc_decile1 = acc_decile10")$`Pr(>F)`[2]
```

<!-- The hedge portfolio return (hedge_ret) is 0.1322 with a p-value of 4.4e-06 (p_val). -->
ヘッジポートフォリオリターン (hedge_ret) は 0.1322 で、p値は 4.4e-06 (p_val) です。

<!-- ### Discussion questions -->
### ディスカッション問題2

<!-- 1. In estimating the hedge portfolio regression, we included a line summarize(size_adj_ret = mean(size_adj_ret)). -->
1. ヘッジポートフォリオ回帰を推定する際、size_adj_ret = mean(size_adj_ret) を含めた理由は何ですか？
<!-- Why is this step important? -->
なぜこのステップが重要なのですか？

2. Green et al. (2011) say “the simplicity of the accruals strategy and the size of the returns it generates have led some scholars to conclude that the anomaly is illusory.
Green et al. (2011)は、「アクルーアル戦略の単純さとそれが生み出すリターンの大きさが、一部の学者たちに、この異常は幻想であると結論付けさせた」と述べている。
<!-- For example, Khan (2008) and Wu et al. (2010) argue that the anomaly can be explained by a mis-specified risk model and the q-theory of time-varying discount rates, respectively; Desai et al. (2004) conclude that the anomaly is deceptive because it is subsumed by a different strategy; Kraft et al. (2006) attribute it to outliers and look-ahead biases; Ng (2005) proposes that the anomaly’s abnormal returns are compensation for high exposure to bankruptcy risk; and Zach (2006) argues that there are firm characteristics correlated with accruals that cause the return pattern.” -->
たとえば，Khan (2008) や Wu et al. (2010) は，異常は誤ったリスクモデルや時間変動する割引率のq理論によって説明できると主張している。Desai et al. (2004) は，異常は異なる戦略に包含されているため欺瞞的であると結論付けている。Kraft et al. (2006) は，外れ値や先読みバイアスに帰する。Ng (2005) は，異常リターンは破産リスクへの高い露出の補償であると提案している。Zach (2006) は，アクルーアルと相関する企業特性がリターンパターンを引き起こすと主張している。
<!-- Looking at Sloan (1996), but without necessarily looking at each of the papers above, what evidence in Sloan (1996) seems inconsistent with the claims made by each paper above?  -->
Sloan (1996) を見ると、上記の各論文で述べられている主張と矛盾すると思われる Sloan (1996) の証拠は何か？
<!-- Which do you think you would need to look more closely at the paper to understand?  -->
上記の各論文で述べられている主張と矛盾すると思われる Sloan (1996) の証拠は何か？
<!-- What evidence do you think Zach (2006) would need to provide to support the claim of an alternative “cause”? -->
Zach (2006) が代替の「原因」を主張するために提供する必要がある証拠は何か？

<!-- 3. Do Green et al. (2011) address the alternative explanations advanced in the quote in Q1 above?  -->
Green et al. (2011) は、Q1で引用された代替説明に対処しているか？
<!-- Do you think that they need to do so? -->
彼らはそうする必要があると思うか？

<!-- 4. How persuasive do you find the evidence regarding the role of hedge funds provided by Green et al. (2011)? -->
Green et al. (2011) が提供するヘッジファンドの役割に関する証拠はどの程度説得力があると思いますか？

<!-- 5. Xie (2001) (p. 360) says that "for firm-years prior to 1988 when Compustat item #308 is unavailable, I estimate  as follows …". -->
5. Xie (2001, p.360)は、「Compustatのアイテム#308が利用できない1988年以前の企業年度については、次のように推定する...」と述べている。
<!-- Why would item #308 be unavailable prior to 1988? What is the equivalent to #308 in Compustat today? -->
なぜアイテム#308が1988年以前に利用できないのか？
Compustatの現在の#308に相当するものは何か？

<!-- 6. Study the empirical model on p.361 of Xie (2001), which is labelled equation (1). -->
Xie (2001) の p.361 にラベル付けされた実証モデル(式1)を調べなさい。
<!-- (This is the much-used “Jones model” from Jones (1991), which we examine in Chapter 16.) -->
(これは、Jones (1991) の「ジョーンズモデル」であり、第16章で説明する。)
<!-- What are the assumptions implicit in this model and the labelling of the residual as “abnormal accruals”? -->
このモデルと残差を「異常アクルーアル」とラベル付けする際の暗黙の仮定は何か？
<!-- (Hint: Take each component of the model and identify circumstances where it would be a reasonable model of “normal” accruals.) -->
(ヒント: モデルの各構成要素を取り上げ、「通常の」アクルーアルの合理的なモデルとなる状況を特定しなさい。)

<!-- 7. What is “channel stuffing”? -->
「チャネルスタッフィング」とは何か？
<!-- (Hint: Wikipedia has a decent entry on this.) -->
(ヒント: Wikipediaにはこれについてのまともなエントリがある。)
<!-- What effect would channel stuffing have on abnormal accruals? -->
チャネルスタッフィングが異常アクルーアルに与える影響は何か？
<!-- (Hint: Think about this conceptually and with regard to equation (1). -->
(ヒント: これを概念的に、および式(1)に関して考えてみなさい。)
<!-- Do you need more information than is provided in Xie (2001) to answer this?) -->
これに答えるために、Xie (2001) で提供されている情報以上の情報が必要か？

[^1]: This is one definition that can be tightened and vary by context.

[^2]: Wickham et al. (2023, p.441) suggest that “a good rule of thumb is to consider writing a function whenever you’ve copied and pasted a block of code more than twice.”

[^3]: Indeed, this is the process often used to create a function like this in the first place.

[^4]: Recall from Chapter 12 that a header variable is one where only the most recent value is retained in the database.

[^5]: We introduced the lag() function in Chapters 7 and 8.

[^6]: Chapter 19 of Wickham (2019) has more details on this “unquote” operator !!.

[^7]: Note that Mishkin (1983) is actually critiquing a different econometric procedure whereby residuals from the first regression are included in a version of the second, but the quoted criticism is equally applicable to the procedure we describe here.

[^8]: This is apparent from inspection of the Stata .ado file provided by Judson Caskey to implement the Mishkin (1983) approach at https://sites.google.com/site/judsoncaskey/data.

[^9]: Note that Kraft et al.(2007) appear to assume that the OLS test used by Mishkin (1983) is the same as the test proposed in Abel and Mishkin (1983), but differences in these tests do not affect the substance of the discussion of Abel and Mishkin (1983).

