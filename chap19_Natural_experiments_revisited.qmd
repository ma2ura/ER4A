<!-- # Natural experiments revisited -->
# 自然実験:再訪


<!-- In this chapter, we return to the topic of natural experiments.  -->
この章では、自然実験のトピックを再訪する。
<!-- We first discuss the notion of registered reports, their purpose, and their limitations.  -->
まず、登録報告（Registered Reports）の概念、その目的、およびその限界について論じる。
<!-- We then focus on an experiment (“Reg SHO”) run by the US Securities and Exchange Commission (SEC) and studies that examined the effects of Reg SHO, with a particular focus on one study (Fang et al., 2016) that exploited this regulation to study effects on earnings management. -->
次に、米国証券取引委員会(SEC)によって実施された実験「Reg SHO」と、それに関する研究を検討する。特に、Fang et al.（2016）の研究に焦点を当て、この規制を利用して利益マネジメント(Earnings Management)への影響を分析した点を詳しく論じる。

<!-- This chapter provides opportunities to sharpen our skills and knowledge in a number of areas.  -->
この章では、以下のような領域でスキルと知識を磨く機会を提供する。
<!-- First, we will revisit the topic of earnings management and learn about some developments in its measurement since Dechow et al. (1995), which we covered in Chapter 16.  -->
第1に、利益マネジメントのテーマを再訪し、Dechow et al. (1995)以来の測定手法の発展について学ぶ。このテーマは第16章ですでに扱っている。
<!-- Second, we further develop our skills in evaluating claimed natural experiments, using Reg SHO and the much-studied setting of broker-closure shocks.  -->
第2に、Reg SHOおよび頻繁に研究されている証券会社の閉鎖(broker-closure shocks)の事例を用いて、自然実験とされる評価スキルをさらに発展させる。
<!-- Third, we explore the popular difference-in-differences approach, both when predicated on random assignment and when based on the so-called parallel trends assumption in the absence of random assignment.  -->
第3に、差分の差(diffrence-in-differences)手法を探求し、ランダムな割当がある場合と、ランダムな割当がない場合の「平行トレンド仮定」(parallel trends assumption)に基づく場合の両方について考察する。
<!-- Fourth, we will have an additional opportunity to apply ideas related to causal diagrams and causal mechanisms (covered in Chapters 4 and 18, respectively).  -->
第4に、第4章および第18章で扱った因果ダイアグラムと因果メカニズムに関連する概念を応用する機会をもつ。
<!-- Fifth, we will revisit the topic of statistical inference, using this chapter as an opportunity to consider randomization inference.  -->
第5に、統計的推論のテーマを再訪し、この章をランダム化推論(randomization inference)を考慮する機会として利用する。
<!-- Sixth, we build on the Frisch-Waugh-Lovell theorem to consider issues associated with the use of two-step regressions, which are common in many areas of accounting research. -->
第6に、Frisch-Waugh-Lovellの定理を基に、会計研究の多くの分野で一般的な二段階回帰の使用に関連する問題を考察する。

:::{.callout-tip}
The code in this chapter uses the packages listed below. We load tidyverse because we use several packages from the Tidyverse. For instructions on how to set up your computer to use the code found in this book, see Section 1.2. Quarto templates for the exercises below are available on GitHub.
:::

```{r}
pacman::p_load(tidyverse, DBI, farr, fixest, modelsummary, furrr, broom)
```

:::{.callout-important}
<!-- This chapter is longer than others in the book, so we have made it easier to run code from one section without having to run all the code preceding it.  -->
この章は、本書の他の章よりも長いため、それ以前のすべてのコードを実行する必要なしに、特定のセクションからコードを実行することができるようになっている。
<!-- Beyond that, the code in each of Sections 19.1–19.3 and 19.5 is independent of code in other parts of this chapter and can be run independently of those other sections.1  -->
さらに、セクション19.1〜19.3および19.5の各セクションのコードは、この章の他の部分のコードとは独立しており、それらの他のセクションとは独立して実行できます。
<!-- Code and exercises in Sections 19.7 and 19.8 depend on code in Section 19.6, so you will need to run the code in Section 19.6 before running the code in those sections. -->
セクション19.7および19.8のコードと演習は、セクション19.6のコードに依存しているため、これらのセクションのコードを実行する前に、セクション19.6のコードを実行する必要があります。
:::


<!-- ##  replication crisis? -->
## 再現危機？

<!-- A Financial Times article by Robert Wigglesworth discusses an alleged “replication crisis” in finance research.  -->
ロバート・ウィグルズワースによるFinancial Timesの記事は、ファイナンス研究における「再現危機」を取り上げた。
<!-- Wigglesworth quotes Campbell Harvey, professor of finance at Duke University, who suggests that “at least half of the 400 supposedly market-beating strategies identified in top financial journals over the years are bogus.” -->
Wigglesworthは、デューク大学のファイナンス教授であるCanmpbell Harveyの発言を引用しており、Harveyは「これまでのトップのファイナンス・ジャーナルで特定された400以上の市場を打ち負かすとされる戦略のうち、少なくとも半分は偽物である」と述べている。

<!-- Wigglesworth identified “the heart of the issue” as what researchers call p-hacking, which is the practice whereby researchers search for “significant” and “positive” results.  -->
Wigglesworthは、研究者が「有意な」および「ポジティブな」結果を探すという実践である**p-hacking**を研究者が行っていると指摘し、「問題の核心」を特定した。
<!-- Here “significant” refers to statistical significance and “positive” refers to results that reject so-called “null hypotheses” and thereby (purportedly) push human knowledge forward.  -->
ここで「有意性」とは統計的有意性を指し、「ポジティブ」とはいわゆる「帰無仮説」を棄却し、それによって（推定される）人間の知識を前進させる結果を指す。
<!-- Harvey (2017) cites research suggesting that 90% of published studies report such “significant” and “positive” results.  -->
Harvey（2017）は、90%の発表された研究がこのような「有意な」および「ポジティブな」結果を報告していると示唆する研究を引用している。
<!-- Reporting “positive” results is important not only for getting published, but also for attracting citations, which can drive behaviour of both researchers and journals. -->
「ポジティブ」な結果を報告することは、出版されるだけでなく、引用を集めるためにも重要であり、これは研究者やジャーナルの行動を促すことができる。

<!-- Simmons et al. (2011, p.1359) describe what they term researcher degrees of freedom.  -->
Simmons et al.（2011, p.1359）は、彼らが研究者の自由度と呼ぶものを説明している。
<!-- “In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected?  -->
<!-- Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both?”  -->
「データを収集し分析する過程で、研究者は多くの決定を下さなければならない。より多くのデータを収集すべきか？ 一部の観測を除外すべきか？ どの条件を統合し、どの条件を比較すべきか？ どの統制変数を考慮すべきか？ 特定の測定値を統合すべきか、それとも変換すべきか、あるいはその両方を行うべきか？」
<!-- Simmons et al. (2011, p.1364) identify another well-known **researcher degree of freedom**, namely that of “reporting only experiments that ‘work’”, which is known as the **file-drawer problem** (because experiments that don’t “work” are put in a file-drawer). -->
Simmons et al. (2011, p.1364) は、もう一つのよく知られた**研究者の自由度**として、「うまくいった実験のみを報告する」ことを挙げている。これは「**ファイルドロワー問題**」として知られており、うまくいかなかった実験が引き出しにしまわれてしまうことを指す。

<!-- To illustrate the power of researcher degrees of freedom, Simmons et al. (2011) conducted two hypothetical studies based experiments with live subjects.  -->
研究者の自由度の影響力を示すために、Simmons et al. (2011) は、ライブの被験者を用いた実験に基づく2つの仮想的な研究を実施した。
<!-- They argue that these studies “demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis” [p.1359].  -->
彼らは、これらの研究が「統計的に有意な証拠を、誤った仮説のために蓄積し（そして報告し）てしまうことが、いかに容認できないほど容易であるかを示している」と主張している [p.1359]。
<!-- Simmons et al. (2011, p.1359) conclude that “flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates.” -->
また、Simmons et al. (2011, p.1359) は、「データ収集、分析、および報告における柔軟性が、実際の偽陽性率を劇的に上昇させる」と結論付けている。

<!-- Perhaps in response to concerns similar to those raised by Simmons et al. (2011), the Journal of Accounting Research (JAR) conducted a trial for its annual conference held in May 2017.  -->
おそらく、Simmons at al. (2011) が提起したものと同様の懸念に対応する形で、Journal of Accounting Research (JAR) は、2017年5月に開催された年次カンファレンスにおいて試験的な取り組みを実施した。
<!-- According to the JAR website, at this conference “authors presented papers developed through a Registration-based Editorial Process (REP).  -->
JARのウェブサイトによると、このカンファレンスでは「著者が登録ベースの編集プロセス（Registration-based Editorial Process, REP）を通じて作成した論文を発表した」。
<!-- The goal of the conference was to see whether REP could be implemented for accounting research, and to explore how such a process could be best implemented.  -->
このカンファレンスの目的は、会計研究においてREPを導入できるかを検証し、またその最適な実施方法を探ることであった。
<!-- Papers presented at the conference were subsequently published in May 2018. -->
カンファレンスで発表された論文は、2018年5月にその後出版された。


<!-- According to Bloomfield et al. (2018, p.317), “under REP, authors propose a plan to gather and analyze data to test their predictions.  -->
Bloomfield et al. (2018, p.317) によると、「REPでは、著者が自身の予測を検証するためにデータを収集・分析する計画を提案する。
<!-- Journals send promising proposals to one or more reviewers and recommend revisions.  -->
ジャーナルは、有望な提案を一人または複数の査読者に送付し、修正を推奨する。
<!-- Authors are given the opportunity to review their proposal in response, often multiple times, before the proposal is either rejected or granted in-principle acceptance … regardless of whether [subsequent] results support their predictions.” -->
著者はこれに応じて提案を見直す機会を与えられ、しばしば複数回の修正を経た後、提案はリジェクトされるか、または‘原則的アクセプト’が与えられる。… これは、[その後の] 結果が彼らの予測を支持するかどうかに関わらず行われる。」

<!-- Bloomfield et al. (2018, p.317) contrast REP with the Traditional Editorial Process (“TEP”).  -->
Bloomfield et al. (2018, p.317) は、REPと従来の編集プロセス（Traditional Editorial Process, TEP）を対比している。
<!-- Under the TEP, “authors gather their data, analyze it, and write and revise their manuscripts repeatedly before sending them to editors.”  -->
TEPのもとでは、「著者はデータを収集し、分析し、原稿を執筆・修正したうえで、編集者に送る前に何度も見直しを行う」。
<!-- Bloomfield et al. (2018, p.317) suggest that “almost all peer-reviewed articles in social science are published under … the TEP.” -->
また、Bloomfield et al. (2018, p.317) は、「社会科学の査読付き論文のほぼすべてが … TEPのもとで出版されている」と指摘している。

<!-- The REP is designed to eliminate questionable research practices, including those identified by Simmons et al. (2011).  -->
REPは、Simmons et al. (2011)によって特定されたものを含む疑問の余地のある研究手法を排除するために設計されている。
<!-- For example, one form of p-hacking is **HARKing** (from “Hypothesizing After Results are Known”).  -->
たとえば、p-hackingの一形態は**HARKing**（「結果がわかってから仮説を立てる」）である。
<!-- In its extreme form, HARKing involves searching for a “significant” correlation and then developing a hypothesis to “predict” it.  -->
その極端な形態では、HARKingは「有意な」相関を探し、それを「予測」するための仮説を立てることを含む。
<!-- To illustrate, consider the spurious correlations website provided by Tyler Vigen.  -->
たとえば、Tyler Vigenが提供する[見せかけの相関のウェブサイト](https://tylervigen.com/spurious-correlations)を考えてみよう。
<!-- This site lists a number of evidently spurious correlations, such as the 99.26% correlation between the divorce rate in Maine and margarine consumption or the 99.79% correlation between US spending on science, space, and technology and suicides by hanging, strangulation, and suffocation.  -->
このサイトは、メイン州の離婚率とマーガリンの消費量の間の99.26%の相関や、アメリカの科学、宇宙、技術への支出と絞首、絞殺、窒息による自殺との間の99.79%の相関など、明らかにスパリアスな相関をいくつかリストしている。
<!-- The correlations are deemed spurious because normal human beings have strong prior beliefs that no underlying causal relation explains these correlations.  -->
これらの相関は、通常の人間は、これらの相関を説明する根底にある因果関係がないという強い事前の信念を持っているため、見せかけのものと見なされる。
<!-- Instead, these are regarded as mere coincidence. -->
これらは単なる偶然と見なされる。

<!-- However, a creative academic can probably craft a story to “predict” any correlation.  -->
しかし、創造的な学者はおそらく、どんな相関も「予測」するためのストーリーを作り出すことができる。
<!-- Perhaps increasing spending on science raises its perceived importance to society.  -->
たとえば、科学への支出を増やすことで、それが社会にとって重要であるという認識が高まるかもしれない。
<!-- But drawing attention to science only serves to highlight how the United States has inevitably declined in relative stature in many fields, including science.  -->
しかし、科学に注目することは、アメリカが科学を含む多くの分野で相対的に衰退していることを浮き彫りにするだけである。
<!-- While many Americans can carry on notwithstanding this decline, others are less sanguine about it and may go to extreme lengths as a result … .  -->
この衰退にもかかわらず、多くのアメリカ人はこれを乗り越えることができるが、他の人々はそれについて楽観的ではなく、その結果として極端な手段に出るかもしれない…。

<!-- This is clearly a silly line of reasoning, but if one added some references to published studies and fancy terminology, it would probably read a lot like the hypothesis development sections of some academic papers. -->
これは明らかにばかげた推論であるが、もしいくつかの参考文献や洒落た用語を加えれば、おそらくいくつかの学術論文の仮説開発セクションとよく似たものになるだろう。

<!-- Bloomfield et al. (2018, p.326) examine “the strength of the papers’ results” from the 2017 JAR conference in their section 4.2 and conclude that “of the 30 predictions made in the … seven proposals, we count 10 as being supported at $p \leq 0.05$ by at least one of the 134 statistical tests the authors reported.  -->
Bloomfield et al. (2018, p.326) は、2017年のJARカンファレンスからの「論文の結果の強さ」を彼らのセクション4.2で検討し、結論付けている。「… 7つの提案で行われた30の予測のうち、著者が報告した134の統計的検定のうち、少なくとも1つで$p \leq 0.05$で支持されると数えられるものは10つある。
<!-- The remaining 20 predictions are not supported at $p \leq 0.05$ by any of the 84 reported tests.  -->
残りの20の予測は、報告された84の検定のいずれにも$p \leq 0.05$で支持されていない。
<!-- Overall, our analysis suggests that the papers support the authors’ predictions far less strongly than is typical among papers published in JAR and its peers.” [^2] -->
全体的に、我々の分析は、論文がJARおよびその同僚誌に掲載された論文の中で一般的なよりもはるかに弱く、著者の予測を支持している」と述べている。[^2]

<!-- ###  Discussion questions -->
### ディスカッション課題

<!-- 1. Simmons et al. (2011) provide a more in-depth examination of issues with the TEP discussed in Bloomfield et al. (2018, pp.318–319).  -->
1. Simmons et al. (2011) は、Bloomfield et al. (2018, pp.318–319)で議論されたTEPの問題について、より詳細な検討を行っている。
<!-- Do you think the two experiments studied by Simmons et al. (2011) are representative of how accounting research works in practice?  -->
Simmons et al.（2011）が研究した2つの実験は、実際の会計研究の進め方を代表しているといえるか。
<!-- What differences are likely to exist in empirical accounting research using archival data? -->
アーカイバル・データを使用した実証会計研究には、どのような違いがあるのか。

<!-- 2. Bloomfield et al. (2018, p.326) say “we exclude Hail et al. (2018) from our tabulation [of results] because it does not state formal hypotheses.”  -->
2. Bloomfield et al.（2018, p.326）は、「Hail et al.（2018）は正式な仮説を提示していないため、[結果の] 集計から除外した」と述べている。
<!-- Given the lack of formal hypotheses, do you think it made sense to include the proposal from Hail et al. (2018) in the 2017 JAR conference?  -->
正式な仮説がないにもかかわらず、Hail et al. (2018) の提案を2017年の Journal of Accounting Research（JAR）カンファレンスに含めるのは妥当だったのだろうか？
<!-- Does the REP have relevance to papers without formal hypotheses?  -->
REPは、正式な仮説を持たない論文にも適用可能なのか？
<!-- Does the absence of formal hypotheses imply that Hail et al. (2018) were not testing hypotheses?  -->
正式な仮説がないことは、Hail et al. (2018) が仮説を検証していないことを意味するのか？
<!-- Is your answer to the last question consistent with how Hail et al. (2018, p.650) discuss results reported in Table 5 of that paper? -->
最後の質問に対するあなたの答えは、Hail et al. (2018, p.650) が論文の Table 5 に関する結果をどのように議論しているかと整合しているだろうか？

<!-- 3. According to the analysis of Bloomfield et al. (2018), there were 218 tests of 30 hypotheses and different hypotheses had different numbers of tests.  -->
3. Bloomfield et al. (2018)の分析によると、30の仮説に対して218のテストが行われ、異なる仮説には異なる数のテストが行われた。
<!-- In the following analysis, we assume 30 hypotheses with each having 7 tests (for a total of 210 tests).  -->
以下の分析では、30の仮説があり、それぞれが7つのテストを行うと仮定する（合計210のテスト）。
<!-- Does this analysis suggest an alternative possible interpretation of the results than the “far less strongly than is typical” conclusion offered by Bloomfield et al. (2018).  -->
この分析は、Bloomfield et al. (2018)が提供した「通常よりもはるかに弱い」という結論以外の結果の可能性の代替的な解釈を示唆しているだろうか。
<!-- Does choosing a different value for `set.seed()` alter the tenor of the results from this analysis? -->
`set.seed()`の値を変更すると、この分析の結果の傾向が変わるだろうか？
<!-- How might you make the analysis below more definitive?3 -->
以下の分析をより確定的にするためには、どのようにすればよいだろうか？[^3]


```{r}
set.seed(2021)
results <-
  expand_grid(hypothesis = 1:30, test = 1:7) |>
  mutate(p = runif(nrow(pick(everything()))),
         reject = p < 0.05)

results |> 
  group_by(hypothesis) |>
  summarize(reject_one = any(reject), .groups = "drop") |>
  count(reject_one)
```

<!-- 4. Bloomfield et al. (2018, p. 326) argue “it is easy to imagine revisions of several conference papers would allow them to report results of strength comparable to those found in most papers published under TEP.”  -->
4. Bloomfield et al. (2018, p. 326) は、「いくつかの会議論文の改訂により、TEP（Traditional Empirical Paradigm）で発表された論文の大半と同程度の強さの結果を報告できるようになることは容易に想像できる」と主張している。
<!-- For example, “Li and Sandino (2018) yielded no statistically significant support for their main hypotheses.  -->
例えば、「Li and Sandino (2018) は、主要な仮説に対する統計的に有意な支持を得られなかった。
<!-- However, they found significant results in their planned additional analyses that are consistent with informal predictions included in the accepted proposal.  -->
<!-- … [In light of this evidence] we are not ready to conclude that the studies in the issue actually provide weaker support for their predictions than most studies published under TEP.” (2018, p. 326).  -->
しかし、受理された提案書に含まれていた非公式な予測と整合的な計画的追加分析において、有意な結果を見出した。… [この証拠を踏まえると] 本号の研究が、TEP の下で発表された研究よりも予測に対する支持が弱いと結論づけるには至らない」（2018, p. 326）と述べている。
<!-- Can these results instead be interpreted as saying something about the strength of results of studies published under TEP? -->
これらの結果は、TEP のもとで発表された研究の結果の強さについて何かを示唆していると解釈することはできるだろうか？

<!-- 5. Do you believe that it would be feasible for REP to become the dominant research paradigm in accounting research? What challenges would such a development face? -->
5. REP（Registered Reports and Enhanced Transparency Paradigm）が会計研究の主要な研究パラダイムとなることは現実的だろうか。その発展にはどのような課題が伴うのか。

<!-- 6. A respondent to the survey conducted by Bloomfield et al. (2018, p. 337) provided the remark quoted below. Comment on this remark. What do you think the respondent has in mind with regard to the “learning channel”? Do you agree that the REP shuts down this channel? -->
6. Bloomfield et al.（2018, p. 337）が実施した調査の回答者のコメントを以下に引用する。このコメントについて論じよ。回答者が「学習チャネル（learning channel）」についてどのような考えを持っていると考えられるか。また、REPがこのチャネルを閉ざしてしまうという指摘に賛同するか。

> 「『帰無結果（null results）』が多く報告されることに驚きは感じない。それは自らの経験からも容易に予測できることだ。研究は反復的なプロセスであり、学習を伴うものだ。特に、非常に新規性の高い研究課題で、我々がまだほとんど知らない領域において、学習チャネルを閉ざすことで研究プロセスにおいて何か有益な発見が得られるとは思えない。」


## The Reg SHO experiment

<!-- To better understand the issues raised by the discussion above in a real research setting, we focus on the Reg SHO experiment, which has been the subject of many studies.  -->
上記の議論で取り上げられた問題を実際の研究環境でよりよく理解するために、多くの研究の対象となってきたReg SHO実験に焦点を当てる。
<!-- In July 2004, the SEC adopted Reg SHO, a regulation governing short-selling activities in equity markets.  -->
2004年7月、SECは、株式市場における空売り活動を統制する規制であるReg SHOを採択した。
<!-- Reg SHO contained a pilot program in which stocks in the Russell 3000 index were ranked by trading volume within each exchange and every third one was designated as a pilot stock.  -->
Reg SHOには、Russell 3000指数の株式が各取引所内で取引量によってランク付けされ、3つおきに1つがパイロット株式として指定されるパイロットプログラムが含まれていた。
<!-- From 2 May 2005 to 6 August 2007, short sales on pilot stocks were exempted from price tests, including the tick test for exchange-listed stocks and the bid test for NASDAQ National Market stocks. -->
2005年5月2日から2007年8月6日まで、パイロット株式に対する空売りは、取引所上場株式のチックテストやNASDAQナショナルマーケット株式のビッドテストを含む価格テストの対象外とされた。

<!-- In its initial order, the SEC stated that “the Pilot will allow [the SEC] to study trading behavior in the absence of a short sale price test.”  -->
最初の命令で、SECは「パイロットは、[SEC]が空売り価格テストのない状況での取引行動を研究することを可能にする」と述べている。
<!-- The SEC’s plan was to “examine, among other things, the impact of price tests on market quality (including volatility and liquidity), whether any price changes are caused by short selling, costs imposed by a price test, and the use of alternative means to establish short positions.” -->
SECの計画は、「価格テストが市場の質（ボラティリティや流動性を含む）に与える影響、価格変動が空売りによって引き起こされるかどうか、価格テストによって課されるコスト、および空売りポジションを確立するための代替手段の使用」などを調査することであった。

<!-- ### 19.2.1 The SHO pilot sample -->
### SHOパイロットサンプル

<!-- The assignment mechanism in the Reg SHO experiment is unusually transparent, even by the standards of natural experiments. -->
Reg SHO実験における割当メカニズムは、自然実験の基準に照らしても、極めて透明性が高い。
<!-- Nonetheless care is needed to identify the treatment and control firms and we believe it is instructive to walk through the steps needed to do so, as we do in this section.  -->
しかしながら、処置群および対照群の企業を特定する際には慎重を要する。本節では、その手順を具体的に示すことが有益であると考える。
<!-- (Readers who find the code details tedious could easily skip ahead to Section 19.2.3 on a first reading.  -->
（コードの詳細を煩雑に感じる読者は、初回の読解時には 19.2.3 節 に進むことも可能である。
<!-- We say “first reading” because there are subtle issues with natural experiments that this section helps to highlight, so it may be worth revisiting this section once you have read the later material.) -->
ただし、「初回の読解」と述べたのは、自然実験には微妙な論点が含まれており、本節がそれらを明確にする助けとなるためである。後の内容を読んだ後に本節に戻ることも有意義であろう。）

<!-- The SEC’s website provides data on the names and tickers of the Reg SHO pilot firms.  -->
SECのウェブサイトでは、Reg SHOパイロット企業の名称およびティッカーシンボルに関するデータが提供されている。
<!-- These data have been parsed and included as the sho_tickers data set in the farr package. -->
これらのデータは解析され、`farr`パッケージの`sho_tickers`データセットとして組み込まれている。


```{r}
sho_tickers
```

<!-- However, these are just the pilot firms and we need to use other sources to obtain the identities of the control firms.  -->
しかし、これらはあくまでパイロット企業に関するものであり、対照群の企業を特定するには別の情報源を利用する必要がある。
<!-- It might seem perverse for the SEC to have published lists of treatment stocks, but no information on control stocks.4  -->
SECが処置群の銘柄リストを公表していながら、対照群に関する情報を提供していないことは、一見すると不可解に思われるかもしれない。
<!-- One explanation for this choice might be that, because special action (i.e., elimination of price tests) was only required for the treatment stocks (for the control stocks, it was business as usual), no lists of controls were needed for the markets to implement the pilot. -->
ひとつの説明として、特別な措置（すなわち、価格テストの撤廃）が処置群の銘柄に対してのみ適用されたため、対照群の銘柄については市場において通常の取引が継続されることになり、パイロット実験の実施にあたって対照群のリストを作成する必要がなかった可能性がある。
<!-- Additionally, because the SEC had a list of the control stocks that it would use in its own statistical analysis, it had no reason to publish lists for this purpose.  -->
さらに、SEC自身が統計分析を行う際に対照群のリストを保有していたため、公表する必要がなかったとも考えられる。
<!-- Fortunately, while the SEC did not identify the control stocks, it provides enough information for us to do so, as we do below. -->
幸いなことに、SECは対照群の銘柄を明示的には特定していないものの、それを特定するために十分な情報を提供しており、本節ではその方法を示す。


<!-- First, we know that the pilot stocks were selected from the Russell 3000, the component stocks of which are found in the sho_r3000 data set from the farr package. -->
まず、パイロット銘柄は Russell 3000 の構成銘柄から選定されたことが分かっている。これらの構成銘柄は、`farr`パッケージの`sho_r3000`データセットに含まれている。


```{r}
sho_r3000
```

<!-- While the Russell 3000 contains 3,000 securities, the SEC and Black et al. (2019) tell us that, in constructing the pilot sample, the SEC excluded 32 stocks in the Russell 3000 index that, as of 25 June 2004, were not listed on the Nasdaq National Market, NYSE, or AMEX “because short sales in these securities are currently not subject to a price test.”  -->
Russell 3000には3,000銘柄が含まれているが、SECおよびBlack et al. (2019)によると、パイロットサンプルの構築にあたり、SECは以下の基準で一部の銘柄を除外している。
まず、2004年6月25日時点でNasdaq National Market, NYSE, AMEX に上場していなかった32銘柄を除外している。
SECによれば、これらの銘柄は「現時点で空売りに対する価格テストの適用対象外である」ため、パイロットサンプルから除外された。
<!-- The SEC also excluded 12 stocks that started trading after 30 April 2004 due to IPOs or spin-offs. And, from Black et al. (2019), we know there were two additional stocks that stopped trading after 25 June 2004 but before the SEC constructed its sample on 28 June 2004.  -->
さらに、2004年4月30日以降に IPO または スピンオフ により新規上場した 12銘柄 も除外された。
また、Black et al. (2019) によると、2004年6月25日以降、SECがサンプルを構築する2004年6月28日までの間に取引が停止した2銘柄 も除外されている。
<!-- We can get the data for each of these criteria from CRSP, but we need to first merge the Russell 3000 data with CRSP to identify the right PERMNO for each security.  -->
これらの基準に該当する銘柄を特定するためには、CRSP のデータを活用する必要がある。ただし、その前にRussell 3000のデータとCRSPを統合し、各銘柄に対応する適切な PERMNO を特定する必要がある。
<!-- For this purpose, we will use data from the five CRSP tables below: -->
そのために、以下のCRSPの5つのデータテーブルを用いる。

:::{.panel-tabset}

### PostgreSQL

```{r}
#| eval: false
db <- dbConnect(RPostgres::Postgres(), bigint = "integer")

mse <- tbl(db, Id(schema = "crsp", table = "mse"))
msf <- tbl(db, Id(schema = "crsp", table = "msf"))
stocknames <- tbl(db, Id(schema = "crsp", table = "stocknames"))
dseexchdates <- tbl(db, Id(schema = "crsp", table = "dseexchdates"))
ccmxpf_lnkhist <- tbl(db, Id(schema = "crsp", table = "ccmxpf_lnkhist"))
```

### parquet

```{r}
#| eval: false

db <- dbConnect(duckdb::duckdb())

mse <- load_parquet(db, schema = "crsp", table = "mse")
msf <- load_parquet(db, schema = "crsp", table = "msf")
stocknames <- load_parquet(db, schema = "crsp", table = "stocknames")
dseexchdates <- load_parquet(db, schema = "crsp", table = "dseexchdates")
ccmxpf_lnkhist <- load_parquet(db, schema = "crsp", table = "ccmxpf_lnkhist")
```


:::


<!-- One thing we note is that some of the tickers from the Russell 3000 sample append the class of stock to the ticker. We can detect these cases by looking for a dot (.) using regular expressions. Because a dot has special meaning in regular expressions (regex), we need to escape it using a backslash (\). (For more on regular expressions, see Chapter 9 and references cited there.) Because a backslash has a special meaning in strings in R, we need to escape the backslash itself to tell R that we mean a literal backslash. In short, we use the regex \\. to detect dots in strings. -->
ラッセル3000のサンプルに含まれる一部のティッカーは、ティッカーに株式のクラスを付加している。このようなケースを検出するには、正規表現（regex）を使用してドット（`.`）を探す。ただし、ドットは正規表現において特別な意味を持つため、バックスラッシュ（`\`）を使ってエスケープする必要がある。Rの文字列内では、バックスラッシュ自体にも特別な意味があるため、入力通りにバックスラッシュを表現するには、さらにバックスラッシュをエスケープしなければならない。したがって、ドットを検出するには、正規表現として "`\\.`" を使用する。


```{r}
sho_r3000 |> 
  filter(str_detect(russell_ticker, "\\."))
```

<!-- In these cases, CRSP takes a different approach.  -->
このような場合、CRSPは異なるアプローチを取る。
<!-- For example, where the Russell 3000 sample has AGR.B, CRSP has ticker equal to AGR and shrcls equal to B. -->
例えば、ラッセル3000のサンプルでは AGR.B となっているが、CRSPではティッカーが AGR、株式クラス（shrcls）が B となっている。


<!-- The other issue is that some tickers from the Russell 3000 data have the letter E appended to what CRSP shows as just a four-letter ticker. -->
もう一つの問題は、ラッセル3000のデータに含まれる一部のティッカーが、CRSPでは4文字のティッカーとして表示されるのに対し、末尾に`E`が付加されているケースがあることである。


```{r}
sho_r3000 |> 
  filter(str_length(russell_ticker) == 5,
         str_sub(russell_ticker, 5, 5) == "E")
```

<!-- A curious reader might wonder how we identified these two issues with tickers, and how we know that they are exhaustive of the issues in the data. We explore these questions in the exercises at the end of this section. -->
好奇心のある読者は、これら2つのティッカーの問題をどのように特定したのか、また、それらがデータ内の問題を網羅しているとどのように判断したのか疑問に思うかもしれない。これらの問いについては、本節の演習で詳しく探求する。

<!-- To address these issues, we create two functions: one (`clean_ticker()`) to “clean” each ticker so that it can be matched with CRSP, and one (`get_shrcls()`) to extract the share class (if any) specified in the Russell 3000 data. -->
これらの問題に対応するために、2つの関数を作成する。1つは、ティッカーをCRSPと照合できるように“クリーン”にする`clean_ticker()`であり、もう1つは、ラッセル3000データ内で指定されている株式クラス（もしあれば）を抽出する`get_shrcls()`である。

<!-- Both functions use a regular expression to match cases where the text ends with either “A” or “B” (`[AB]$` in regex) preceded by the a dot (`\\.` in regex, as discussed above). The expression uses capturing parentheses (i.e., `(` and `)`) to capture the text before the dot from the beginning of the string `(^(.*))` to the dot and to capture the letter “A” or “B” at the end (`([AB])$`). -->
どちらの関数も、正規表現を用いて、テキストがドット（`\\.`）の後にAまたはBで終わるケースを検出する（正規表現では`[AB]$`）。
この表現では、キャプチャ用の括弧( `(` と `)` )を使用し、文字列の先頭からドットまでの部分を `(^(.*))` で、末尾のAまたはBを`([AB])$`でキャプチャする。


```{r}
regex <- "^(.*)\\.([AB])$"
```

<!-- The `clean_ticker()` function uses `case_when()`, which first handles cases with an E at the end of five-letter tickers, then applies the regex to extract the “clean” ticker (the first captured text) from cases matching `regex`. Finally, the original ticker is returned for all other cases. -->
`clean_ticker()`関数は、`case_when()`を使用して、5文字のティッカーの末尾にEが付加されているケースを処理し、次に`regex`に一致するケースから“クリーン”なティッカー（最初にキャプチャされたテキスト）を抽出する。最後に、それ以外のケースでは元のティッカーを返す。


```{r}
clean_ticker <- function(x) {
  case_when(str_length(x) == 5 & str_sub(x, 5, 5) == "E" ~ str_sub(x, 1, 4),
            str_detect(x, regex) ~ str_replace(x, regex, "\\1"),
            .default = x)
}
```

<!-- The `get_shrcls()` function extracts the second capture group from the regex (the first value returned by `str_match()` is the complete match, the second value is the first capture group, so we use `[, 3]` to get the second capture group). -->
`get_shrcls()`関数は、正規表現から2番目のキャプチャグループを抽出する（`str_match()`が返す最初の値は完全な一致、2番目の値は最初のキャプチャグループであるため、2番目のキャプチャグループを取得するために`[, 3]`を使用する）。


```{r}
get_shrcls <- function(x) {
  str_match(x, regex)[, 3]
}
```

<!-- We can use `clean_ticker()` and `get_shrcls()` to construct `sho_r3000_tickers`. -->
`clean_ticker()`と`get_shrcls()`を使用して、`sho_r3000_tickers`を構築することができる。


```{r}
sho_r3000_tickers <-
  sho_r3000 |>
  select(russell_ticker, russell_name) |>
  mutate(ticker = clean_ticker(russell_ticker),
         shrcls = get_shrcls(russell_ticker))

sho_r3000_tickers |>
  filter(russell_ticker != ticker)
```

<!-- Now that we have “clean” tickers, we can merge with CRSP. The following code proceeds in two steps. First, we create `crsp_sample`, which contains the `permno`, `ticker`, and `shrcls` values applicable on `2004-06-25`, the date on which the Russell 3000 that the SEC used was created. -->
これで“クリーン”なティッカーを取得したので、CRSPと結合することができる。以下のコードは2つのステップで進行する。まず、SECが使用したRussell 3000が作成された日である`2004-06-25`に適用される`permno`、`ticker`、`shrcls`の値を含む`crsp_sample`を作成する。

```{r}
#| eval: false
crsp_sample <-
  stocknames |>
  mutate(test_date = as.Date("2004-06-25")) |>
  filter(test_date >= namedt, test_date <= nameenddt) |>
  select(permno, permco, ticker, shrcls) |>
  distinct() |>
  collect()
```


<!-- Second, we merge `sho_r3000_tickers` with `crsp_sample` using `ticker` and then use `filter()` to retain cases where, if a share class is specified in the SEC-provided ticker, it matches the one row in CRSP with that share class, while retaining all rows where no share class is specified in the SEC-provided ticker. -->
次に、`ticker`を使用して`sho_r3000_tickers`と`crsp_sample`を結合し、`filter()`を使用して、SECが提供したティッカーに株式クラスが指定されている場合、その株式クラスに一致するCRSPの1行を保持し、SECが提供したティッカーに株式クラスが指定されていない場合はすべての行を保持する。


```{r}
#| eval: false
sho_r3000_merged <-
  sho_r3000_tickers |>
  inner_join(crsp_sample, by = "ticker", suffix = c("", "_crsp")) |>
  filter(shrcls == shrcls_crsp | is.na(shrcls)) |>
  select(russell_ticker, permco, permno)
```

<!-- Unfortunately, this approach results in some tickers being matched to multiple PERMNO values. -->
残念ながら、このアプローチでは、いくつかのティッカーが複数のPERMNO値にマッチする結果となる。


```{r}
#| eval: false
sho_r3000_merged |>
  group_by(russell_ticker) |>
  filter(n() > 1) |>
  ungroup()
```

<!-- In each case, these appear to be cases where there are multiple securities (permno values) for the same company (permco value). To choose the security that is the one most likely included in the Russell 3000 index used by the SEC, we will keep the one with the greatest dollar trading volume for the month of June 2004. We collect the data on dollar trading volumes in the data frame trading_vol. -->
各ケースでは、同じ企業（`permco`値）に対して複数の証券（`permno`値）が存在するようである。SECが使用したRussell 3000指数に最も含まれていると考えられる証券を選択するために、2004年6月の最も大きな取引量を持つ証券を保持することにする。取引量に関するデータを`trading_vol`データフレームに収集する。

```{r}
#| eval: false
trading_vol <-
  msf |>
  filter(date == "2004-06-30") |> 
  mutate(dollar_vol = coalesce(abs(prc) * vol, 0)) |> 
  select(permno, dollar_vol) |>
  collect()
```

<!-- We can now make a new version of the table sho_r3000_merged that includes just the permno value with the greatest trading volume for each ticker. -->
これで、各ティッカーに対して最も取引量の多い`permno`値を含む新しいバージョンの`sho_r3000_merged`テーブルを作成することができる。



```{r}
#| eval: false
sho_r3000_merged <-
  sho_r3000_tickers |>
  inner_join(crsp_sample, by = "ticker", suffix = c("", "_crsp")) |>
  filter(is.na(shrcls) | shrcls == shrcls_crsp) |>
  inner_join(trading_vol, by = "permno") |>
  group_by(russell_ticker) |>
  filter(dollar_vol == max(dollar_vol, na.rm = TRUE)) |>
  ungroup() |>
  select(russell_ticker, permno)
```

<!-- Black et al. (2019) identify 32 stocks that are not listed on the Nasdaq National Market, NYSE, or AMEX firms “using historical exchange code (exchcd) and Nasdaq National Market Indicator (nmsind) from the CRSP monthly stock file” (in practice, these 32 stocks are smaller Nasdaq-listed stocks).  -->
Black et al. (2019) は、「CRSPの月次株式ファイルからの歴史的な取引所コード（`exchcd`）とNASDAQナショナルマーケットインジケータ（`nmsind`）を使用して、Nasdaq National Market、NYSE、AMEXに上場していない32銘柄を特定した」と述べている（実際には、これらの32銘柄は小規模なNasdaq上場銘柄である）。
<!-- However, exchcd and nmsind are not included in the crsp.msf file we use. Black et al. (2019) likely used the CRSP monthly stock file obtained from the web interface provided by WRDS, which often merges in data from other tables. -->
しかし、`exchcd`と`nmsind`は、私たちが使用している`crsp.msf`ファイルには含まれていない。Black et al. (2019) は、おそらくWRDSが提供するWebインターフェースから取得したCRSPの月次株式ファイルを使用しており、このファイルは他のテーブルからのデータを結合することが多い。

<!-- Fortunately, we can obtain `nmsind` from the CRSP monthly events file (`crsp.mse`).  -->
幸いなことに、CRSPの月次イベントファイル（`crsp.mse`）から`nmsind`を取得することができる。
<!-- This file includes information about delisting events, distributions (such as dividends), changes in NASDAQ information (such as `nmsind`), and name changes. We get data on `nmsind` by pulling the latest observation on `crsp.mse` on or before `2004-06-28` where the event related to NASDAQ status (`event == "NASDIN"`). -->
このファイルには、上場廃止イベント、配当などの配布、NASDAQ情報の変更（`nmsind`など）、名前の変更に関する情報が含まれている。`nmsind`に関するデータは、`2004-06-28`以前の`crsp.mse`の最新の観測値を取得し、NASDAQステータスに関連するイベント（`event == "NASDIN"`）を取得することで取得できる。


```{r}
#| eval: false
nmsind_data <-
  mse |> 
  filter(date <= "2004-06-28", event == "NASDIN") |>
  group_by(permno) |>
  filter(date == max(date, na.rm = TRUE)) |>
  ungroup() |>
  select(permno, date, nmsind) |>
  collect()
```

<!-- We can obtain exchcd from the CRSP stock names file (`crsp.stocknames`), again pulling the value applicable on `2004-06-28`.[^5] -->
`exchcd`は、CRSPの株式名ファイル（`crsp.stocknames`）から取得することができる。再度、`2004-06-28`に適用される値を取得する。[^5]

[^5]: We exclude non-positive values of `exchcd` as these appear to be rows flagging special temporary trading statuses that create duplicate rows.

```{r}
#| eval: false
exchcd_data <-
  stocknames |>
  filter(exchcd > 0) |>
  mutate(test_date = as.Date("2004-06-28")) |>
  filter(between(test_date, namedt, nameenddt)) |>
  select(permno, exchcd) |>
  distinct() |>
  collect()
```

<!-- According to [its website](https://www.sec.gov/rule-release/34-50104), the SEC “also excluded issuers whose initial public offerings commenced after April 30, 2004.”  -->
SECの[ウェブサイト](https://www.sec.gov/rule-release/34-50104)によると、「2004年4月30日以降に開始された新規株式公開（IPO）を行った発行体も除外した」という。
<!-- Following Black et al. (2019), we use CRSP data to identify these firms.  -->
Black et al. (2019) に従い、これらの企業を特定するためにCRSPデータを使用する。
<!-- Specifically, the table `crsp.dseexchdates` includes the variable `begexchdate`. -->
具体的には、テーブル`crsp.dseexchdates`には変数`begexchdate`が含まれている。


```{r}
#| eval: false

ipo_dates <-
  dseexchdates |> 
  distinct(permno, begexchdate) |> 
  collect()
```

<!-- Finally, it appears that there were stocks listed in the Russell 3000 file likely used by the SEC (created on 2004-06-25) that were delisted prior to 2004-06-28, the date on which the SEC appears to have finalized the sample for its pilot program. We again use crsp.mse to identify these firms. -->
最後に、SECがパイロットプログラムのサンプルを最終的に確定したと思われる2004-06-28より前に上場廃止された銘柄が、SECがおそらく使用したRussell 3000ファイルに含まれていたようである。これらの企業を特定するために、再度`crsp.mse`を使用する。



```{r}
#| eval: false

recent_delistings <-
  mse |>
  filter(event == "DELIST", 
         between(date, "2004-06-25", "2004-06-28")) |>
  rename(delist_date = date) |>
  select(permno, delist_date) |>
  collect()
```

<!-- Now, we put all these pieces together and create variables nasdaq_small, recent_listing, and delisted corresponding to the three exclusion criteria discussed above. -->
これで、これらの要素を組み合わせて、上記で説明した3つの除外基準に対応する変数`nasdaq_small`、`recent_listing`、`delisted`を作成する。


```{r}
#| eval: false

sho_r3000_permno <-
  sho_r3000_merged |>
  left_join(nmsind_data, by = "permno") |>
  left_join(exchcd_data, by = "permno") |>
  left_join(ipo_dates,   by = "permno") |>
  left_join(recent_delistings, by = "permno") |>
  mutate(nasdaq_small = coalesce(nmsind == 3 & exchcd == 3, FALSE), 
         recent_listing = begexchdate > "2004-04-30",
         delisted = !is.na(delist_date),
         keep = !nasdaq_small & !recent_listing & !delisted)
```

<!-- As can be seen in Table 19.1, we have a final sample of 2954 stocks that we can merge with sho_tickers to create the pilot indicator. -->
表19.1に示すように、パイロット指標を作成するために結合できる2954銘柄の最終サンプルが得られた。

```{r}
#| eval: false
sho_r3000_permno |> 
  count(nasdaq_small, recent_listing, delisted, keep)
```

```{r}
#| eval: false

sho_r3000_sample <-
  sho_r3000_permno |>
  filter(keep) |>
  rename(ticker = russell_ticker) |>
  left_join(sho_tickers, by = "ticker") |>
  mutate(pilot = !is.na(co_name)) |>
  select(ticker, permno, pilot)
```

<!-- As can be seen the number of treatment and control firms in sho_r3000_sample corresponds exactly with the numbers provided in Black et al. (2019, p. 42). -->
表19.1に示すように、`sho_r3000_sample`の処置群と対照群の企業数は、Black et al. (2019, p. 42)で提供された数値と完全に一致している。


```{r}
#| eval: false

sho_r3000_sample |>
  count(pilot)
```

<!-- Finally, we will want to link these data with data from Compustat, which means linking these observations with GVKEYs.  -->
最後に、これらのデータをCompustatのデータとリンクしたいと考えている。つまり、これらの観測値をGVKEYとリンクする必要がある。
<!-- For this, we use ccm_link (as used and discussed in Chapter 7) to produce sho_r3000_gvkeys, the sample we can use in later analysis. -->
これには、後の分析で使用できるサンプルである`sho_r3000_gvkeys`を生成するために、`ccm_link`（第7章で使用および議論された）を使用する。


<!-- Finally, we will want to link these data with data from Compustat, which means linking these observations with GVKEYs.  -->
最後に、これらのデータをCompustatのデータとリンクしたいと考えている。つまり、これらの観測値をGVKEYとリンクする必要がある。
<!-- For this, we use `ccm_link` (as used and discussed in [Chapter 7]()) to produce `sho_r3000_gvkeys`, the sample we can use in later analysis. -->
これには、後の分析で使用できるサンプルである`sho_r3000_gvkeys`を生成するために、`ccm_link`（第7章で使用および議論された）を使用する。


```{r}
#| eval: false

ccm_link <-
  ccmxpf_lnkhist |>
  filter(linktype %in% c("LC", "LU", "LS"),
         linkprim %in% c("C", "P")) |>
  rename(permno = lpermno) |>
  mutate(linkenddt = coalesce(linkenddt, max(linkenddt, na.rm = TRUE))) |>
  select(gvkey, permno, linkdt, linkenddt)
```


<!-- Because we focus on a single “test date”, our final link table includes just two variables: `gvkey` and `permno`.[^6] -->
単一の「テスト日」に焦点を当てるため、最終的なリンクテーブルには`gvkey`と`permno`の2つの変数のみが含まれる。[^6]

[^6]: In other words, we don’t need date ranges in the `gvkeys` table.


```{r}
#| eval: false
gvkeys <-
  ccm_link |>
  mutate(test_date = as.Date("2004-06-28")) |>
  filter(between(test_date, linkdt, linkenddt)) |>
  select(gvkey, permno) |>
  collect()
```


<!-- Finally, we can add `gvkey` to `sho_r3000_sample` to create `sho_r3000_gvkeys`. -->
最後に、`sho_r3000_sample`に`gvkey`を追加して`sho_r3000_gvkeys`を作成することができる。

```{r}
#| eval: false
sho_r3000_gvkeys <-
  sho_r3000_sample |>
  inner_join(gvkeys, by = "permno")

sho_r3000_gvkeys
```



<!-- To better understand the potential issues with constructing the `pilot` indicator variable, it is useful to compare the approach above with that taken in Fang et al. ([2016](references.html#ref-Fang:2016uy)).  -->
`pilot`指標変数を構築する際の潜在的な問題をよりよく理解するために、上記のアプローチとFang et al. ([2016](references.html#ref-Fang:2016uy))のアプローチを比較することが有益である。
<!-- To construct `sho_data` as Fang et al. ([2016](references.html#ref-Fang:2016uy)) do, we use `fhk_pilot` from the `farr` package.[7](#fn7) We compare `sho_r3000_sample` and `sho_r3000_gvkeys` with `sho_data` in the exercises below. -->
Fang et al. ([2016](references.html#ref-Fang:2016uy))のように`sho_data`を構築するために、`farr`パッケージから`fhk_pilot`を使用する。以下の演習で、`sho_r3000_sample`と`sho_r3000_gvkeys`を`sho_data`と比較する。




```{r}
#| eval: false
sho_data <- 
  fhk_pilot |>
  select(gvkey, pilot) |>
  distinct() |>
  group_by(gvkey) |>
  filter(n() == 1) |>
  ungroup() |>
  inner_join(fhk_pilot, by = c("gvkey", "pilot")) 
```


###  Exercises


<!-- 1. Before running the following code, can you tell from output above how many rows this query will return?  -->
1. 以下のコードを実行する前に、上記の出力からこのクエリが何行を返すかを判断できますか？
<!-- What is this code doing?  -->
このコードは何をしているのか？
<!-- At what stage would code like this have been used in process of creating the sample above?  -->
このようなコードは、上記のサンプルを作成する過程でどの段階で使用されたか？
<!-- Why is code like this not included above? -->
なぜこのようなコードは上記に含まれていないのか？

```{r}
#| eval: false

sho_r3000 |>
  anti_join(crsp_sample, join_by(russell_ticker == ticker)) |>
  collect()
```


<!-- 2. Focusing on the values of `ticker` and `pilot` in `fhk_pilot`, what differences do you observe between `fhk_pilot` and `sho_r3000_sample`? -->
2. `fhk_pilot`の`ticker`と`pilot`の値に焦点を当てると、`fhk_pilot`と`sho_r3000_sample`の間にどのような違いがあるかを観察しなさい。
<!-- What do you believe is the underlying cause for these discrepancies? -->
これらの相違の根本的な原因は何だと思うか？


<!-- 3. What do the following observations represent?  -->
3. 以下の観察は何を表しているか？
<!-- Choose a few observations from this output and examine whether these reveal issues in the `sho_r3000_sample` or in `fhk_pilot`. -->
この出力からいくつかの観察を選択し、これらが`sho_r3000_sample`または`fhk_pilot`に問題を示しているかどうかを調べなさい。

```{r}
#| eval: false
sho_r3000_sample |>
  inner_join(fhk_pilot, by = "ticker", suffix = c("_ours", "_fhk")) |>
  filter(permno_ours != permno_fhk)
```


<!-- 4. In constructing the `pilot` indicator, FHK omit cases (`gvkey` values) where there is more than one distinct value for the indicator.  -->
4. `pilot`指標を構築する際、FHKは、指標の値が複数ある場合（`gvkey`値）にはケースを省略している。
<!-- A question is: Who are these firms?  -->
質問は：これらの企業は誰か？
<!-- Why is there more than one value for `pilot` for these firms?  -->
これらの企業に対して`pilot`に複数の値があるのはなぜか？
<!-- And does omission of these make sense?  -->
そして、これらを省略することは理にかなっているか？
<!-- (*Hint*: Identify duplicates in `fhk_pilot` and compare `sho_r3000_gvkeys` for these firms.) -->
（*ヒント*：`fhk_pilot`の重複を特定し、これらの企業について`sho_r3000_gvkeys`を比較する。）


<!-- 5. What issue is implicit in the output from the code below?  -->
5. 以下のコードの出力には何の問題が含まれているか？
<!-- How could you fix this issue?  -->
この問題をどのように修正できるか？
<!-- Would you expect a fix for this issue to significantly affect the regression results? Why or why not? -->
この問題の修正が回帰結果に大きな影響を与えると予想されるか？その理由は何か？


```{r}
#| eval: false
sho_data |> 
  count(gvkey, ticker) |> 
  arrange(desc(n))
```



<!-- ###  Early studies of Reg SHO -->
### Reg SHOの初期の研究

<!-- The first study of the effects of Reg SHO was conducted by the SEC’s own Office of Economic Analysis.  -->
Reg SHOの影響に関する最初の研究は、SECの経済分析局によって行われた。
<!-- [The SEC study](https://www.sec.gov/news/studies/2007/regshopilot020607.pdf) examines the “effect of pilot on short selling, liquidity, volatility, market efficiency, and extreme price changes” [p. 86]. -->
[SECの研究](https://www.sec.gov/news/studies/2007/regshopilot020607.pdf)は、「パイロットがショートセール、流動性、ボラティリティ、市場効率、および極端な価格変動に与える影響」を調査している[p. 86]。


<!-- The authors of the 2007 SEC study “find that price restrictions reduce the volume of executed short sales relative to total volume, indicating that price restrictions indeed act as a constraint to short selling. -->
2007年のSECの調査の著者らは、「価格規制は、総取引量に対する空売りの実行量を減少させることを確認しており、これは価格規制が実際に空売りの制約として機能していることを示している」と述べている。
<!-- However, in neither market do we find significant differences in short interest across pilot and control stocks.  -->
しかし、どの市場においても、パイロット銘柄とコントロール銘柄の間で空売り残高に有意な差は見られなかった。
<!-- … We find no evidence that short sale price restrictions in equities have an impact on option trading or open interest.  -->
また、株式の空売りに対する価格規制がオプション取引や建玉に影響を与える証拠も見つからなかった。
<!-- … We find that quoted depths are augmented by price restrictions but realized liquidity is unaffected.  -->
さらに、気配値の厚み（quoted depth）は価格規制によって増加するものの、実現流動性には影響を与えないことが分かった。
<!-- Further, we find some evidence that price restrictions dampen short term within-day return volatility, but when measured on average, they seem to have no effect on daily return volatility.” -->
加えて、価格規制が日中の短期的なリターンの変動を抑える証拠はあるものの、平均的に測定すると、日次リターンの変動性には影響を与えないようであることが示された。


<!-- The SEC researchers conclude “based on the price reaction to the initiation of the pilot, we find limited evidence that the tick test distorts stock prices—on the day the pilot went into effect, Listed Stocks in the pilot sample underperformed Listed Stocks in the control sample by approximately 24 basis points.  -->
<!-- However, the pilot and control stocks had similar returns over the first six months of the pilot.” -->
SECの研究者は次のように結論づけている。「パイロット・プログラムの開始に対する価格の反応に基づくと、ティック・テストが株価を歪めるという証拠は限定的である。パイロット・プログラムが発効した当日、パイロット・サンプルに含まれる上場株は、対照サンプルに含まれる上場株と比べて約24ベーシスポイント低いパフォーマンスを示した。しかし、パイロット株と対照株のリターンは、プログラム開始から最初の6か月間で類似していた。」


<!-- In summary, it seems fair to say that the SEC found that exemption from price tests had relatively limited effect on the market outcomes of interest, with no apparent impact on several outcomes. -->
要約すると、SECは価格テストの免除が市場の関心のある結果に対して比較的限定的な影響しか与えず、いくつかの結果には明らかな影響がなかったと判断したと言える。


<!-- @Alexander:2008th [p.84] “examine how price tests affect trader behavior and market quality, which are areas of interest given by the [SEC] in evaluating these tests.” @Alexander:2008th [p.86] find that NYSE pilot stocks have similar spreads, but smaller trade sizes, more short trades, more short volume, and smaller ask depths.  -->
<!-- With regard to Nasdaq, @Alexander:2008th [p.86] find that the removed “bid test is relatively inconsequential.” -->
@Alexander:2008th [p.84] は、「価格テストがトレーダーの行動や市場の質にどのような影響を与えるかを調査しており、これはSECがこれらのテストを評価する際に関心を持っている領域である」と述べている。@Alexander:2008th [p.86] によれば、NYSEのパイロット株はスプレッドが類似しているものの、取引サイズが小さく、ショート取引が増え、ショート取引量が多く、アスクの深さが小さくなっているとしている。
Nasdaqに関して、@Alexander:2008th [p.86] は「撤廃されたビッド・テストの影響は比較的軽微である」と指摘している。

<!-- @Diether:2009vu [p.37] find that “while short-selling activity increases both for NYSE- and Nasdaq-listed Pilot stocks, returns and volatility at the daily level are unaffected.” -->
@Diether:2009vu [p.37] は、「NYSEおよびNasdaqに上場するパイロット株では空売り活動が増加するものの、日次レベルでのリターンやボラティリティには影響がない」と述べている。


<!-- ###  Discussion questions and exercises -->
### ディスカッション課題と練習問題

<!-- 1. Earlier we identified one feature of a randomized controlled trial (RCT) as that “proposed analyses are specified in advance”, as in a registered reports process.  -->
<!-- Why do you think the SEC did not use a registered report for its 2007 paper?  -->
<!-- Do you think the analyses of the SEC would be more credible if conducted as part of a registered reports process? Why or why not? -->
1. 先ほど、ランダム化比較試験（RCT）の特徴の一つとして、「提案された分析が事前に指定される」ことを挙げた。これは登録報告（registered reports）のプロセスと同様である。
SECが2007年の論文で登録報告を使用しなかったのはなぜだと思うか。
また、SECの分析が登録報告プロセスの一環として実施されていれば、より信頼性が高まったと思うか。その理由も含めて説明せよ。

<!-- 2. Do you have concerns that the results @Alexander:2008th have been p-hacked? What factors increase or reduce your concerns in this regard? -->
2. @Alexander:2008th の結果がp-hackedされた可能性について懸念があるか。この点に関して懸念を増大させる要因と減少させる要因は何か。


<!-- 3. Evaluate the hypotheses found in the section of @Diether:2009vu entitled *Testable Hypotheses* with particular sensitivity to concerns about HARKing. What kind of expertise is necessary in evaluating hypotheses in this way? -->
3. @Diether:2009vu の**検証可能な仮説**というセクションに見られる仮説をHARKingに関する懸念に特に敏感に評価せよ。このような仮説を評価する際にはどのような専門知識が必要か。


<!-- 4. How might the SEC have conducted Reg SHO as part of a registered reports process open to outside research teams, such as @Alexander:2008th and @Diether:2009vu? How might such a process have been run? What challenges would such a process face? -->
4. SECは、@Alexander:2008th や@Diether:2009vu のような外部研究チームに公開された登録報告プロセスの一環としてReg SHOを実施することができたか。そのようなプロセスはどのように実施されたか。そのようなプロセスはどのような課題に直面するか。


<!-- ## Analysing natural experiments -->
## 自然実験の分析

<!-- Both Alexander and Peterson ([2008](references.html#ref-Alexander:2008th)) and Diether et al. ([2009](references.html#ref-Diether:2009vu)) use the difference-in-differences estimator (“DiD”) of the causal effect that we saw in [Chapter 3](reg-basics.html). The typical approach to DiD involves estimating a regression of the following form: -->
@Alexander:2008th と @Diether:2009vu は、[第3章](reg-basics.html)で見た因果効果の差分の差推定量（"DiD"）を使用している。DiDの典型的なアプローチは、以下の形式の回帰を推定することを含む。


$$
\begin{aligned}
Y_{it} = &\beta_0 + \beta_1 \times POST_t + \beta_2 \times TREAT _i + \\
&\beta _3 \times POST_t \times TREAT_i + \varepsilon_{it}
\end{aligned}
$$

<!-- In this specification, the estimated treatment effect is given by the fitted coefficient $\hat{\beta} _3$ . -->
このモデルで、推定された処置効果は、推定された係数 $\hat{\beta} _3$ によって与えられる。

<!-- While DiD is clearly popular among researchers in economics and adjacent fields, it is important to note that it is not obvious that it is the best choice in every experimental setting and that credible alternatives exist. -->
DiDは、経済学や関連分野の研究者の間で明らかに人気があるが、すべての実験設定で最良の選択肢であるとは明らかではなく、信頼性のある代替手法が存在することに注意することが重要である。


<!-- Another approach would be to limit the sample to the post-treatment period and estimate the following regression: -->
別のアプローチは、サンプルを処置後の期間に限定し、以下の回帰を推定することである。


$$
Y_{it} = \beta_0 + \beta_1 \times TREAT_i + \varepsilon_{it}
$$ 

<!-- In this specification, the estimated treatment effect is given by the fitted coefficient $\hat{\beta}_1$ .  -->
このモデルで、推定された処置効果は、推定された係数 $\hat{\beta}_1$ によって与えられる。
<!-- This approach is common in drug trials, which are typically conducted as RCTs.  -->
このアプローチは、通常RCTとして実施される薬物試験で一般的である。
<!-- For example, in the paxlovid trial “participants were randomised 1:1, with half receiving paxlovid and the other half receiving a placebo orally every 12 hours for five days.  -->
たとえば、paxlovid試験では、「参加者は1:1で無作為に割り付けられ、半分はpaxlovidを、もう半分は5日間12時間ごとに経口でプラセボを受け取った。」
<!-- Of those who were treated within three days of symptom onset, 0.8% (3/389) of patients who received paxlovid were admitted to hospital up to day 28 after randomization, with no deaths.  -->
「症状発現後3日以内に治療を受けた患者のうち、paxlovidを受け取った患者の0.8%（3/389）が入院し、死亡者はいなかった。」
<!-- In comparison, 7% (27/385) of patients who received placebo were admitted, with seven deaths.” ([Mahase, 2021, p. 1](references.html#ref-Mahase:2021tz)).  -->
「対照群の患者のうち、7%（27/385）が入院し、7人が死亡した。」([Mahase, 2021, p. 1](references.html#ref-Mahase:2021tz))。
<!-- For the hospital admission outcome, it would have been possible to incorporate prior hospitalization rates in a difference-in-differences analysis, but this would only make sense if hospitalization rates in one period had a high predictive power for subsequent hospitalization rates.[^8] -->
病院入院の結果については、差分の差分（difference-in-differences）分析に過去の入院率を組み込むことも可能であった。しかし、これはある期間の入院率がその後の入院率を高い精度で予測できる場合にのみ意味をなす。[^8]



<!-- Yet another approach would include pre-treatment values of the outcome variable as a control: -->
もう1つのアプローチは、処置前の結果変数の値を制御変数として含めることである。

$$
Y_{it} = \beta_0 + \beta_1 \times TREAT _i + \beta_2 \times Y_{i,t-1} + \varepsilon_{it}
$$

<!-- To evaluate each of these approaches, we can use simulation analysis. The following analysis is somewhat inspired by Frison and Pocock ([1992](references.html#ref-Frison:1992te)), who use different assumptions about their data more appropriate to their (medical) setting and who focus on mathematical analysis instead of simulations. -->
これらのアプローチを評価するために、シミュレーション分析を使用することができる。以下の分析は、Frison and Pocock ([1992](references.html#ref-Frison:1992te))に触発されたもので、彼らは（医学的な）設定に適した異なるデータの仮定を使用し、数学的な分析に焦点を当てている。


<!-- @Frison:1992te assume a degree of correlation in measurements of outcome variables for a given unit (e.g., patient) that is independent of the time between observations.  -->
@Frison:1992te は、観測間の時間に依存しない、ある単位（たとえば、患者）の結果変数の測定値における相関の程度を仮定している。
<!-- A more plausible model in many business settings would be correlation in outcome measures for a given unit (e.g., firm) that fades as observations become further apart in time.  -->
多くのビジネス設定では、ある単位（たとえば、企業）の結果変数の測定値における相関が、観測間の時間が経つにつれて薄れるというモデルがより妥当である。
<!-- Along these lines, we create `get_outcomes()` below to generate data for outcomes in the absence of treatment.  -->
このような考え方に沿って、以下の`get_outcomes()`を作成して、処置がない場合の結果のデータを生成する。
<!-- Specifically, we assume that, if there are no treatment or period effects, the outcome in question follows the autoregressive process embedded in `get_outcomes()`, which has the key parameter $\rho$ (`rho`).[^9] -->
具体的には、処置効果や期間効果がない場合、対象の結果は、`get_outcomes()`に埋め込まれた自己回帰過程に従うと仮定する。この過程には、主要なパラメータ $\rho$ (`rho`) が含まれている。[^9]

[^9]: 999



```{r}
#| eval: false

get_outcomes <- function(
  rho = 0, 
  periods = 7
  ) {
  e <- rnorm(periods)
  y <- rep(NA, periods)
  y[1] <- e[1]
  for (i in 2:periods) {
    y[i] <- rho * y[i - 1] + e[i]
  }
  tibble(t = 1:periods, y = y)
}
```


<!-- The `get_sample()` function below uses `get_outcomes()` for `n` firms for given values of `rho`, `periods` (the number of periods observed for each firm), and `effect` (the underlying size of the effect of treatment on `y`).  -->
次の`get_sample()`関数は、`rho`、`periods`（各企業について観測される期間の数）、および`effect`（`y`に対する処置の効果の基礎となるサイズ）の与えられた値に対して、`n`企業のために`get_outcomes()`を使用する。
<!-- Here treatment is randomly assigned to half the firms in the sample and the effect is added to `y` when both `treat` and `post` are true.  -->
ここでは、処置はサンプルの半分の企業にランダムに割り当てられ、`treat`と`post`の両方が真の場合に`y`に効果が追加される。
<!-- We also add a time-specific effect (`t_effect`) for each period, which is common to all observations (a common justification for the use of DiD is the existence of such period effects).  -->
また、各期間に対して時間特有の効果（`t_effect`）を追加し、すべての観測に共通である（DiDの使用の一般的な正当化の理由の1つは、このような期間効果が存在することである）。






```{r}
#| eval: false

get_sample <- function(n = 100, rho = 0, periods = 7, effect = 0) {
  treat <- sample(1:n, size = floor(n / 2), replace = FALSE)
  
  t_effects <- tibble(t = 1:periods, t_effect = rnorm(periods))
  
  f <- function(x) tibble(id = x, get_outcomes(rho = rho, 
                                               periods = periods))
  df <- 
    map(1:n, f) |>
    list_rbind() |> 
    inner_join(t_effects, by = "t") |>
    mutate(treat = id %in% treat,
           post = t > periods / 2,
           y = y + if_else(treat & post, effect, 0) + t_effect) |>
    select(-t_effect)
}
```


<!-- The `est_effect()` function below applies a number of estimators to a given data set and returns the estimated treatment effect for each estimator.  -->
以下の`est_effect()`関数は、与えられたデータセットにいくつかの推定量を適用し、各推定量の推定処置効果を返す。
<!-- The estimators we consider are the following (the labels *POST*, *CHANGE*, and *ANCOVA* come from [Frison and Pocock, 1992](references.html#ref-Frison:1992te)): -->
考慮する推定量は以下の通りである（ラベル*POST*、*CHANGE*、*ANCOVA*は[Frison and Pocock, 1992](references.html#ref-Frison:1992te)から来ている）。


<!-- - *DiD*, the difference-in-differences estimator estimated by regressing `y` on the treatment indicator, `treat` interacted with the post-treatment indicator, `post` (with the `[lm()](https://rdrr.io/r/stats/lm.html)` function automatically including the main effects of `treat` and `post`), as in [Equation 19.1](#eq-did). -->
- *DiD*は、処置指標`treat`と処置後指標`post`を交互作用させたものを回帰することによって推定される差分の差推定量であり、`treat`と`post`の主効果を自動的に含む`[lm()](https://rdrr.io/r/stats/lm.html)`関数によって推定される。[Equation 19.1](#eq-did)のようなものである。
<!-- - *POST*, which is based on OLS regression of `y` on `treat`, but with the sample restricted to the post-treatment observations, as in [Equation 19.2](#eq-post). -->
- *POST*は、`y`を`treat`に対してOLS回帰するが、サンプルを処置後の観測に制限したものであり、[Equation 19.2](#eq-post)のようなものである。
<!-- - *CHANGE*, which is based on OLS regression of the change in the outcome on `treat`. The change in outcome (`y_change`) is calculated as the mean of post-treatment outcome value (`y_post`) minus the mean of the pre-treatment outcome value (`y_pre`) for each unit. -->
- *CHANGE*は、アウトカムの変化を`treat`に対してOLS回帰するものである。アウトカムの変化（`y_change`）は、各単位について処置後のアウトカム値（`y_post`）の平均から処置前のアウトカム値（`y_pre`）の平均を引いたものである。
<!-- - *ANCOVA*, which is a regression of `y_post` on `treat` and `y_pre`, as in [Equation 19.3](#eq-ancova). -->
- *ANCOVA*は、`y_post`を`treat`と`y_pre`に対して回帰するものであり、[Equation 19.3](#eq-ancova)のようなものである。




```{r}
#| eval: false

est_effect <- function(df) {
  
  fm_DiD <- lm(y ~ treat * post, data = df)
  
  df_POST <- 
    df |> 
    filter(post) |>
    group_by(id, treat) |>
    summarize(y = mean(y), .groups = "drop")
    
  fm_POST <- lm(y ~ treat, data = df_POST)
  
  df_CHANGE <- 
    df |> 
    group_by(id, treat, post) |>
    summarize(y = mean(y), .groups = "drop") |>
    pivot_wider(names_from = "post", values_from = "y") |>
    rename(y_pre = `FALSE`,
           y_post = `TRUE`) |>
    mutate(y_change = y_post - y_pre) 
  
  fm_CHANGE <- lm(I(y_post - y_pre) ~ treat, data = df_CHANGE)
  fm_ANCOVA <- lm(y_post ~ y_pre + treat, data = df_CHANGE)
  
  tibble(est_DiD = fm_DiD$coefficients[["treatTRUE:postTRUE"]],
         est_POST = fm_POST$coefficients[["treatTRUE"]], 
         est_CHANGE = fm_CHANGE$coefficients[["treatTRUE"]], 
         est_ANCOVA = fm_ANCOVA$coefficients[["treatTRUE"]])
}
```


<!-- The `run_sim()` function below calls `get_sample()` for supplied parameter values to create a data set, and returns a data frame containing the results of applying `est_effect()` to that data set.  -->
この`run_sim()`関数は、与えられたパラメータ値に対して`get_sample()`を呼び出してデータセットを作成し、そのデータセットに`est_effect()`を適用した結果を含むデータフレームを返す。

```{r}
#| eval: false
run_sim <- function(i, n = 100, rho = 0, periods = 7, effect = 0) {
  df <- get_sample(n = n, rho = rho, periods = periods, effect = effect)
  tibble(i = i, est_effect(df))
}
```


<!-- To facilitate running of the simulation for various values of `effect` and `rho`, we create a data frame (`params`) with effect sizes running from 0 to 1 and $\rho \in \{ 0, 0.18, 0.36, 0.54, 0.72, 0.9 \}$ . -->
`effect`と`rho`のさまざまな値に対してシミュレーションを実行するために、`effect`が0から1まで、$\rho \in \{ 0, 0.18, 0.36, 0.54, 0.72, 0.9 \}$ であるデータフレーム（`params`）を作成する。



```{r}
#| eval: false

rhos <- seq(from = 0, to = 0.9, length.out = 6) 
effects <- seq(from = 0, to = 1, length.out = 5)
params <- expand_grid(effect = effects, rho = rhos)
```

<!-- The `run_sim_n()` function below runs 1000 simulations for the supplied values of `effect` and `rho` and returns a data frame with the results. -->
以下の`run_sim_n()`関数は、与えられた`effect`と`rho`の値に対して1000回のシミュレーションを実行し、結果を含むデータフレームを返す。

```{r}
#| eval: false

run_sim_n <- function(effect, rho, ...) {
  n_sims <- 1000
  set.seed(2021)
  
  res <- 
    1:n_sims |>
    map(\(x) run_sim(x, rho = rho, effect = effect)) |>
    list_rbind()
  
  tibble(effect, rho, res)
                                    
}
```


:::{.callout-important}
<!-- The following code takes several minutes to run. Using `future_pmap` from the `furrr` package in place of `pmap()` reduces the time needed to run the simulation significantly. -->
次のコードは実行に数分かかる。`pmap()`の代わりに`furrr`パッケージの`future_pmap`を使用すると、シミュレーションを実行するために必要な時間が大幅に短縮される。
<!-- Fortunately, nothing in the subsequent exercises requires that you run either variant of this code, so only do so if you have time and want to examine `results` directly. -->
幸いなことに、後続の演習では、このコードのいずれかを実行する必要はない。したがって、時間がある場合や`results`を直接調べたい場合にのみ実行すること。
:::


```{r}
#| eval: false
plan(multisession)

results <-
  params |> 
  future_pmap(run_sim_n, 
              .options = furrr_options(seed = 2021)) |> 
  list_rbind() |> 
  system_time()
```


<!-- With `results` in hand, we can do some analysis.  -->
`results`が手に入ったので、いくつかの分析を行うことができる。
<!-- The first thing to note is that `est_CHANGE` is equivalent to `est_DiD`, as all estimates are within rounding error of each other for these two methods. -->
最初に注意すべきことは、`est_CHANGE`が`est_DiD`と同等であることである。これら2つの方法について、すべての推定値がお互いに丸め誤差の範囲内にあるためである。

```{r}
#| eval: false

results |> 
  filter(abs(est_DiD - est_CHANGE) > 0.00001) |> 
  nrow()
```


<!-- Thus we just use the label *DiD* in subsequent analysis. -->
したがって、以降の分析では、*DiD*というラベルを使用する。

<!-- The second thing we check is that the methods provide unbiased estimates of the causal effect.  -->
2つ目に確認することは、各方法が因果効果のバイアスのない推定値を提供するかどうかである。
<!-- [Figure 19.1](#fig-bias) suggests that the estimates are very close to the true values of causal effects for all three methods. -->
[Figure 19.1](#fig-bias)は、すべての3つの方法について、推定値が因果効果の真の値に非常に近いことを示している。

```{r}
#| eval: false

results |>
  pivot_longer(starts_with("est"), 
               names_to = "method", values_to = "est") |>
  mutate(method = str_replace(method, "^est.(.*)$", "\\1")) |>
  group_by(rho, method) |>
  summarize(bias = mean(est - effect), .groups = "drop") |>
  filter(method != "CHANGE") |>
  ggplot(aes(x = rho, y = bias, 
             colour = method, linetype = method)) +
  geom_line() +
  ylim(-0.1, 0.1)
```


<!-- Having confirmed that there is no apparent bias in any of the estimators in this setting, we next consider the empirical standard errors for each method.  -->
この設定において、どの推定量にも明らかなバイアスがないことを確認した後、次に各方法の実証的標準誤差を考慮する。
<!-- Because we get essentially identical plots with each value of the true effect, we focus on `effect == 0.5` in the following analysis.  -->
真の効果の各値で本質的に同一のプロットを得るため、以下の分析では`effect == 0.5`に焦点を当てる。
<!-- Here we rearrange the data so that we have a `method` column and an `est` column for the estimated causal effect.  -->
ここでは、推定された因果効果の`method`列と`est`列を持つデータを再配置する。
<!-- We then calculate, for each `method` and value of `rho`, the standard deviation of `est`, which is the empirical standard error we seek.  -->
次に、各`method`と`rho`の値について、`est`の標準偏差を計算し、求めている実証的標準誤差を得る。
<!-- Finally, we plot the values for each value of `rho` in [Figure 19.2](#fig-ses). -->
最後に、[Figure 19.2](#fig-ses)に各`rho`の値をプロットする。



```{r}
#| eval: false
results |>
  filter(effect == 0.5) |>
  pivot_longer(
    starts_with("est"), # estで始まる列を対象
    names_to = "method",
    values_to = "est"
    ) |>
  mutate(
    method = str_replace(method, "^est.(.*)$", "\\1")
    ) |>
  filter(method != "CHANGE") |> # CHANGEは除外
  group_by(method, rho) |> # methodとrhoでグループ化
  summarize(
    se = sd(est), .groups = "drop" # estの標準偏差を計算
    ) |>
  ggplot() + 
    aes(x = rho, y = se, colour = method, linetype = method) +
    geom_line()
```


From the above, we can see that for low values of $\rho$ , subtracting pre-treatment outcome values adds noise to our estimation of treatment effects. We actually have lower standard errors when we throw away the pre-treatment data and just compare post-treatment outcomes. But for higher levels of $\rho$ , we see that *DiD* outperforms *POST*; by subtracting pre-treatment outcome values, we get a more precise estimate of the treatment effect. However, we see that both *DiD* and *POST* are generally outperformed by *ANCOVA*, which in effect allows for a flexible, data-driven relation between pre- and post-treatment outcome values.


In short, notwithstanding its popularity, it is far from clear that DiD is the best approach to use for all analyses of causal effects based on experimental data. Even in the context of the Reg SHO experiment, the appropriate method may depend on the outcome of interest. For a persistent outcome, *DiD* may be better than *POST*, but for a less persistent outcome, *POST* may be better than *DiD*. And *ANCOVA* may be a better choice than either *POST* or *DiD* unless there are strong *a priori* reasons to believe that *DiD* or *POST* is more appropriate (and such reasons seem more likely to hold for *POST* than for *DiD*).


## valuating natural experiments


Because of the plausibly random assignment mechanism used by the SEC, Reg SHO provides a very credible natural experiment. However, in many claimed natural experiments, it will be “nature” who is assigning treatment. In Michels ([2017](@Michels:2017uc)), it was literally nature doing the assignment through the timing of natural disasters. While natural disasters are clearly not completely random, as hurricanes are more likely to strike certain locations at certain times of year, this is not essential for the natural experiment to provide a setting from which causal inferences can be credibly drawn. What is necessary in Michels ([2017](@Michels:2017uc)) is that treatment assignment is **as-if random** with regard to the timing of the natural disaster before or after the end of the fiscal period and discussion questions in [Chapter 17](natural.html) explored these issues. 


More often in claimed natural experiments, it will be economic actors or forces, rather than nature, assigning treatment. While such economic actors and forces are unlikely to act at random, again the critical question is whether treatment assignment is *as-if* random. To better understand the issues, we consider a well-studied setting, that of brokerage closures as studied in @Kelly:2012ul


@Kelly:2012ul [p.1368] argue that “brokerage closures are a plausibly exogenous source of variation in the extent of analyst coverage, as long as two conditions are satisfied. First, the resulting coverage terminations must correlate with an increase in information asymmetry. … Second, the terminations must only affect price and demand through their effect on information asymmetry.” Interestingly, these are essentially questions 2 and 3 that we ask in evaluating instrumental variables in [Section 20.3](iv.html#sec-iv-reasoning). The analogue with instrumental variables applies because @Kelly:2012ul are primarily interested in the effects of changes in information asymmetry, not the effects of brokerage closures *per se*. In principle, brokerage closures could function much like an instrumental variable, except that @Kelly:2012ul estimate **reduced-form** regressions whereby outcomes are related directly to brokerage closures, such as in Table 3 @Kelly:2012ul [p.1391]. But the first of the three questions from [Section 20.3](iv.html#sec-iv-reasoning) remains relevant, and this is the critical question for evaluating any natural experiment: **Is treatment assignment (as-if) random?**


Like many researchers, @Kelly:2012ul do not address the (as-if) randomness of treatment assignment directly. Instead, @Kelly:2012ul focus on whether brokerage closure-related terminations of analyst coverage “constitute a suitably exogenous shock to the investment environment”. @Kelly:2012ul argue that they do “*unless* brokerage firms quit research because their analysts possessed negative private information about the stocks they covered.” But this reasoning is incomplete. For sure, brokerage firms not quitting research for the reason suggested is a *necessary* condition for a natural experiment (otherwise the issues with using brokerage closures as a natural experiment are quite apparent). But it is not a *sufficient* condition. If the firms encountering brokerage closure-related terminations of analyst coverage had different trends in information asymmetry for other reasons, the lack of private information is inadequate to give us a natural experiment.


In general, we should be able to evaluate the randomness of treatment assignment much as we would do so with an explicitly randomized experiment. Burt ([2000](references.html#ref-Burt:2000tm)) suggest that “statisticians will compare the homogeneity of the treatment group populations to assess the distribution of the pretreatment demographic characteristics and confounding factors.” With explicit randomization, statistically significant differences in pretreatment variables might prompt checks to ensure that, say, there was not “deliberate alteration of or noncompliance with the random assignment code” or any other anomalies. Otherwise, we might have greater confidence that randomization was implemented effectively, and hence that causal inferences might reliably be drawn from the study.


So, a sensible check with a natural experiment would seem to be to compare various pre-treatment variables across treatment groups to gain confidence that “nature” has indeed randomized treatment assignment. In this regard, Table 1 of @Kelly:2012ul is less than assuring. Arguably, one can only encounter brokerage closure-related terminations of analyst coverage if one has analyst coverage in the first place; so the relevant comparison is arguably between the first and third columns of data. There we see that the typical firm in the terminations sample (column 1) is larger, has higher monthly stock turnover, higher daily return volatility, and more brokers covering the stock than does the typical firm in the universe of covered stocks in 2004 (column 3). So clearly “nature” has not randomly selected firms from the universe of covered stocks in 2004.


However, we might come to a similar conclusion if we compared the Reg SHO pilot stocks with the universe of traded stocks or some other broad group. Just as it was essential to correctly identify the population that the SEC considered in randomizing treatment assignment, it is important to identify the population that “nature” considered in assigning treatment in evaluating any natural experiment. While the SEC provided a statement detailing how it constructed the sample, “nature” is not going to do the same and researchers need to consider carefully which units were considered for (as if) random assignment to treatment.


In this regard, even assuming that the controls used in Table 2 of@Kelly:2012ul [p.1388] were the ones “nature” herself considered, it seems that the natural experiment did not assign treatment in a sufficiently random way. Table 2 studies four outcomes: bid-ask spreads, the Amihud illiquidity measure, missing- and zero-return days, and measures related to earnings announcements. In each case, there are pre-treatment differences that sometimes exceed the DiD estimates. For example, pre-treatment bid-ask spreads for treatment and control firms are 1.126 and 1.089, a 0.037 difference that is presumably statistically significant given that the smaller DiD estimate of 0.020 has a p-value of 0.011.[10](#fn10) 
In light of this evidence, it seems that @Kelly:2012ul need to rely on the parallel trends assumption to draw causal inferences and we evaluate the plausibility of this assumption in the next section.


It is important to recognize that the shortcomings of broker closures as a natural experiment do not completely undermine the ability of @Kelly:2012ul to draw causal inferences. There appears to be an unfortunate tendency to believe, on the one hand, that without some kind of natural experiment, one cannot draw causal inferences. On the other hand, there is an apparent tendency to view natural experiments as giving *carte blanche* to researchers to draw all kinds of causal inferences, even when the alleged identification strategies do not, properly understood, support such inferences.


In the case of @Kelly:2012ul, it seems the authors would like to believe that they have a natural experiment that allows them to draw inferences about the effects of broker closures on information asymmetry (Table 2) and, because broker closures only affect stock prices through their effects on information asymmetry, to conclude from the evidence in Table 3 that increases in information asymmetry reduce stock prices. But Table 2 could have been based on a bullet-proof identification strategy without implying that broker closures only affect stock prices through their effects on information asymmetry. There is really no evidence offered for this claim, one that is arguably very difficult to support.


At the same time, it is conceptually possible that @Kelly:2012ul could provide compelling evidence that the only plausible explanation for the abnormal returns in Table 3 is reductions in information asymmetry, even if the results in Table 2 were irredeemably corrupted (e.g., because of failure of parallel trends). Evidence that firms did not “quit research because their analysts possessed negative private information about the stocks they covered” might support drawing certain inferences from Table 3 even without a credible equivalent of Table 2.


## The parallel trends assumption



Examining the studies of the direct effects of Reg SHO discussed in [Section 19.2.3](#sec-sho-early), we see that randomization generally provided balance in pre-treatment outcome values. In such settings, we see that DiD can provide unbiased estimates of causal effects, but that it has little appeal when treatment assignment is random. Indeed, if there is little to distinguish treatment and control observations in terms of pre-treatment outcome values, DiD will differ little from simply comparing differences in post-treatment means (the *POST* estimator discussed above).


But in many settings, such as that in @Kelly:2012ul, differences in pre-treatment outcome values exist, suggesting that random assignment is not an appropriate assumption. In such settings, it will therefore be necessary to rely on a different assumption to justify the use of DiD for causal inferences. This **parallel trends assumption** posits that, but for treatment, the expected change in the outcome variable for the treated observations would equal that for control observations. Using this assumption, we can attribute any difference in the change in the outcome variable between the treated and control observations to a treatment effect and random variation and use standard tools of statistical inference to evaluate the null hypothesis that any difference is due to random variation. 


That DiD can be predicated on an assumption other than (as-if) random assignment may explain its popularity. Cunningham ([2021, p. 406](references.html#ref-Cunningham:2021vk)) suggests that DiD “has become the single most popular research design in the quantitative social sciences” and @Armstrong:2021uo [p.4] find rapid “increase in [the number of] papers using quasi-experimental methods to draw causal inferences, that more than 75% of such papers use variations of the classic difference-in-differences design.” 


Arguably DiD is more often used in settings that *without* (as-if) random assignment of treatment. For example, one of the most highly cited papers using DiD is @Card:1994aa, which compares the change in employment in the fast-food industry in New Jersey and Philadelphia before and after an increase in the minimum wage in New Jersey. In this case, treatment assignment was very clearly non-random—it was a function of being located in New Jersey.


Unlike the assumption of random assignment, the parallel trends assumption is not implied by a reasonable description of a physical or economic process and thus is of a fundamentally different nature. Random assignment is widely regarded as a reasonable description of, say, a coin toss or the generation of pseudo-random numbers using a computer. In contrast, it is difficult to think of a mechanism for imposing parallel trends on the data. Because DiD is—unlike instrumental variables or regression discontinuity designs—generally *not* predicated on “as-if random variation in the explanatory variable of interest”, it is not correct to consider DiD as a **quasi-experimental method** ([Armstrong et al., 2022, p. 3](references.html#ref-Armstrong:2021uo)).[11](#fn11) 


Instead the basis for the parallel trends assumption appears to be that it is *the* assumption that is necessary (and sufficient) for the DiD estimator to provide an unbiased estimator of the causal effect of treatment. But [“assuming a can-opener”](https://en.wikipedia.org/wiki/Assume_a_can_opener) seems to be a weak foundation for an approach as widespread as DiD.


On the one hand, as discussed above, there is no obvious economic basis for the parallel trends assumption with general applicability. On the other hand, there are often reasons to believe that the parallel trends assumption will *not* hold for various outcomes. The parallel trends assumption will be dubious when the outcome variable tends to be mean-reverting. For example, it is well known that accounting-based measures of operating performance tend to revert towards the mean. So if treatment and control observations have different levels of pre-treatment operating performance, the parallel trends assumption will be a highly dubious basis for causal inference.


Another reason to doubt the parallel trends assumption is the fact that the measurement of outcomes is often arbitrary. For example, Li et al. ([2018](references.html#ref-Li:2018tj)) examine the effect of legal changes on disclosure of customer identities using a variant of DiD.[12](#fn12) One primary outcome measure considered by Li et al. ([2018](references.html#ref-Li:2018tj)) is $ratio$ , the proportion of significant customers whose identities are not disclosed. But if the parallel trends assumption holds in $ratio$ then, so long as there are pre-treatment differences between treatment and control observations, it is not mathematically possible for parallel trends to hold in $\log(1 + ratio)$ , which is the measure used in the regression analysis in Li et al. ([2018](references.html#ref-Li:2018tj)).


The apparent flimsiness of the parallel trends assumption underlying DiD analysis in non-randomized settings is perhaps reinforced by the treatment of DiD in textbooks. Imbens and Rubin ([2015](references.html#ref-Imbens:2015aa)), a significant recent tome on causal inference, buries DiD in endnotes, merely noting that DiD is “widely used” ([2015, p. 44](references.html#ref-Imbens:2015aa)) before directing the reader to Angrist and Pischke ([2008](references.html#ref-Angrist:2008vk)). While Angrist and Pischke ([2008](references.html#ref-Angrist:2008vk)) discuss DiD and its assumptions, and relate it to fixed-effects regressions and panel data methods, they do little to justify the parallel trends assumption. Cunningham ([2021](references.html#ref-Cunningham:2021vk)) is much more cautious in discussing the parallel trends assumption, which he notes “is by definition untestable since we cannot observe this counterfactual conditional expectation [of post-treatment outcomes absent treatment]”.


Two popular approaches to address the parallel trends assumption are discussed by Cunningham ([2021](references.html#ref-Cunningham:2021vk)) and Huntington-Klein ([2021](references.html#ref-Huntington-Klein:2021uc)). The first approach compares the trends in pre-treatment outcome values for treatment and control observations. If these trends are similar before treatment, it is perhaps reasonable to assume they are similar after treatment. But Cunningham ([2021, p. 426](references.html#ref-Cunningham:2021vk)) notes that “pre-treatment similarities are neither necessary nor sufficient to guarantee parallel counterfactual trends” and this seems an especially dubious assumption if treatment is endogenously selected.


The second approach is the placebo test, variants of which are discussed by Cunningham ([2021](references.html#ref-Cunningham:2021vk)) and Huntington-Klein ([2021](references.html#ref-Huntington-Klein:2021uc)). One placebo test involves evaluating the treatment effect in a setting where prior beliefs hold that there should be no effect. Another approach involves a kind of random assignment of a pseudo-treatment. In either case, not finding an effect is considered as providing support for the parallel trends assumption in the analysis of greater interest to the researcher. Of course, one might be sceptical of such placebo tests in light of the concerns raised at the start of this chapter. If applying DiD to state-level data on spending on science, space, and technology provides evidence of an effect on suicides by hanging, strangulation, and suffocation, not finding an effect on deaths by drowning after falling out of a canoe or kayak may provide limited assurance.


To illustrate, we now apply a kind of placebo test to evaluate the parallel trends assumption for bid-ask spreads, one of the variables considered in the DiD analysis of Table 2 of @Kelly:2012ul (we choose spreads in part because it is easy to calculate).


We first create the data set `spreads`, which contains data on the average spread for stocks over three-month periods—aligning with one measure used Table 2 of @Kelly:2012ul—for a sample period running from Q1, 2001 (the first quarter of 2001) to Q1, 2008, which is the sample period in @Kelly:2012ul. We will conduct a study of a pseudo-treatment that we will assume applies for periods beginning in Q1, 2004, which is roughly halfway through the sample period and we code `post` accordingly.


:::{.panel-tabset}

### PostgreSQL

```{r}
#| eval: false
db <- dbConnect(RPostgres::Postgres(), bigint = "integer")

dsf <- tbl(db, Id(schema = "crsp", table = "dsf"))

spreads <-
  dsf |>
  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),
         quarter = as.Date(floor_date(date, 'quarter'))) |>
  group_by(permno, quarter) |>
  summarize(spread = mean(spread, na.rm = TRUE), .groups = "drop") |>
  mutate(post = quarter >= "2004-01-01") |>
  filter(!is.na(spread), 
         between(quarter, "2000-01-01", "2008-01-01")) |>
  collect()

dbDisconnect(db)
```

### parquet

```{r}
#| eval: false
 
db <- dbConnect(duckdb::duckdb())

dsf <- load_parquet(db, schema = "crsp", table = "dsf")

spreads <-
  dsf |>
  mutate(spread = 100 * (ask - bid) / ((ask + bid) / 2),
         quarter = as.Date(floor_date(date, 'quarter'))) |>
  group_by(permno, quarter) |>
  summarize(spread = mean(spread, na.rm = TRUE), .groups = "drop") |>
  mutate(post = quarter >= "2004-01-01") |>
  filter(!is.na(spread), 
         between(quarter, "2000-01-01", "2008-01-01")) |>
  collect()

dbDisconnect(db)
```

:::

We now randomize treatment assignment. Because we want to evaluate the parallel trends assumption and completely randomized treatment assignment *implies* a trivial version of the parallel trends assumption, we specify a small difference in the probability of receiving treatment for observations whose pre-treatment spread exceeds the median (\(p = 0.55\)) from those whose pre-treatment spread is below the median (\(p = 0.45\)). This ensures that we have pre-treatment differences and thus need to rely on the parallel trends assumption in a meaningful way.[13](#fn13)


```{r}
#| eval: false

set.seed(2021)

treatment <-
  spreads |>
  filter(!post) |>
  group_by(permno) |>
  summarize(spread = mean(spread, na.rm = TRUE), .groups = "drop") |>
  mutate(treat_prob = if_else(spread > median(spread), 0.55, 0.45),
         rand = runif(n = nrow(pick(everything()))),
         treat = rand < treat_prob) |>
  select(permno, treat)
```

Obviously the null hypothesis of zero treatment effect holds with this “treatment”, but the question is whether the parallel trends assumption holds for `spread`. If we find evidence of a “treatment effect”, the only sensible interpretation is a failure of the parallel trends assumption for `spread`.


Merging in the `treatment` data set, we estimate a DiD regression (and cluster standard errors by `permno` for reasons that will be apparent after reading the discussion below). Results are reported in [Table 19.2](#tbl-placebo).




```{r}
#| eval: false
reg_data <-
  spreads |>
  inner_join(treatment, by = "permno") 

fm <- feols(spread ~ post * treat, vcov = ~ permno, data = reg_data)
```


```{r}
#| eval: false
modelsummary(fm,
             estimate = "{estimate}{stars}",
             gof_map = c("nobs"),
             stars = c('*' = .1, '**' = 0.05, '***' = .01))
```

Because we find a statistically significant effect of $-0.343$ with a t-statistic of $-4.96$ with this meaningless “treatment”, we can conclude with some confidence that the parallel trends assumption simply does not hold for `spread` in this sample. Given that we might have passed this placebo test even if the parallel trends assumption did not hold for a particular treatment, say due to endogenous selection, it seems reasonable to view this test as being better suited to detecting a failure of parallel trends (as it does here) than it is to validation of that assumption. 


Cunningham ([2021](references.html#ref-Cunningham:2021vk)) and Huntington-Klein ([2021](references.html#ref-Huntington-Klein:2021uc)) provide excellent pathways to a recent literature examining DiD. However, it is important to recognize that some variant of the scientifically flimsy parallel trends assumption imbues all of these treatments. It would seem to be productive for researchers to discard the “quasi-experimental” pretence attached to DiD and to apply techniques appropriate to what some call **interrupted time-series designs** (e.g., [Shadish et al., 2002](references.html#ref-Shadish:2002uv)).[14](#fn14)


While a randomized experiment provides a sound basis for attributing observed differences in outcomes to either treatment effects or sampling variation, without such randomization, it is perhaps more appropriate to take a more **abductive** approach of identifying causal mechanisms, deeper predictions about the timing and nature of causal effects, explicit consideration of alternative explanations, and the like ([Armstrong et al., 2022](references.html#ref-Armstrong:2021uo); [Heckman and Singer, 2017](references.html#ref-Heckman:2017uj)). Some evidence of this is seen in the discussion of specific papers in Cunningham ([2021](references.html#ref-Cunningham:2021vk)), perhaps reflecting reluctance to lean too heavily on the parallel trends assumption.


## Indirect effects of Reg SHO


The early studies of Reg SHO discussed above can be viewed as studying the more *direct* effects of Reg SHO. As a policy change directly affecting the ability of short-sellers to trade in securities, the outcomes studied by these earlier studies are more closely linked to the Reg SHO pilot than are the outcomes considered in later studies. Black et al. ([2019, pp. 2–3](references.html#ref-Black:2019vv)) point out that “despite little evidence of direct impact of the Reg SHO experiment on pilot firms, over 60 papers in accounting, finance, and economics report that suspension of the price tests had wide-ranging *indirect* effects on pilot firms, including on earnings management, investments, leverage, acquisitions, management compensation, workplace safety, and more.”


One indirect effect of short-selling that has been studied in subsequent research is that on earnings management. To explore this topic, we focus on Fang et al. ([2016, p. 1251](references.html#ref-Fang:2016uy)), who find “that short-selling, or its prospect, curbs earnings management.”


###  Earnings management after Dechow et al. ([1995](references.html#ref-Dechow:1995wr))


In [Chapter 16](earnings-mgt.html), we saw that early earnings management research used firm-specific regressions in estimating standard models such as the Jones ([1991](references.html#ref-Jones:1991vx)) model. Fang et al. ([2016](references.html#ref-Fang:2016uy)) apply subsequent innovations in measurement of earnings management, such the **performance-matched discretionary accruals** measure developed in @Kothari:2005aa .


@Kothari:2005aa  replace the firm-specific regressions seen in Dechow et al. ([1995](references.html#ref-Dechow:1995wr)) with regressions by industry-year, where industries are defined as firms grouped by two-digit SIC codes. @Kothari:2005aa  also add an intercept term to the Jones Model and estimate discretionary accruals under the Modified Jones Model by applying the coefficients from the Jones Model to the analogous terms of the Modified Jones Model.[15](#fn15)


To calculate performance-matched discretionary accruals, Kothari et al. ([2005, p. 1263](references.html#ref-Kothari:2005aa)) “match each sample firm with the firm from the same fiscal year-industry that has the closest return on assets as the given firm. The performance-matched discretionary accruals … are then calculated as the firm-specific discretionary accruals minus the discretionary accruals of the matched firm.” This is the primary measure of earnings management used by Fang et al. ([2016](references.html#ref-Fang:2016uy)), but note that Fang et al. ([2016](references.html#ref-Fang:2016uy)) use the 48-industry Fama-French groupings, rather than the two-digit SIC codes used in @Kothari:2005aa .


###  FHK: Data steps


To construct measures of discretionary accruals, Fang et al. ([2016](references.html#ref-Fang:2016uy)) obtain data primarily from Compustat, along with data on Fama-French industries from Ken French’s website and data on the SHO pilot indicator from the SEC’s website. The following code is adapted from [code posted by the authors](https://go.unimelb.edu.au/f7d8) of Fang et al. ([2016](references.html#ref-Fang:2016uy)), which was used to produce the results found in Tables 15 and 16 of Fang et al. ([2019](references.html#ref-Fang:2019tt)). The bulk of the Compustat data used in Fang et al. ([2016](references.html#ref-Fang:2016uy)) come from `comp.funda`. Following Fang et al. ([2019](references.html#ref-Fang:2019tt)), the code below collects data from that table for fiscal years between 1999 and 2012, excluding firms with SIC codes between 6000 and 6999 or between 4900 and 4949.


:::{.panel-tabset}

### PostgreSQL

```{r}
#| eval: false

db <- dbConnect(RPostgres::Postgres(), bigint = "integer")

funda <- tbl(db, Id(schema = "comp", table = "funda"))

compustat_annual <-
  funda |>
  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', 
         between(fyear, 1999, 2012),
         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |>
  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,
         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, 
         rect, ceq, csho, prcc_f) |>
  mutate(fyear = as.integer(fyear)) |>
  collect()

dbDisconnect(db)
```

### parquet

```{r}
#| eval: false

db <- dbConnect(duckdb::duckdb())

funda <- load_parquet(db, schema = "comp", table = "funda")

compustat_annual <-
  funda |>
  filter(indfmt == 'INDL', datafmt == 'STD', popsrc == 'D', consol == 'C', 
         between(fyear, 1999, 2012),
         !(between(sich, 6000, 6999) | between(sich, 4900, 4949))) |>
  select(gvkey, fyear, datadate, fyr, sich, dltt, dlc, seq,
         oibdp, ib, ibc, oancf, xidoc, at, ppegt, sale, 
         rect, ceq, csho, prcc_f) |>
  mutate(fyear = as.integer(fyear)) |>
  collect()

dbDisconnect(db)
```

:::


Some regressions in Fang et al. ([2019](references.html#ref-Fang:2019tt)) consider controls for market-to-book, leverage, and return on assets, which are calculated as `mtob`, `leverage`, and `roa`, respectively, in the following code:




```{r}
#| eval: false

controls_raw <-
  compustat_annual |>
  group_by(gvkey) |>
  arrange(fyear) |>
  mutate(lag_fyear = lag(fyear),
         mtob = if_else(lag(ceq) != 0, 
                        lag(csho) * lag(prcc_f) / lag(ceq), NA),
         leverage = if_else(dltt + dlc + seq != 0, 
                            (dltt + dlc) / (dltt + dlc + seq), NA),
         roa = if_else(lag(at) > 0, oibdp / lag(at), NA)) |>
  filter(fyear == lag(fyear) + 1) |>
  ungroup() |>
  select(gvkey, datadate, fyear, at, mtob, leverage, roa)
```


Following Fang et al. ([2019](references.html#ref-Fang:2019tt)), we create `controls_filled`, which uses `[fill()](https://tidyr.tidyverse.org/reference/fill.html)` to remove many missing values for the controls.




```{r}
#| eval: false

controls_filled <-
  controls_raw |>
  group_by(gvkey) |>
  arrange(fyear) |>
  fill(at, mtob, leverage, roa) |>
  ungroup()
```


Following Fang et al. ([2019](references.html#ref-Fang:2019tt)), we create `controls_fyear_avg`, which calculates averages of controls by fiscal year.




```{r}
#| eval: false

controls_fyear_avg <-
  controls_filled |>
  group_by(fyear) |>
  summarize(across(c(at, mtob, leverage, roa), 
                   \(x) mean(x, na.rm = TRUE)))
```


Like Fang et al. ([2019](references.html#ref-Fang:2019tt)), we use values from `controls_fyear_avg` to replace missing values for controls.




```{r}
#| eval: false
df_controls <-
  controls_filled |>
  inner_join(controls_fyear_avg, by = "fyear", suffix = c("", "_avg")) |>
  mutate(at = coalesce(at, at_avg),
         mtob = coalesce(mtob, mtob_avg),
         leverage = coalesce(leverage, leverage_avg),
         roa = coalesce(roa, roa_avg)) |>
  select(gvkey, fyear, at, mtob, leverage, roa)
```

There are multiple steps in the code above and the reasons for the steps involved in calculating `controls_filled` from `controls_raw` and `df_controls` from `controls_filled` are explored in the exercises below.


As discussed above, FHK estimate discretionary-accrual models by industry and year, where industries are based on the Fama-French 48-industry grouping. To create these industries here, we use `[get\_ff\_ind()](https://rdrr.io/pkg/farr/man/get_ff_ind.html)`, introduced in [Chapter 9](web-data.html) and provided by the `farr` package.


```{r}
#| eval: false
ff_data <- get_ff_ind(48)
```


We now create functions to compile the data we need to estimate performance-matched discretionary accruals. We use a function to compile the data because (i) doing so is easy in R and (ii) it allows us to re-use code much easily, which will be important for completing the exercises in this chapter.


The first function we create is `get_das()`, which takes as its first argument (`compustat`) a data set derived from Compustat with the requisite data. The second argument (`drop_extreme`) allows to easily tweak the handling of outliers in a way examined in the exercises.


Within `get_das()`, the first data set we compile is `for_disc_accruals`, which contains the raw data for estimating discretionary-accruals models. Following FHK, we require each industry-year to have at least 10 observations for inclusion in our analysis and impose additional data requirements, some of which we explore in the exercises below.


Following FHK, we estimate discretionary-accrual models by industry and year and store the results in the data frame `fm_da`. We then merge the underlying data (`for_disc_accruals`) with `fm_da` to use the estimated models to calculate non-discretionary accruals (`nda`). Because the coefficient on `sale_c_at` is applied to `salerect_c_at`, we cannot use `[predict()](https://rdrr.io/r/stats/predict.html)` or `[residuals()](https://rdrr.io/r/stats/residuals.html)` in a straightforward fashion and need calculate `nda` “by hand”. We then calculate discretionary accruals (`da = acc_at - nda`) and return the data.


```{r}
#| eval: false
get_das <- function(compustat, drop_extreme = TRUE) {
  
  for_disc_accruals <-
    compustat |>
    inner_join(ff_data, 
               join_by(between(sich, sic_min, sic_max))) |>
    group_by(gvkey, fyr) |>
    arrange(fyear) |>
    filter(lag(at) > 0) |>
    mutate(lag_fyear = lag(fyear),
           acc_at = (ibc - (oancf - xidoc)) / lag(at),
           one_at = 1 / lag(at),
           ppe_at = ppegt / lag(at),
           sale_c_at = (sale - lag(sale)) / lag(at),
           salerect_c_at = ((sale - lag(sale)) - 
                              (rect - lag(rect))) / lag(at)) |>
    ungroup() |>
    mutate(keep = case_when(drop_extreme ~ abs(acc_at) <= 1,
                            .default = TRUE)) |>
    filter(lag_fyear == fyear - 1,
           keep, 
           !is.na(salerect_c_at), !is.na(acc_at), !is.na(ppe_at)) |>
    group_by(ff_ind, fyear) |>
    mutate(num_obs = n(), .groups = "drop") |>
    filter(num_obs >= 10) |>
    ungroup()
  
  fm_da <-
    for_disc_accruals |>
    group_by(ff_ind, fyear) |>
    do(model = tidy(lm(acc_at ~ one_at + sale_c_at + ppe_at, data = .))) |>
    unnest(model) |>
    select(ff_ind, fyear, term, estimate) |>
    pivot_wider(names_from = "term", values_from = "estimate", 
                names_prefix = "b_")
  
  for_disc_accruals |>
    left_join(fm_da, by = c("ff_ind", "fyear")) |>
    mutate(nda = `b_(Intercept)` + one_at * b_one_at + ppe_at * b_ppe_at + 
                   salerect_c_at * b_sale_c_at,
           da = acc_at - nda) |>
    select(gvkey, fyear, ff_ind, acc_at, da) 
}
```


The next step in the data preparation process is to match each firm with another based on performance. Following FHK, we calculate performance as lagged “Income Before Extraordinary Items” (`ib`) divided by lagged “Total Assets” (`at`) and `perf_diff`, the absolute difference between performance for each firm-year and each other firm-year in the same industry. We then select the firm (`gvkey_other`) with the smallest value of `perf_diff`. We rename the variable containing the discretionary accruals of the matching firm as `da_other` and calculate performance-matched discretionary accruals (`da_adj`) as the difference between discretionary accruals for the target firm (`da`) and discretionary accruals for the matched firm (`da_other`), and return the results. Note that `get_pm()` includes the argument `pm_lag` with default value `TRUE`. If `pm_lag` is set to `FALSE`, then performance for matching is calculated using contemporary values of `ib` and `at` (this option is examined in the exercises below).




```{r}
#| eval: false
get_pm <- function(compustat, das, pm_lag = TRUE, drop_extreme = TRUE) {
  
  das <- get_das(compustat, drop_extreme = drop_extreme)
  
  perf <-
    compustat |>
    group_by(gvkey) |>
    arrange(fyear) |>
    mutate(ib_at = 
      case_when(pm_lag ~ if_else(lag(at) > 0, lag(ib) / lag(at), NA),
                .default = if_else(at > 0, ib / at, NA))) |>
    ungroup() |>
    inner_join(das, by = c("gvkey", "fyear")) |>
    select(gvkey, fyear, ff_ind, ib_at)
  
  perf_match <-
    perf |>
    inner_join(perf, by = c("fyear", "ff_ind"),
               suffix = c("", "_other")) |>
    filter(gvkey != gvkey_other) |>
    mutate(perf_diff = abs(ib_at - ib_at_other)) |>
    group_by(gvkey, fyear) |>
    filter(perf_diff == min(perf_diff)) |>
    select(gvkey, fyear, gvkey_other)
  
  perf_matched_accruals <- 
    das |>
    rename(gvkey_other = gvkey,
           da_other = da) |>
    select(fyear, gvkey_other, da_other) |>
    inner_join(perf_match, by = c("fyear", "gvkey_other")) |>
    select(gvkey, fyear, gvkey_other, da_other)
  
  das |>
    inner_join(perf_matched_accruals, by = c("gvkey", "fyear")) |>
    mutate(da_adj = da - da_other) |>
    select(gvkey, fyear, acc_at, da, da_adj, da_other, gvkey_other)
}
```


The final step is performed in `get_pmdas()`. This function gets the needed data using `get_pm()`, then filters duplicate observations based on `(gvkey, fyear)` (the rationale for this step is explored in the discussion questions).




```{r}
#| eval: false
get_pmdas <- function(compustat, pm_lag = TRUE, drop_extreme = TRUE) {
  
  get_pm(compustat, 
         pm_lag = pm_lag,
         drop_extreme = drop_extreme) |>
    group_by(gvkey, fyear) |>
    filter(row_number() == 1) |>
    ungroup() 
}
```

Finally, we simply pass the data set `compustat_annual` to `get_pmdas()` and store the result in `pmdas`.

```{r}
#| eval: false
pmdas <- get_pmdas(compustat_annual)
```

The remaining data set used by FHK is `sho_data`, which we discussed in [Section 19.2.1](#sec-sho-pilot).




```{r}
#| eval: false

sho_data <- 
  fhk_pilot |>
  select(gvkey, pilot) |>
  distinct() |>
  group_by(gvkey) |>
  filter(n() == 1) |>
  ungroup() |>
  inner_join(fhk_pilot, by = c("gvkey", "pilot")) 
```


The final sample `sho_accruals` is created in the following code and involves a number of steps. We first merge data from FHK’s `sho_data` with `fhk_firm_years` to produce the sample firm-years and treatment indicator for FHK. As `fhk_firm_years` can contain multiple years for each firm, so we expect each row in `sho_data` to match multiple rows in `fhk_firm_years`. At the same time, some `gvkey` values link with multiple PERMNOs, so some rows in `fhk_firm_years` will match multiple rows in `sho_data`. As such, we set `relationship = "many-to-many"` in this join below. We then merge the resulting data set with `df_controls`, which contains data on controls. The final data merge brings in data on performance-matched discretionary accruals from `pm_disc_accruals_sorted`. Finally, following FHK, we winsorize certain variables using the `[winsorize()](https://rdrr.io/pkg/farr/man/winsorize.html)` function from the `farr` package to do this here.[16](#fn16) 

```{r}
#| eval: false
win_vars <- c("at", "mtob", "leverage", "roa", "da_adj", "acc_at")

sho_accruals <-
  sho_data |>
  inner_join(fhk_firm_years, 
             by = "gvkey",
             relationship = "many-to-many") |>
  select(gvkey, datadate, pilot) |>
  mutate(fyear = year(datadate) - (month(datadate) <= 5)) |>
  left_join(df_controls, by = c("gvkey", "fyear")) |>
  left_join(pmdas, by = c("gvkey", "fyear")) |>
  group_by(fyear) |>
  mutate(across(all_of(win_vars),
                \(x) winsorize(x, prob = 0.01))) |>
  ungroup()
```


###  Discussion questions and exercises


1. What would be the effect of replacing the code that creates `ff_data` above with the following code? What changes would we need to make to the code creating `for_disc_accruals` in `get_das()` to use this modified version of `ff_data`?

```{r}
#| eval: false
ff_data <- 
  get_ff_ind(48) |>
  rowwise() |>
  mutate(sich = list(seq(from = sic_min, to = sic_max))) |> 
  unnest(sich)
```


2. What issue is `filter(row_number() == 1)` addressing in the code above? What assumptions are implicit in this approach? Do these assumptions hold in this case? What would be an alternative approach to address the issue?


3. Why is `filter(fyear == lag(fyear) + 1)` required in the creation of `controls_raw`?


4. Does the argument for using `salerect_c_at * b_sale_c_at` in creating non-discretionary accruals make sense to you? How do @Kothari:2005aa  explain this?


5. Does the code above ensure that a performance-matched control firm is used as a control just once? If so, which aspect of the code ensures this is true? If not, how might you ensure this and does this cause problems? (Just describe the approach in general; no need to do this.)


6. What are FHK doing in the creation of `controls_filled`? (Hint: The key “verb” is `[fill()](https://tidyr.tidyverse.org/reference/fill.html)`.) Does this seem appropriate? Does doing this make a difference?


7. What are FHK doing in the creation of `df_controls` from `controls_fyear`? Does this seem appropriate? Does doing this make a difference?


###  FHK: Regression analysis


FHK consider a number of regression specifications including: with and without controls, with and without firm fixed effects, and with standard errors clustered by firm alone and clustered by firm and year. We make a small function (`reg_year_fe()`) that calculates variables used in the regression (like `during` and `post`) and allows us to specify each of these different options, to change the dependent variable from the default (`dv = "da_adj"`) and to supply a different data set. This function returns a fitted model that is estimated using `[feols()](https://lrberge.github.io/fixest/reference/feols.html)` from the `fixest` package. 




```{r}
#| eval: false
ctrls_list <- c("log(at)", "mtob", "roa", "leverage")

reg_year_fe <- function(df, dv = "da_adj",
                        controls = TRUE, firm_fe = FALSE, cl_2 = TRUE,
                        vcov = NULL) {
  df <- 
    df |>
    mutate(year = year(datadate),
           during = year %in% c(2005, 2006, 2007),
           post = year %in% c(2008, 2009, 2010))
  
  model <- str_c(dv, " ~ pilot * (during + post) ",
                 if_else(controls, 
                         str_c(" + ", str_c(ctrls_list, 
                                            collapse = " + ")), ""),
                    if_else(firm_fe, "| gvkey + year ", "| year "))
  if (is.null(vcov)) {
    vcov = as.formula(if_else(!cl_2, "~ gvkey ", "~ year + gvkey"))
  }
  
  feols(as.formula(model), 
        vcov = vcov,
        notes = FALSE,
        data = df)
}
```


To facilitate the output of variations, we next make a function that runs regressions with and without controls and with and without firm fixed effects and returns a nicely formatted regression table.


```{r}
#| eval: false

make_reg_table <- function(df, dv = "da_adj", cl_2 = TRUE) {
  omit <- str_c("^(", str_c(str_replace_all(c("during", "post", ctrls_list),
                                            "[()]", "."), 
                            collapse="|"), ")")
  
  run_reg <- function(controls, firm_fe) {
    reg_year_fe(df, dv = dv, controls = controls, firm_fe = firm_fe,
                cl_2 = cl_2)
  }
  
  params <- tibble(controls = c(FALSE, TRUE, FALSE, TRUE),
                   firm_fe = c(FALSE, FALSE, TRUE, TRUE))
  
  fms <- pmap(params, run_reg)
  
  notes <- tribble(~term,  ~`1`,  ~`2`, ~`3`, ~`4`,
                   "Firm FEs", "No", "No", "Yes", "Yes",
                   "Controls", "No", "Yes", "No", "Yes")
  
  modelsummary(fms,
               estimate = "{estimate}{stars}",
               gof_map = "nobs",
               stars = c('*' = .1, '**' = 0.05, '***' = .01),
               coef_omit = str_c(str_replace_all(ctrls_list, "[()]", "."),
                                 collapse = "|"),
               add_rows = notes)
}
```


We now use this function with our version of FHK’s data set (`sho_accruals`) to create the regression results reported in [Table 19.3](#tbl-baseline).

```{r}
#| eval: false
make_reg_table(sho_accruals)
```




We next create a function that allows us to plot by-year coefficients for the treatment and control firms. (We leave the details of what this function is doing as an exercise for the reader below.)

```{r}
#| eval: false
plot_coefficients <- function(model) {
  tibble(name = names(model$coefficients),
         value = as.vector(model$coefficients)) |>
  filter(str_detect(name, "^year.")) |>
  separate(name, into = c("year", "pilot"), sep = ":", fill = "right") |>
  mutate(year = as.integer(str_replace(year, "^year", "")),
         pilot = coalesce(pilot == "pilotTRUE", FALSE)) |>
  ggplot(aes(x = year, y = value, 
             linetype = pilot, color = pilot)) +
  geom_line() +
  scale_x_continuous(breaks = 2000:2012L) +
  geom_rect(xmin = 2005, xmax = 2007, ymin = -Inf, ymax = Inf,
              color = NA, alpha = 0.01) +
  theme_bw()
}
```

To produce [Figure 19.3](#fig-coef-plot), we estimate one of the models above by year and feed the fitted model to `plot_coefficients()`.

```{r}
#| eval: false

sho_accruals |>
  mutate(year = as.factor(year(datadate))) |>
  feols(da_adj ~ year * pilot - pilot - 1 + log(at) + mtob + roa + leverage,
        vcov = ~ year + gvkey, data = _) |>
  plot_coefficients()
```

###  Exercises


1. In words, how does `sho_accruals_alt` (defined below) differ from `sho_accruals`? Does using `sho_accruals_alt` in place of `sho_accruals` affect the regression results?

```{r}
#| eval: false
firm_years <-
  controls_raw |>
  select(gvkey, datadate, fyear)

sho_accruals_alt <-
  sho_r3000_gvkeys |>
  inner_join(firm_years, by = "gvkey") |>
  left_join(df_controls, by = c("gvkey", "fyear")) |>
  left_join(pmdas, by = c("gvkey", "fyear")) |>
  group_by(fyear) |>
  mutate(across(all_of(win_vars), winsorize, prob = 0.01)) |>
  ungroup()
```

2. In an online appendix, BDLYY say “FHK winsorize covariates for their covariate balance table at 1/99%. We inferred that they also winsorized accruals at this level. Whether they winsorize across sample years or within each year, they do not specify.” The code above winsorized within each year. How would you modify the code to winsorize “across sample years”? Does doing so make a difference?


3. How would you modify the code to winsorize at the 2%/98% level? Does this make a difference to the results? (*Hint*: With the `farr` package loaded, type `[? winsorize](https://rdrr.io/pkg/farr/man/winsorize.html)` in the R console to get help on this function.)


4. How would you modify the code to not winsorize at all? Does this make a difference to the results?


5. Some of the studies discussed by BDLYY exclude 2004 data from the sample. How would you modify the code above to do this here? Does excluding 2004 here make a significant difference?


6. What is the range of values for `year` in `sho_accruals`? Does this suggest any issues with the code `post = year %in% c(2008, 2009, 2010)` above? If so, does fixing any issue have an impact on the results reported above?


7. Would it make sense, in creating `perf` above, if we instead calculated `ib_at` as `if_else(at > 0, ib / at, NA))`? What is the effect on the regression results if we use this modified calculation of `ib_at`? What do @Kothari:2005aa  recommend on this point? (*Hint*: Use `pm_lag = FALSE` where applicable.)


8. Fang et al. ([2019, p. 10](references.html#ref-Fang:2019tt)) follow Fang et al. ([2016](references.html#ref-Fang:2016uy)), who “exclude observations for which the absolute value of total accruals-to-total assets exceeds one. This is a standard practice in the accounting literature because firms with such high total accruals-to-total assets are often viewed as extreme outliers. Nonetheless, the FHK results are robust to winsorizing the accrual measures at the 1% and 99% levels instead of excluding extreme outliers.” Does this claim hold up in the reproduction above? What happens if the `[filter()](https://dplyr.tidyverse.org/reference/filter.html)` on `abs(acc_at) <= 1` is removed from the code above? (*Hint*: Use `drop_extreme = FALSE` where applicable.)


9. Explain what each line of the function `plot_coefficients()` before the line starting with `[ggplot()](https://ggplot2.tidyverse.org/reference/ggplot.html)` is doing. (*Hint*: It may be helpful to store the model that is fed to the function above in the variable `model` and then run the function line by line.)


<!-- ## Statistical inference -->
## 統計的推論


<!-- One point of difference between FHK and BDLYY concerns clustered standard errors.  -->
FHKとBDLYYの間の違いの一つは、クラスター標準誤差に関することである。
<!-- Fang et al. ([2016](references.html#ref-Fang:2016uy)) generally use “standard errors clustered by year and firm” ([2016, p.1269](references.html#ref-Fang:2016uy)), while Black et al. ([2019](references.html#ref-Black:2019vv)) advocate the use of standard errors clustered by firm.  -->
Fang et al. ([2016](references.html#ref-Fang:2016uy))は一般的に「年と企業でクラスタリングされた標準誤差」を使用しているが、Black et al. ([2019](references.html#ref-Black:2019vv))は企業でクラスタリングされた標準誤差の使用を提唱している。
<!-- Citing Cameron et al. ([2008](references.html#ref-Cameron:2008ws)), Black et al. ([2019, p.30](references.html#ref-Black:2019vv)) suggest that “clustered standard errors with a small number of clusters can be downward biased.”  -->
Cameron et al. ([2008](references.html#ref-Cameron:2008ws))を引用して、Black et al. ([2019, p.30](references.html#ref-Black:2019vv))は「クラスタリングされた標準誤差は、クラスター数が少ない場合には下方バイアスになる可能性がある」と述べている。
<!-- In the context of FHK, there are thousands of firms, but a relatively small number of years, so clustering by year (or firm *and* year) may result in problematic standard error estimates (see [Section 5.6.6](stat-inf.html#sec-se-calcs)). -->
FHKの文脈では数千の企業が存在しているが、比較的少数の年数であり、年でクラスタリング（または企業*および*年度でクラスタリング）を行うと、問題のある標準誤差の推定値が得られる可能性があります（[Section 5.6.6](stat-inf.html#sec-se-calcs)を参照）。


<!-- One approach to determining the appropriate clustering is more empirical.  -->
適切なクラスタリングを決定するためのアプローチの一つは、より経験的なものである。
<!-- In this regard, it is useful to note that cluster-robust standard errors are a generalization of an idea from White ([1980](references.html#ref-White:1980aa)).  -->
この点で、クラスター誤差の標準誤差は、White ([1980](references.html#ref-White:1980aa))のアイデアの一般化であることを指摘しておくと、有用である。
White ([1980](references.html#ref-White:1980aa)) provides not only an estimator of standard errors that is robust to heteroskedasticity, but also a test of a null hypothesis of homoskedasticity. 
Intuitively, if the covariance matrix assuming heteroskedasticity is sufficiently different from that assuming homoskedasticity, then we may reject the null hypothesis of homoskedasticity. 
With a little algebra, it would be possible to develop a test analogous to that of White ([1980](references.html#ref-White:1980aa)) of the null hypothesis of no clustering on variable \(g\). 
In practice, many researchers will, lacking a formally derived test, compare standard errors with and without clustering on variable \(g\) and elect to cluster on variable \(g\) when the standard errors when doing so seem significantly higher than when not doing so. 
This heuristic breaks down in the case of Fang et al. ([2016](references.html#ref-Fang:2016uy)) because standard errors are generally lower when clustering on firm and year than when clustering firm alone. 
However, if clustering on firm alone is appropriate, standard errors clustering on firm and year will provide noisier estimates than clustering on firm alone, and thus could be lower or higher in any given data set.


A more theoretical approach can be used in the setting of FHK because of our deeper understanding of the assignment mechanism. In this regard, it is important to note that cluster-robust standard errors address correlation in *both* \(X\) *and* \(\epsilon\) across units within clusters. To explore this (slightly) more formally, recall from [Chapter 5](stat-inf.html) that the cluster-robust covariance matrix is estimated using the following expression:


$$ 
\begin{aligned}
\hat{V}(\hat{\beta}) = 
(\boldsymbol{X}'\boldsymbol{X})^{-1} \hat{\boldsymbol{B}} (\boldsymbol{X}'\boldsymbol{X})^{-1}, \quad  \text{where} \quad  \hat{B} = \sum_{g = 1}^G \boldsymbol{X}'_g u_g u'_g \boldsymbol{X}_g
\end{aligned}
$$


where the observations grouped into $G$ clusters of $N_g$ observations for $g$ in ${1, \dots, G}$ , $X_g$ is the $N_g \times K$ matrix of regressors, and $u_g$ is the $N_g$ -vector of residuals for cluster $g$ .


If we have a single regressor, demeaned $x$ with no constant term and two firms ( $i$ and $j$ ) in a cluster, then the contribution of that cluster to $\hat{B}$ will be


$$
\begin{aligned}
\begin{bmatrix}
x_i & x_j
\end{bmatrix}
\begin{bmatrix}
u_i \\
u_j
\end{bmatrix}
\begin{bmatrix}
u_i & u_j
\end{bmatrix}
\begin{bmatrix}
x_i \\
x_j
\end{bmatrix} &=
\begin{bmatrix}
x_i & x_j
\end{bmatrix}
\begin{bmatrix}
u_i^2 & u_i u_j \\
u_i u_j & u_j^2
\end{bmatrix}
\begin{bmatrix}
x_i \\
x_j
\end{bmatrix} \\
&=
\begin{bmatrix}
x_i & x_j
\end{bmatrix}
\begin{bmatrix}
x_i u_i^2 + x_j u_i u_j \\
x_i u_i u_j + x_j u_j^2
\end{bmatrix} \\
&=
\begin{bmatrix}
x_i^2 u_i^2 + x_i x_j u_i u_j \\
x_i x_j u_i u_j + x_j^2 u_j^2
\end{bmatrix}
\end{aligned}
$$


Now, if $x_i$ and $x_j$ are uncorrelated then, even if $\epsilon_i$ and $\epsilon_j$ are correlated, this resolves in expectation to


$$
\begin{bmatrix}
x_i^2 \sigma _i^2 \\
x_j^2 \sigma _j^2
\end{bmatrix}
$$


<!-- which is the expectation of the analogous component of the heteroskedasticity-robust estimator from White ([1980](references.html#ref-White:1980aa)).  -->
これは、White ([1980](references.html#ref-White:1980aa))の不均一分散頑健推定量の類似の成分の期待値である。
<!-- In the setting of Fang et al. ([2016](references.html#ref-Fang:2016uy)), the “\(x\)” of primary interest is the Reg SHO pilot indicator, which is assumed to be randomly assigned, and thus (in expectation) uncorrelated across firms.  -->
Fang et al. ([2016](references.html#ref-Fang:2016uy))の設定では、主要な関心事である「\(x\)」は、ランダムに割り当てられると仮定されているReg SHOパイロット指標であり、したがって（期待値では）企業間で相関がない。
<!-- For this reason, we do not expect cross-sectional dependence to affect standard error estimates on average.  -->
このため、クロスセクション依存性が平均的に標準誤差推定値に影響を与えることはないと予想される。
<!-- On the other hand, the Reg SHO pilot indicator is perfectly correlated over time within firm, so any serial dependence in errors within firm over time will lead to effects of time-series dependence on standard error estimates.  -->
一方、Reg SHOパイロット指標は、企業内で時間経過とともに完全に相関しているため、時間経過にわたる企業内の誤差の直列依存性が標準誤差推定値に時間系列依存性の影響を与える。
<!-- This (somewhat loose) theoretical analysis suggests we should cluster by firm (time-series dependence), but not by year (cross-sectional dependence), as suggested by Black et al. ([2019, p. 12](references.html#ref-Black:2019vv)). -->
この（やや緩い）理論的分析は、Black et al. ([2019, p. 12](references.html#ref-Black:2019vv))が示唆するように、企業（時間系列依存性）でクラスタリングするべきであるが、年でクラスタリングするべきではないことを示唆している。


<!-- However, the assumed random assignment of treatment allows us to adopt an alternative approach to statistical inference that is agnostic to the form of clustering in the data.  -->
しかし、仮定されたランダムな処置の割り当てにより、データのクラスタリングの形式に無関心な統計推論の代替アプローチを採用することができる。
<!-- This approach is known as **randomization inference** and builds on the **Fisher sharp null** hypothesis of no effect of any kind.  -->
このアプローチは**ランダム化推論**(randomization inference)と呼ばれ、いかなる影響も存在しないというフィッシャーの**強い帰無仮説**(Fisher sharp null hypothesis)に基づく。
<!-- This is a “sharp null” because it is more restrictive that a null hypothesis of zero mean effect, which could be true even if half the observations had a treatment effect of +1 and half the observations had a treatment effect of -1, in which case the Fisher sharp null would not be true even though null hypothesis of zero mean effect is true. -->
これは「シャープの強い帰無仮説」と呼ばれるのは、ゼロ平均効果の帰無仮説よりも制約が厳しいためである。たとえば、観測の半数が $+1$ の処置効果を持ち、残りの半数が $-1$ の処置効果を持つ場合、ゼロ平均効果の帰無仮説は成立するが、フィッシャーの強い帰無仮説は成立しない。


<!-- Under the Fisher sharp null hypothesis and with random assignment to treatment, in principle we can evaluate the distribution of any given test statistic by considering all possible assignments.  -->
フィッシャーの強い帰無仮説の下で、処置へのランダムな割り当てが行われた場合、原則として、すべての可能な割り当てを考慮することで、任意の検定統計量の分布を評価することができる。
<!-- Focusing on the 2954 firms that the SEC focused on as its initial sample, if assignment to treatment were purely random, then any other assignment of treatment to 985 was as likely as the one chosen.  -->
初期サンプルとしてSECが焦点を当てた2954社に焦点を当てると、処置の割り当てが純粋にランダムであれば、985社への処置の他の割り当ては、選択された割り当てと同じくらい可能性があった。
<!-- Given that the Fisher sharp null implies that there was no impact of treatment assignment on outcomes, we know what the distribution of the test statistic would have been if the SEC had chosen any one of those alternative assignments because the outcomes would have been exactly the same.  -->
フィッシャーの強い帰無仮説が処置の割り当てが結果に影響を与えなかったことを意味するので、SECが他の割り当てのいずれかを選択した場合、テスト統計量の分布がどのようになるかを知っている。なぜなら、結果はまったく同じだったからである。
<!-- With smaller samples, we might proceed to calculate the test statistic for every possible assignment and thereby construct the exact distribution of the test statistic under the Fisher sharp null.[17] -->
サンプルが小さい場合、すべての可能な割り当てに対して検定統計量を計算し、フィッシャーの強い帰無仮説の下での検定統計量の正確な分布を構築することができる。[^17]
<!-- But in our case, there will be a huge number of ways to choose 985 treatment firms from 2954 possibilities; so a more feasible approach is to draw a random sample of possible assignments and use the empirical distribution of the test statistic for that random sample as an approximation for the exact distribution. -->
しかし、我々の場合、2954の可能性から985の処置企業を選ぶ方法は膨大であるため、可能な割り当てのランダムサンプルを抽出し、そのランダムサンプルの検定統計量の経験分布を正確な分布の近似として使用することがより現実的である。



```{r}
#| eval: false

get_coef_rand <- function(i) {
  treatment <-
    sho_accruals |>
    select(gvkey, pilot) |>
    distinct() |>
    mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE))
  
  reg_data_alt <-
    sho_accruals |>
    select(-pilot) |>
    inner_join(treatment, by = "gvkey")
  
  reg_data_alt |> 
    reg_year_fe(controls = TRUE, firm_fe = TRUE) |> 
    broom::tidy() |> 
    select(term, estimate) |>
    pivot_wider(names_from = "term", values_from = "estimate") |>
    mutate(iteration = i) |>
    suppressWarnings()
}
```


<!-- The test statistic we are interested in here is the coefficient on $PILOT \times DURING$ . -->
ここで興味がある検定統計量は、$PILOT \times DURING$ の係数である。
<!-- Below we calculate the p-value of the coefficients on variables involving $PILOT$ using the empirical distribution of coefficients, and the standard errors associated with the coefficients as the standard deviation of those coefficients. -->
以下では、$PILOT$ に関する変数の係数について、係数の経験的分布を用いてp値を計算し、係数に対応する標準誤差をそれらの係数の標準偏差として求める。


```{r}
#| eval: false
set.seed(2021)
rand_results <-
  1:1000 |>
  map(get_coef_rand) |> 
  list_rbind() |>
  system_time()
```


```{r}
#| eval: false
plan(multisession)

rand_results <- 
  1:1000 |> 
  future_map(get_coef_rand, 
             .options = furrr_options(seed = 2021)) |> 
  list_rbind() |>
  system_time()
```



<!-- In the following, we run regressions with standard errors based on clustering by firm and year, by firm alone, and using randomization inference.  -->
以下では、企業と年によるクラスタリング、企業のみによるクラスタリング、およびランダム化推論を用いた標準誤差を基に回帰を実行する。
<!-- We start by running regressions—with controls and firm fixed effects—with standard errors based on clustering by firm (`"CL-i"`) and by firm and year (`"CL-2"`). -->
最初に、企業によるクラスタリング（`"CL-i"`）と企業と年によるクラスタリング（`"CL-2"`）に基づいた標準誤差を用いて、コントロールと企業固定効果を持つ回帰を実行する。

```{r}
#| eval: false
fms <- list(reg_year_fe(sho_accruals, cl_2 = FALSE),
            reg_year_fe(sho_accruals, cl_2 = TRUE))
```

<!-- We extract the variance-covariance matrices for each of these two models and place them in the list `vcovs`. -->
これら2つのモデルの分散共分散行列を抽出し、それらをリスト`vcovs`に配置する。

```{r}
#| eval: false
vcovs <- list(vcov(fms[[1]]), vcov(fms[[2]]))
```


<!-- Next, we add a third model for which we will calculate standard errors using randomization inference (`"RI"`).  -->
次に、ランダム化推論（`"RI"`）を用いて標準誤差を計算するための第3のモデルを追加する。
<!-- The coefficients stored in `fms` for this third model can be taken from either of the two models already stored there. -->
この第3のモデルの`fms`に格納された係数は、すでに格納されている2つのモデルのどちらかから取得できる。

```{r}
#| eval: false
fms[[3]] <- fms[[2]]
```

<!-- For the variance-covariance matrix, we use `CL-i` standard errors as the starting point. -->
分散共分散行列については、`CL-i`標準誤差を出発点として使用する。
<!-- Then we replace the elements for coefficients on variables involving $PILOT$ using the empirical distribution stored in `rand_results`. -->
`rand_results`に格納された経験的分布を用いて、$PILOT$ に関する変数の係数の要素を置き換える。

```{r}
#| eval: false

vcov <- vcovs[[1]]
vcov["pilotTRUE:duringTRUE", "pilotTRUE:duringTRUE"] <-
  var(rand_results[["pilotTRUE:duringTRUE"]])
vcov["pilotTRUE:postTRUE", "pilotTRUE:postTRUE"] <- 
  var(rand_results[["pilotTRUE:postTRUE"]])
vcovs[[3]] <- vcov
```


<!-- Results of this analysis are provided in [Table 19.4](#tbl-rand-inf). -->
この分析の結果は、[Table 19.4](#tbl-rand-inf)に示されている。

```{r}
#| eval: false
se_notes <- tribble(~term,  ~`1`,  ~`2`, ~`3`,
                    "SEs", "CL-i", "CL-2", "RI")

modelsummary(fms, vcov = vcovs, 
             estimate = "{estimate}{stars}",
             gof_map = "nobs",
             stars = c('*' = .1, '**' = 0.05, '***' = .01),
             coef_omit = "^(during|post|pilot)TRUE$",
             add_rows = se_notes)
```


<!-- ### Exercises -->
### 練習問題

<!-- 1. In the function `get_coef_rand()`, we first created the data set `treatment`, then merged this with `reg_data_alt`.  -->
1. `get_coef_rand()`関数では、まずデータセット`treatment`を作成し、次にこれを`reg_data_alt`とマージする。
<!-- Why did we do it this way rather than simply applying the line `mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE))` directly to `reg_data_alt`? -->
なぜ、`reg_data_alt`に直接`mutate(pilot = sample(pilot, size = length(pilot), replace = FALSE))`を適用するのではなく、このように行ったのか。


<!-- 2. Using randomization inference, calculate a p-value for a one-sided alternative hypothesis that $H_1: \beta < 0$ where $\beta$ is the coefficient on $PILOT \times DURING$ .  -->
2. ランダム化推論を使用して、$H_1: \beta < 0$（ここで、 $\beta$ は $PILOT \times DURING$ の係数である）の片側の代替仮説の $p$ 値を計算する。
<!-- (*Hint*: You should not need to run the randomization again; modifying the calculation of `p_value` should suffice.) -->
（*ヒント*：ランダム化を再実行する必要はありません。`p_value`の計算を変更するだけで十分である。）


<!-- 3. What is the empirical standard error implied by the distribution of coefficients in `rand_results`?  -->
3. `rand_results`の係数の分布によって示される経験的標準誤差は何か？
<!-- Is it closer to the two-way cluster robust standard errors obtained in estimating with `cl_2 = TRUE` or with `cl_2 = FALSE`?  -->
`cl_2 = TRUE` または `cl_2 = FALSE` で推定した2方向クラスター頑健標準誤差により近いか？
<!-- Why might it be preferable to calculate p-values under randomization inference using the empirical distribution of the test statistic, instead of calculating these from t-statistics based on the estimated coefficient and the empirical standard error?  -->
推定された係数と経験的標準誤差に基づいたt統計量からp値を計算する代わりに、ランダム化推論を用いて検定統計量の経験的分布を用いてp値を計算することが望ましい理由は何か？
<!-- Would we get different p-values using the former approach? -->
前者のアプローチを使用して異なるp値を得ることができるか？


<!-- 4. Why did we not use the empirical standard error implied by the distribution of coefficients in `rand_results` to calculate standard errors for the control variables (e.g., `log(at)`)? -->
4. `rand_results`の係数の分布によって示される経験的標準誤差を使用して、制御変数（例：`log(at)`）の標準誤差を計算しなかった理由は何か？


<!-- ## Causal diagrams -->
## 因果ダイアグラム


<!-- It is important to note that we *observe* total accruals, not discretionary accruals.  -->
我々は、裁量的アクルーアルではなく、総アクルーアルを*観測*していることに注意することが重要である。
<!-- Instead we need to construct *measures* of discretionary accruals.  -->
代わりに、裁量的アクルーアルの*尺度*を構築する必要がある。
<!-- The Jones ([1991](references.html#ref-Jones:1991vx)) model of discretionary accruals “controls for” sales growth and PP&E and the @Kothari:2005aa  model additionally “controls for” performance. -->
裁量的アクルーアルのJones ([1991](references.html#ref-Jones:1991vx))モデルは、売上高成長とPP&Eを「制御」し、@Kothari:2005aa モデルはさらにパフォーマンスを「コントロール」している。


<!-- Assuming that the causal diagram below is correct, we get unbiased estimates of causal effects whether we “control for” pre-treatment outcome values (e.g., using DiD) or not (e.g., using *POST*), and it is not clear that we need to control for other factors that drive total accruals.  -->
以下の因果図が正しいと仮定すると、前処置の結果値（例：DiDを使用）を「制御」するかどうか（例：*POST*を使用）に関係なく、因果効果のバイアスのない推定値を得ることができ、総アクルーアルを駆動する他の要因を制御する必要があるかどうかは明確ではない。
<!-- If being a Reg SHO pilot firm leads to a reduction in earnings management, we should observe lower *total* accruals, even if we posit that the effect is through discretionary accruals, which we do not observe directly.  -->
Reg SHOのパイロット企業であることが収益管理の低下につながる場合、我々は、我々が直接観測していない裁量的アクルーアルを通じて効果があると仮定していても、より低い*総*アクルーアルを観測するはずである。
<!-- If we accept this causal diagram, then the decision as to which factors to control for is—like the choice between *DiD*, *POST*, and *ANCOVA* - a question of statistical efficiency rather than bias.  -->
この因果図を受け入れると、制御する要因の選択は、*DiD*、*POST*、*ANCOVA*の選択と同様に、バイアスではなく統計的効率性の問題である。


<!-- In this context, it is perhaps useful to consider causal diagrams to sharpen our understanding of the issues, which we explore in the discussion questions below, as matters can be more complicated if the causal diagram in [Figure 19.4](#fig-sho-causal) is incomplete. -->
この文脈では、[Figure 19.4](#fig-sho-causal)の因果図が不完全である場合、問題が複雑になる可能性があるため、問題をより鮮明に理解するために因果図を考慮することが役立つかもしれない。





<!-- ###  Discussion questions -->
### ディスカッション課題


<!-- 1. What features of [Figure 19.4](#fig-sho-causal) imply that we do not need to control for performance, sales, and PP&E in estimating the causal effect of Reg SHO on accruals?  -->
1. [Figure 19.4](#fig-sho-causal)のどの特徴が、Reg SHOが総勘定科目に対する因果効果を推定する際に、パフォーマンス、売上高、PP&Eを制御する必要がないことを示しているのか。
<!-- What is the basis for assuming these features in the causal diagram? -->
これらの特徴を因果図で仮定する根拠は何か。


<!-- 2. Black et al. ([2024](references.html#ref-Black:2022tz)) report that “over 60 papers in accounting, finance, and economics report that suspension of the price tests had wide-ranging indirect effects on pilot firms, including on earnings management, investments, leverage, acquisitions, management compensation, workplace safety, and more (see Internet Appendix, Table IA-1 for a summary).”  -->
2. Black et al. (2024)は、「会計、ファイナンス、経済学の60以上の論文が、価格テストの停止がパイロット企業に間接的な影響を及ぼし、収益管理、投資、レバレッジ、買収、経営者の報酬、職場安全性などに広範囲の影響を及ぼしたことを報告している（Internet Appendix, Table IA-1を参照）」と報告している。
<!-- In light of the Internet Appendix of Black et al. ([2024](references.html#ref-Black:2022tz)), is there any evidence that Reg SHO might plausibly have an effect on performance, sales growth, or PP&E?  -->
Black et al. (2024)のInternet Appendixを踏まえると、Reg SHOがパフォーマンス、売上成長、またはPP&Eに影響を与える可能性があるかどうかについての証拠はあるか。
<!-- If so, how would [Figure 19.4](#fig-sho-causal) need to be modified to account for these consequences?  -->
もしそうであれば、これらの結果を考慮するために[Figure 19.4](#fig-sho-causal)をどのように修正する必要があるか。
<!-- What would be the implications of these changes on the appropriate tests for estimating the causal effects of Reg SHO on accruals? -->
これらの変更が、Reg SHOがアクルーアルに対する因果効果を推定するための適切な検定に与える影響は何か。


<!-- 3. Produce a regression table like [Table 19.3](#tbl-baseline) and a plot like [Figure 19.3](#fig-coef-plot), but using discretionary accruals *without* performance matching instead of performance-matched discretionary accruals.  -->
3. [Table 19.3](#tbl-baseline)のような回帰結果の表と[Figure 19.3](#fig-coef-plot)のようなプロットを作成しなさい。ただしパフォーマンスマッチング裁量的アクルーアルではなく、パフォーマンスマッチングを行わない裁量的アクルーアルを使用なさい。
<!-- How do you interpret these results? -->
これらの結果をどのように解釈しますか。


<!-- 4. Produce a regression table and a plot like the ones in the FHK replication above, but using total accruals instead of discretionary accruals and excluding controls (so the coefficients will be simple conditional sample means).  -->
4. FHKのレプリケーションのような回帰表とプロットを作成しなさい。ただし、裁量的アクルーアルの代わりに総アクルーアルを使用し、コントロールを除外しなさい（したがって、係数は単純な条件付きサンプル平均になります）。
<!-- How do you interpret these results? -->
これらの結果をどのように解釈しますか。


<!-- 5. Suppose you had been brought in by the SEC to design a study examining the research question examined by FHK in the form of a registered report.  -->
5. SECによって、FHKが検討した研究問題を登録された報告書の形で調査する研究を設計するように依頼されたとする。
<!-- What analyses would you conduct to try to understand the best research design?  -->
最適な研究設計を理解するためにどのような分析を行いますか。
<!-- For example, how would you choose between *DiD*, *POST*, *ANCOVA*, and other empirical approaches?  -->
たとえば、*DiD*、*POST*、*ANCOVA*、およびその他の経験的アプローチの間でどのように選択するのか？
<!-- What controls would you include?  -->
どのようなコントロールを含めるのか？
<!-- How would you decide how to include controls?  -->
どのようにしてコントロールを含めるかを決定するのか？
<!-- (For example, one could control for performance by including performance as a regressor in the model of earnings management, by matching on performance, or by including performance in the main regression specification.)  -->
（たとえば、パフォーマンスを収益管理のモデルの回帰変数として含める、パフォーマンスにマッチングする、またはパフォーマンスを主要な回帰仕様に含めることで、パフォーマンスを制御することができる。）
<!-- How would you calculate standard errors?  -->
標準誤差はどのように計算するのか？
<!-- Discuss how your proposed empirical test differs from that of FHK.  -->
提案された経験的テストがFHKのものとどのように異なるかについて議論しなさい。
<!-- Would you have reported similar results to what FHK reported? -->
FHKが報告した結果と同様の結果を報告するか？


<!-- 6. Suppose that FHK’s empirical analysis had produced a positive effect of Reg SHO on earnings management?  -->
6. FHKの実証分析が利益マネジメントに対するReg SHOの正の効果を示したと仮定する。
<!-- Would this imply a lack of support for their hypotheses?  -->
これは彼らの仮説を支持しないことを意味するか？
<!-- Do you believe that publication in the *Journal of Finance* depended on finding a negative effect? -->
*Journal of Finance*での発表が負の効果を見つけることに依存していたと思うか？


<!-- 7. What implications would there have been for publication of FHK in the *Journal of Finance* if they had failed to find an effect of Reg SHO on earnings management? -->
7. Reg SHOが収益管理に与える影響を見つけることができなかった場合、FHKの*Journal of Finance*での発表にはどのような影響があったか。


<!-- ## Causal mechanisms -->
## 因果メカニズム


<!-- Black et al. ([2024](references.html#ref-Black:2022tz)) suggest a number of possible causal channels through which the Reg SHO experiment could have affected the behavior of firms or third parties, including short interest, returns, price efficient, and “manager fear”.  -->
Black et al. (2024)は、Reg SHO実験が企業や第三者の行動に影響を与える可能性のあるいくつかの因果関係を示唆している。これには、ショートインタレスト、リターン、価格効率、および「manager fear」が含まれる。
<!-- On the last of these, Black et al. ([2024, p. 4](references.html#ref-Black:2022tz)) suggest that “even if the Reg SHO experiment did not actually affect short interest or returns, pilot firm managers could have feared being targeted by short sellers and taken pre-emptive actions.” -->
最後のものに関して、Black et al. (2024, p.4)は、「Reg SHO実験がショートインタレストやリターンに実際に影響を与えなかったとしても、パイロット企業の経営者はショートセラーに狙われることを恐れ、予防的な行動を取る可能性がある」と述べている。


<!-- Black et al. ([2024, p. 5133](references.html#ref-Black:2022tz)) argue that “if firm managers were fearful that relaxing the price tests would affect them, one might expect them to voice concerns in various ways: speaking with business news reporters; writing to the SEC when it sought public comments, or seeking meetings with SEC officials to express opposition. … We searched the business press during 2003 when the rule was proposed, in 2004 when the experiment was announced, in 2006 when the SEC proposed repeal … We found no evidence of manager opposition.” -->
Black et al. (2024, p.5133) は「もし企業の経営者が、価格テストの緩和が自社に影響を及ぼすことを恐れていたのであれば、さまざまな方法で懸念を表明すると考えられる。例えば、ビジネスニュースの記者に話す、SEC（証券取引委員会）が意見を募集した際に書面で意見を提出する、あるいはSEC当局者との面会を求めて反対意見を伝えるなどである。… 私たちは、規則が提案された2003年、実験が発表された2004年、SECが撤廃を提案した2006年の間に、ビジネス報道を調査した。しかし、経営者による反対の証拠は見つからなかった。」
と主張している。

<!-- Black et al. (2024, p. 5134](references.html#ref-Black:2022tz)) suggest that “FHK rely on the manager fear channel. They conjecture that, in response to a greater threat of short selling, pilot firms’ managers reduced earnings management to preemptively deter short sellers.” -->
Black et al. (2024, p.5134) は次のように示唆している。「FHK は、経営者の恐怖チャネルに依拠している。彼らは、空売りの脅威が高まることに対応して、パイロット企業の経営者が空売り投資家を事前に抑止するために、利益操作を抑制したと推測している。」

<!-- ###  Discussion questions -->
### ディスカッション課題

<!-- 1. Do you agree with the assertion of Black et al. ([2024](references.html#ref-Black:2022tz)) that “FHK rely on the manager fear channel”? -->
1. Black et al. (2024)の主張「FHK は、経営者の恐怖チャネルに依存している」と同意するか？
<!-- What causal mechanisms are suggested in Fang et al. ([2016](references.html#ref-Fang:2016uy))?  -->
Fang et al. (2016)では、どのような因果メカニズムが示唆されているか？
<!-- What evidence do Fang et al. ([2016](references.html#ref-Fang:2016uy)) offer in support of these mechanisms? -->
Fang et al. (2016)は、これらのメカニズムを支持する証拠をどのように提供しているか？


<!-- 2. Evaluate the response of Fang et al. ([2019](references.html#ref-Fang:2019tt)) to Black et al. ([2024](references.html#ref-Black:2022tz)) as it relates to causal mechanisms? -->
2. 因果メカニズムに関連するFang et al. (2019)のBlack et al. (2024)への反応を評価しなさい。


<!-- 3. Do you think evidence of causal mechanisms is more or less important when using a natural experiment (i.e., an experiment outside the control of the researcher that is typically analysed after it has been run) than when conducting a randomized experiment?  -->
3. 研究者のコントロール外で実施される自然実験（つまり、通常は実施後に分析される実験）を行う場合、因果メカニズムの証拠は、無作為化実験を行う場合よりも重要だと思うか？
<!-- Explain your reasoning given the various issues raised in this chapter. -->
この章で取り上げられたさまざまな問題を考慮して、その理由を説明しなさい。


<!-- ## Two-step regressions -->
## 二段階回帰

<!-- @Chen:2018wh examine the question of statistical inference when residuals from one regression are used as a dependent variable in a subsequent regression, which they refer to as “the two-step procedure”.  -->
@Chen:2018wh は、1つの回帰の残差が後続の回帰の従属変数として使用される場合の統計推論について検討しており、これを「2段階手続き」と呼んでいる。
<!-- For example, discretionary accruals measured using the Jones ([1991](references.html#ref-Jones:1991vx)) model are residuals from a regression of total accruals on changes in sales and PP&E.  -->
たとえば、Jones (1991)モデルを用いた裁量的アクルーアルは、売上高の変化額とPP&Eの変化額に関する回帰の残差である。
<!-- As we saw in Dechow et al. ([1995](references.html#ref-Dechow:1995wr)), which we covered in [Chapter 16](chap16_em.html), many papers examine how Jones ([1991](references.html#ref-Jones:1991vx)) model discretionary accruals relate to various posited incentives for earnings management. -->
第16章でカバーしたDechow et al. (1995)で見たように、Jones (1991)モデルの裁量的アクルーアルが、利益マネジメントの様々なインセンティブとどのように関連しているかを多くの論文が検討している。


<!-- @Chen:2018wh [p.755] show that “the two-step procedure is likely to generate biased coefficients and t-statistics in many studies” and, drawing on the Frisch-Waugh-Lovell theorem (see [Section 3.3](reg-basics.html#sec-fwl)), propose using a single regression in place of the two-step procedure.  -->
@Chen:2018wh [p.755]は、「2段階手続きは、多くの研究でバイアスのある係数とt統計量を生成する可能性が高い」と述べており、Frisch-Waugh-Lovellの定理（[Section 3.3](reg-basics.html#sec-fwl)を参照）に基づいて、2段階手続きの代わりに単一の回帰を使用することを提案している。
<!-- In the case of the Jones ([1991](references.html#ref-Jones:1991vx)) model, this would entail including the regressors from the first step in same regression as the second step and using total accruals in place of discretionary accruals as the dependent variable. -->
Jones (1991)モデルの場合、これは、第1ステップの回帰変数を第2ステップと同じ回帰式に含め、従属変数として裁量的アクルーアルの代わりに総アクルーアルを使用することを意味する。


<!-- ### Discussion questions -->
### ディスカッション課題

<!-- 1. What challenges would exist in implementing the single-regression recommendation of @Chen:2018wh  for a researcher using @Kothari:2005aa performance-matched discretionary accruals? -->
1. @Chen:2018wh の単一回帰の推奨事項を使用する研究者が、@Kothari:2005aa のパフォーマンスマッチングされた裁量的アクルーアルを使用する際には、どのような課題があるか。

<!-- 2. Do you believe the issues raised by @Chen:2018wh with regard to two-step procedures also apply if using randomization inference? Why or why not? -->
2. @Chen:2018wh が2段階手続きに関して提起した問題が、ランダム化推論を使用する場合にも当てはまると考えるのか？その理由は何か？


## References

